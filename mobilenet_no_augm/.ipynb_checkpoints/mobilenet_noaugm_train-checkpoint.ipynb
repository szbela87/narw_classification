{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "845c8b9b-fb9a-4e89-a5e8-eff6f1e9af4c",
   "metadata": {
    "id": "845c8b9b-fb9a-4e89-a5e8-eff6f1e9af4c",
    "outputId": "130750bf-8c86-42c0-8c3b-7cdad8ea4c34"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szekeres/anaconda3/envs/pytorch-gpu/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from timeit import default_timer\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import random\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "from torchvision.models import mobilenet_v2\n",
    "from sklearn.metrics import balanced_accuracy_score, accuracy_score\n",
    "from torchsummary import summary\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] ='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60f47788-9a7b-46a1-abb2-1254484d142d",
   "metadata": {
    "id": "60f47788-9a7b-46a1-abb2-1254484d142d"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "SEED = 2024\n",
    "BATCH_SIZE = 32\n",
    "TEST_SPLIT_RATIO = 0.25\n",
    "\n",
    "N_FFT = 256\n",
    "HOP_LEN = 256 // 6\n",
    "AUGM = False\n",
    "# Creating the results directory\n",
    "if not os.path.exists('results'):\n",
    "    os.makedirs('results')\n",
    "RESULTS_FILENAME = \"./results/inrun_results\" # _x.csv\n",
    "VALID_RESULTS_FILENAME = \"./results/valid_results\" # _x.csv\n",
    "TRAIN_RESULTS_FILENAME = \"./results/train_results\" # _x.csv\n",
    "BEST_MODEL_FILENAME = \"./results/best-model\" # _x.pt\n",
    "DIV_FACTOR = 10.\n",
    "FINAL_DIV_FACTOR = 10.\n",
    "WEIGHT_DECAY = 0.005\n",
    "LEARNING_RATE = 0.0001\n",
    "EVAL_FREQ=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1385666-618c-4f6b-a7c3-f6557b4ea2d3",
   "metadata": {
    "id": "d1385666-618c-4f6b-a7c3-f6557b4ea2d3"
   },
   "outputs": [],
   "source": [
    "TRAIN_DATASET = \"../data/train_whales.csv\"\n",
    "TEST_DATASET = \"../data/test_whales.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01b41649-9ec3-4cb7-be98-df999eed5c22",
   "metadata": {
    "id": "01b41649-9ec3-4cb7-be98-df999eed5c22",
    "outputId": "eb7218fd-e35b-4061-d24b-561c0865c799"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda is available: True\n",
      "Device: ['Quadro P5000']\n"
     ]
    }
   ],
   "source": [
    "# Fixing the seeds\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "print(f\"Cuda is available: {torch.cuda.is_available()}\")\n",
    "dev_names = [torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())]\n",
    "print(f\"Device: {dev_names}\")\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fec4c0d-112b-42ea-ba6e-04608a844e08",
   "metadata": {
    "id": "1fec4c0d-112b-42ea-ba6e-04608a844e08"
   },
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623c8ab5-2e23-4939-806b-969468d9ce65",
   "metadata": {
    "id": "623c8ab5-2e23-4939-806b-969468d9ce65"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a952ea02-aa93-4150-b635-85887b595635",
   "metadata": {
    "id": "a952ea02-aa93-4150-b635-85887b595635"
   },
   "outputs": [],
   "source": [
    "target_names = [\"no-whale\",\"whale\"]\n",
    "target_names_dict = {target_names[i]: i for i in range(len(target_names))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d720b170-4e81-4d4c-b478-52369fb4c7d8",
   "metadata": {
    "id": "d720b170-4e81-4d4c-b478-52369fb4c7d8",
    "outputId": "399a3fae-3bb9-45a2-bae4-ac20dcfdffea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading - Elapsed time: 12.00s\n"
     ]
    }
   ],
   "source": [
    "t_s = default_timer()\n",
    "data_train = pd.read_csv(TRAIN_DATASET,sep=\",\")\n",
    "columns = data_train.columns\n",
    "data_train[columns[-1]]=data_train[columns[-1]].replace(target_names_dict)\n",
    "data_train = data_train.values\n",
    "data_train_labels = data_train[:,-1].reshape(-1)\n",
    "data_train_labels = data_train_labels.astype(int)\n",
    "data_train = data_train[:,:-1]\n",
    "t_e = default_timer()\n",
    "\n",
    "print(f\"Data loading - Elapsed time: {t_e-t_s:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b0df2b5-7202-44f7-8a0a-1b3568825cac",
   "metadata": {
    "id": "9b0df2b5-7202-44f7-8a0a-1b3568825cac",
    "outputId": "e234e0db-acab-440d-b333-149a95d55b38"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10316, 4000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0e0d81b-b6c6-4c2e-a290-c7711ade1e33",
   "metadata": {
    "id": "b0e0d81b-b6c6-4c2e-a290-c7711ade1e33",
    "outputId": "b2ec6496-b2cd-4919-ce76-e95cc99123df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10316, 4000)\n"
     ]
    }
   ],
   "source": [
    "print(data_train.shape)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(data_train, data_train_labels, stratify = data_train_labels, test_size = TEST_SPLIT_RATIO, random_state = SEED)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c020fb5-1825-4bb7-9c68-0de8fd4e8ba2",
   "metadata": {
    "id": "1c020fb5-1825-4bb7-9c68-0de8fd4e8ba2"
   },
   "outputs": [],
   "source": [
    "def random_data_shift(data, u=1.0):\n",
    "    if np.random.random() < u:\n",
    "        shift = int(round(np.random.uniform(-len(data)*0.25, len(data)*0.25)))\n",
    "        data = np.roll(data, shift)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91e112d8-1b5e-4cb5-a9d6-8904f5f724e9",
   "metadata": {
    "id": "91e112d8-1b5e-4cb5-a9d6-8904f5f724e9",
    "outputId": "7cb5035c-cd9c-4859-e1ec-0da6118ef926"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20,) [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n",
      "(20,) [18 19  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17]\n"
     ]
    }
   ],
   "source": [
    "test = np.arange(20)\n",
    "print(test.shape, test)\n",
    "test_out = random_data_shift(test)\n",
    "print(test_out.shape,test_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b87ae3c7-31b5-4c42-8b7e-0eaa25b68a67",
   "metadata": {
    "id": "b87ae3c7-31b5-4c42-8b7e-0eaa25b68a67"
   },
   "outputs": [],
   "source": [
    "class AugmentedSTFTDataset(Dataset):\n",
    "    def __init__(self, inputs, targets, n_fft, hop_length, augment=False):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        sample = self.inputs[idx]\n",
    "       \n",
    "        if self.augment:\n",
    "            sample = random_data_shift(sample)\n",
    "       \n",
    "        data = librosa.stft(sample, n_fft=self.n_fft, hop_length=self.hop_length)\n",
    "        data = librosa.amplitude_to_db(np.abs(data), ref=np.max)\n",
    "        data = np.flipud(data)  # Flip vertically\n",
    "        data = data.copy() \n",
    "        data = np.expand_dims(data, axis=-1)  # Add channel dimension\n",
    "        data = np.transpose(data, (2, 0, 1))  # Reorder dimensions to match PyTorch expectations\n",
    "        return torch.FloatTensor(data), torch.LongTensor([self.targets[idx]])\n",
    "\n",
    "# Data loader\n",
    "def create_dataloader(inputs, targets, batch_size, n_fft, hop_length, shuffle=True, augment=False):\n",
    "    dataset = AugmentedSTFTDataset(inputs, targets, n_fft, hop_length, augment=augment)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74451d0f-8f3c-4c42-b2ad-2bcef36ef3f6",
   "metadata": {
    "id": "74451d0f-8f3c-4c42-b2ad-2bcef36ef3f6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf264c0-bb5b-4b3e-9a0d-f4c6dcccfbce",
   "metadata": {
    "id": "ccf264c0-bb5b-4b3e-9a0d-f4c6dcccfbce"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2fba2086-ddeb-4a27-a265-7fed49ba7a1b",
   "metadata": {
    "id": "2fba2086-ddeb-4a27-a265-7fed49ba7a1b"
   },
   "outputs": [],
   "source": [
    "train_loader = create_dataloader(X_train, y_train, BATCH_SIZE, N_FFT, HOP_LEN, shuffle=True, augment=AUGM)\n",
    "valid_loader = create_dataloader(X_valid, y_valid, BATCH_SIZE, N_FFT, HOP_LEN, shuffle=False, augment=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc2b5e6c-dc77-464d-a675-1437c8475029",
   "metadata": {
    "id": "bc2b5e6c-dc77-464d-a675-1437c8475029"
   },
   "outputs": [],
   "source": [
    "EVAL_FREQ = len(train_loader)//EVAL_FREQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dce2400f-4cec-4f51-8dbc-4aa88a3ea1b0",
   "metadata": {
    "id": "dce2400f-4cec-4f51-8dbc-4aa88a3ea1b0",
    "outputId": "b007a7f6-6eea-4ac7-8309-66a2fce98dde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242 60\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader), EVAL_FREQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12d02862-0406-4e2d-95c5-b9b11ff1bda7",
   "metadata": {
    "id": "12d02862-0406-4e2d-95c5-b9b11ff1bda7",
    "outputId": "f2ad2d16-7df4-48ba-d004-61437ef37b52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 129, 96]) torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_loader:\n",
    "    break\n",
    "print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87dd433e-215f-466b-b373-f7d0ef3d8751",
   "metadata": {
    "id": "87dd433e-215f-466b-b373-f7d0ef3d8751",
    "outputId": "c1db668e-4412-4596-b44e-eea132e445db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 129, 96]) torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "for x, y in valid_loader:\n",
    "    break\n",
    "print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3aeed54-70fb-4ac4-87f0-0d534bef901d",
   "metadata": {
    "id": "b3aeed54-70fb-4ac4-87f0-0d534bef901d"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb480af7-7e5d-4c3b-969c-4b7554ed28bf",
   "metadata": {
    "id": "cb480af7-7e5d-4c3b-969c-4b7554ed28bf"
   },
   "outputs": [],
   "source": [
    "class MobileNetV2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = mobilenet_v2(weights=None).features\n",
    "        self.model[0][0] = torch.nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.classifier = torch.nn.Sequential(torch.nn.Linear(1280, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model.forward(x)\n",
    "        out = torch.nn.functional.avg_pool2d(out, kernel_size = out.shape[2:], stride= out.shape[2:], padding=0, count_include_pad = False)\n",
    "        out = self.classifier(out.view(out.shape[0], -1))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fad4ce4b-f93b-4945-97d0-010181735fdf",
   "metadata": {
    "id": "fad4ce4b-f93b-4945-97d0-010181735fdf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MobileNetV2(\n",
       "  (model): Sequential(\n",
       "    (0): Conv2dNormActivation(\n",
       "      (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace=True)\n",
       "    )\n",
       "    (1): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (4): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (8): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (9): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (10): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (11): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (12): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (13): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (14): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (15): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (16): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (17): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (18): Conv2dNormActivation(\n",
       "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=1280, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MobileNetV2()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d58780-d180-47d2-af34-6e07cfc8d362",
   "metadata": {
    "id": "02d58780-d180-47d2-af34-6e07cfc8d362"
   },
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a5fbf62-5852-4d05-8f21-56c458e791f4",
   "metadata": {
    "id": "2a5fbf62-5852-4d05-8f21-56c458e791f4"
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83695a75-92cb-4df8-b695-3cbb44b1dd1f",
   "metadata": {
    "id": "83695a75-92cb-4df8-b695-3cbb44b1dd1f"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion, device):\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "    epoch_acc = 0.0\n",
    "\n",
    "    model.eval()\n",
    "    number_of_elements = 0\n",
    "\n",
    "    correct_pred = torch.zeros(2)\n",
    "    total_pred = torch.zeros(2)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        for x, y in iterator:\n",
    "\n",
    "            x = x.to(device)\n",
    "            y = y.float().to(device).view(-1,1)\n",
    "\n",
    "            batch_size = x.shape[0]\n",
    "            number_of_elements += batch_size\n",
    "\n",
    "            pred = model(x).view(-1,1)\n",
    "            loss = criterion(pred, y)\n",
    "\n",
    "            top_pred = (torch.sigmoid(pred) > 0.5).int()\n",
    "            acc = top_pred.eq(y.int().view_as(top_pred)).sum()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "            y_true.append(y.int().cpu().numpy())\n",
    "            y_pred.append(top_pred.cpu().numpy())\n",
    "\n",
    "        y_true_a = np.concatenate(y_true, axis=0)\n",
    "        y_pred_a = np.concatenate(y_pred, axis=0)\n",
    "\n",
    "        \n",
    "        acc = accuracy_score(y_true_a, y_pred_a)\n",
    "\n",
    "    return epoch_loss / number_of_elements, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c640c8ec-9972-4e4d-9b79-1b3bc1aee51b",
   "metadata": {
    "id": "c640c8ec-9972-4e4d-9b79-1b3bc1aee51b"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9233037b-22f8-4668-8ae9-b01dcb7a2c02",
   "metadata": {
    "id": "9233037b-22f8-4668-8ae9-b01dcb7a2c02",
    "outputId": "893b66e8-9e75-440c-9d5a-d434b6b50f00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of the parameters: 2224577\n",
      "\n",
      "Training\n",
      "-----    0    ----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 001 | Batch Id: 00060 | ET: 8.82s | lr: 1.04e-05 | Val. Loss: 0.693 |  Val. Acc: 50.02% | B. Val. Loss: 0.693 |  B. Val. Acc: 50.02%\n",
      "\t | Epoch: 001 | Batch Id: 00120 | ET: 20.73s | lr: 1.15e-05 | Val. Loss: 0.693 |  Val. Acc: 51.61% | B. Val. Loss: 0.693 |  B. Val. Acc: 51.61%\n",
      "\t | Epoch: 001 | Batch Id: 00180 | ET: 32.78s | lr: 1.33e-05 | Val. Loss: 0.694 |  Val. Acc: 49.98% | B. Val. Loss: 0.693 |  B. Val. Acc: 51.61%\n",
      "\t | Epoch: 001 | Batch Id: 00240 | ET: 44.91s | lr: 1.59e-05 | Val. Loss: 0.693 |  Val. Acc: 49.98% | B. Val. Loss: 0.693 |  B. Val. Acc: 51.61%\n",
      "Epoch: 001 | ET: 68.49s | \t Train Loss: 0.693 | Train Acc: 50.01% \t Val. Loss: 0.693 |  Val. Acc: 49.98% \t | B. Val. Loss: 0.693 |  B. Val. Acc: 51.61%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 002 | Batch Id: 00060 | ET: 7.39s | lr: 1.92e-05 | Val. Loss: 0.687 |  Val. Acc: 54.09% | B. Val. Loss: 0.687 |  B. Val. Acc: 54.09%\n",
      "\t | Epoch: 002 | Batch Id: 00120 | ET: 19.35s | lr: 2.31e-05 | Val. Loss: 0.675 |  Val. Acc: 58.36% | B. Val. Loss: 0.675 |  B. Val. Acc: 58.36%\n",
      "\t | Epoch: 002 | Batch Id: 00180 | ET: 31.30s | lr: 2.74e-05 | Val. Loss: 0.684 |  Val. Acc: 56.73% | B. Val. Loss: 0.675 |  B. Val. Acc: 58.36%\n",
      "\t | Epoch: 002 | Batch Id: 00240 | ET: 43.11s | lr: 3.23e-05 | Val. Loss: 0.643 |  Val. Acc: 63.71% | B. Val. Loss: 0.643 |  B. Val. Acc: 63.71%\n",
      "Epoch: 002 | ET: 66.76s | \t Train Loss: 0.572 | Train Acc: 70.38% \t Val. Loss: 0.596 |  Val. Acc: 68.09% \t | B. Val. Loss: 0.596 |  B. Val. Acc: 68.09%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 003 | Batch Id: 00060 | ET: 7.40s | lr: 3.77e-05 | Val. Loss: 0.563 |  Val. Acc: 70.88% | B. Val. Loss: 0.563 |  B. Val. Acc: 70.88%\n",
      "\t | Epoch: 003 | Batch Id: 00120 | ET: 19.34s | lr: 4.32e-05 | Val. Loss: 0.517 |  Val. Acc: 75.07% | B. Val. Loss: 0.517 |  B. Val. Acc: 75.07%\n",
      "\t | Epoch: 003 | Batch Id: 00180 | ET: 31.29s | lr: 4.89e-05 | Val. Loss: 0.506 |  Val. Acc: 75.65% | B. Val. Loss: 0.506 |  B. Val. Acc: 75.65%\n",
      "\t | Epoch: 003 | Batch Id: 00240 | ET: 43.33s | lr: 5.48e-05 | Val. Loss: 0.485 |  Val. Acc: 77.67% | B. Val. Loss: 0.485 |  B. Val. Acc: 77.67%\n",
      "Epoch: 003 | ET: 67.47s | \t Train Loss: 0.444 | Train Acc: 78.98% \t Val. Loss: 0.487 |  Val. Acc: 77.16% \t | B. Val. Loss: 0.485 |  B. Val. Acc: 77.67%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 004 | Batch Id: 00060 | ET: 7.63s | lr: 6.08e-05 | Val. Loss: 0.505 |  Val. Acc: 75.61% | B. Val. Loss: 0.485 |  B. Val. Acc: 77.67%\n",
      "\t | Epoch: 004 | Batch Id: 00120 | ET: 19.46s | lr: 6.65e-05 | Val. Loss: 0.503 |  Val. Acc: 75.46% | B. Val. Loss: 0.485 |  B. Val. Acc: 77.67%\n",
      "\t | Epoch: 004 | Batch Id: 00180 | ET: 31.28s | lr: 7.21e-05 | Val. Loss: 0.481 |  Val. Acc: 77.43% | B. Val. Loss: 0.481 |  B. Val. Acc: 77.67%\n",
      "\t | Epoch: 004 | Batch Id: 00240 | ET: 43.10s | lr: 7.73e-05 | Val. Loss: 0.519 |  Val. Acc: 74.29% | B. Val. Loss: 0.481 |  B. Val. Acc: 77.67%\n",
      "Epoch: 004 | ET: 66.65s | \t Train Loss: 0.463 | Train Acc: 76.84% \t Val. Loss: 0.507 |  Val. Acc: 74.84% \t | B. Val. Loss: 0.481 |  B. Val. Acc: 77.67%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 005 | Batch Id: 00060 | ET: 7.53s | lr: 8.23e-05 | Val. Loss: 0.488 |  Val. Acc: 76.81% | B. Val. Loss: 0.481 |  B. Val. Acc: 77.67%\n",
      "\t | Epoch: 005 | Batch Id: 00120 | ET: 19.69s | lr: 8.67e-05 | Val. Loss: 0.539 |  Val. Acc: 73.87% | B. Val. Loss: 0.481 |  B. Val. Acc: 77.67%\n",
      "\t | Epoch: 005 | Batch Id: 00180 | ET: 31.55s | lr: 9.06e-05 | Val. Loss: 0.462 |  Val. Acc: 79.26% | B. Val. Loss: 0.462 |  B. Val. Acc: 79.26%\n",
      "\t | Epoch: 005 | Batch Id: 00240 | ET: 43.72s | lr: 9.39e-05 | Val. Loss: 0.458 |  Val. Acc: 78.98% | B. Val. Loss: 0.458 |  B. Val. Acc: 79.26%\n",
      "Epoch: 005 | ET: 67.65s | \t Train Loss: 0.416 | Train Acc: 82.65% \t Val. Loss: 0.480 |  Val. Acc: 77.28% \t | B. Val. Loss: 0.458 |  B. Val. Acc: 79.26%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 006 | Batch Id: 00060 | ET: 7.62s | lr: 9.66e-05 | Val. Loss: 0.475 |  Val. Acc: 78.05% | B. Val. Loss: 0.458 |  B. Val. Acc: 79.26%\n",
      "\t | Epoch: 006 | Batch Id: 00120 | ET: 19.77s | lr: 9.84e-05 | Val. Loss: 0.497 |  Val. Acc: 77.20% | B. Val. Loss: 0.458 |  B. Val. Acc: 79.26%\n",
      "\t | Epoch: 006 | Batch Id: 00180 | ET: 31.92s | lr: 9.96e-05 | Val. Loss: 0.471 |  Val. Acc: 78.21% | B. Val. Loss: 0.458 |  B. Val. Acc: 79.26%\n",
      "\t | Epoch: 006 | Batch Id: 00240 | ET: 44.08s | lr: 1.00e-04 | Val. Loss: 0.457 |  Val. Acc: 79.26% | B. Val. Loss: 0.457 |  B. Val. Acc: 79.26%\n",
      "Epoch: 006 | ET: 68.10s | \t Train Loss: 0.372 | Train Acc: 83.64% \t Val. Loss: 0.470 |  Val. Acc: 79.02% \t | B. Val. Loss: 0.457 |  B. Val. Acc: 79.26%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 007 | Batch Id: 00060 | ET: 7.62s | lr: 9.99e-05 | Val. Loss: 0.474 |  Val. Acc: 78.75% | B. Val. Loss: 0.457 |  B. Val. Acc: 79.26%\n",
      "\t | Epoch: 007 | Batch Id: 00120 | ET: 19.77s | lr: 9.97e-05 | Val. Loss: 0.470 |  Val. Acc: 78.32% | B. Val. Loss: 0.457 |  B. Val. Acc: 79.26%\n",
      "\t | Epoch: 007 | Batch Id: 00180 | ET: 31.91s | lr: 9.93e-05 | Val. Loss: 0.448 |  Val. Acc: 79.80% | B. Val. Loss: 0.448 |  B. Val. Acc: 79.80%\n",
      "\t | Epoch: 007 | Batch Id: 00240 | ET: 44.19s | lr: 9.88e-05 | Val. Loss: 0.496 |  Val. Acc: 78.17% | B. Val. Loss: 0.448 |  B. Val. Acc: 79.80%\n",
      "Epoch: 007 | ET: 68.22s | \t Train Loss: 0.346 | Train Acc: 85.20% \t Val. Loss: 0.451 |  Val. Acc: 79.29% \t | B. Val. Loss: 0.448 |  B. Val. Acc: 79.80%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 008 | Batch Id: 00060 | ET: 7.61s | lr: 9.81e-05 | Val. Loss: 0.487 |  Val. Acc: 78.91% | B. Val. Loss: 0.448 |  B. Val. Acc: 79.80%\n",
      "\t | Epoch: 008 | Batch Id: 00120 | ET: 19.76s | lr: 9.72e-05 | Val. Loss: 0.463 |  Val. Acc: 79.84% | B. Val. Loss: 0.448 |  B. Val. Acc: 79.84%\n",
      "\t | Epoch: 008 | Batch Id: 00180 | ET: 32.04s | lr: 9.63e-05 | Val. Loss: 0.511 |  Val. Acc: 79.49% | B. Val. Loss: 0.448 |  B. Val. Acc: 79.84%\n",
      "\t | Epoch: 008 | Batch Id: 00240 | ET: 44.19s | lr: 9.51e-05 | Val. Loss: 0.458 |  Val. Acc: 78.64% | B. Val. Loss: 0.448 |  B. Val. Acc: 79.84%\n",
      "Epoch: 008 | ET: 68.22s | \t Train Loss: 0.315 | Train Acc: 87.44% \t Val. Loss: 0.455 |  Val. Acc: 79.06% \t | B. Val. Loss: 0.448 |  B. Val. Acc: 79.84%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 009 | Batch Id: 00060 | ET: 7.62s | lr: 9.38e-05 | Val. Loss: 0.496 |  Val. Acc: 79.45% | B. Val. Loss: 0.448 |  B. Val. Acc: 79.84%\n",
      "\t | Epoch: 009 | Batch Id: 00120 | ET: 19.76s | lr: 9.24e-05 | Val. Loss: 0.503 |  Val. Acc: 77.90% | B. Val. Loss: 0.448 |  B. Val. Acc: 79.84%\n",
      "\t | Epoch: 009 | Batch Id: 00180 | ET: 31.92s | lr: 9.09e-05 | Val. Loss: 0.449 |  Val. Acc: 80.11% | B. Val. Loss: 0.448 |  B. Val. Acc: 80.11%\n",
      "\t | Epoch: 009 | Batch Id: 00240 | ET: 44.20s | lr: 8.93e-05 | Val. Loss: 0.488 |  Val. Acc: 80.50% | B. Val. Loss: 0.448 |  B. Val. Acc: 80.50%\n",
      "Epoch: 009 | ET: 68.34s | \t Train Loss: 0.242 | Train Acc: 90.18% \t Val. Loss: 0.500 |  Val. Acc: 80.50% \t | B. Val. Loss: 0.448 |  B. Val. Acc: 80.50%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 010 | Batch Id: 00060 | ET: 7.61s | lr: 8.74e-05 | Val. Loss: 0.498 |  Val. Acc: 80.46% | B. Val. Loss: 0.448 |  B. Val. Acc: 80.50%\n",
      "\t | Epoch: 010 | Batch Id: 00120 | ET: 19.77s | lr: 8.55e-05 | Val. Loss: 0.571 |  Val. Acc: 77.55% | B. Val. Loss: 0.448 |  B. Val. Acc: 80.50%\n",
      "\t | Epoch: 010 | Batch Id: 00180 | ET: 31.92s | lr: 8.35e-05 | Val. Loss: 0.464 |  Val. Acc: 80.92% | B. Val. Loss: 0.448 |  B. Val. Acc: 80.92%\n",
      "\t | Epoch: 010 | Batch Id: 00240 | ET: 44.20s | lr: 8.14e-05 | Val. Loss: 0.509 |  Val. Acc: 79.45% | B. Val. Loss: 0.448 |  B. Val. Acc: 80.92%\n",
      "Epoch: 010 | ET: 68.28s | \t Train Loss: 0.179 | Train Acc: 93.10% \t Val. Loss: 0.512 |  Val. Acc: 79.41% \t | B. Val. Loss: 0.448 |  B. Val. Acc: 80.92%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 011 | Batch Id: 00060 | ET: 7.61s | lr: 7.92e-05 | Val. Loss: 0.539 |  Val. Acc: 79.29% | B. Val. Loss: 0.448 |  B. Val. Acc: 80.92%\n",
      "\t | Epoch: 011 | Batch Id: 00120 | ET: 19.76s | lr: 7.69e-05 | Val. Loss: 0.529 |  Val. Acc: 79.53% | B. Val. Loss: 0.448 |  B. Val. Acc: 80.92%\n",
      "\t | Epoch: 011 | Batch Id: 00180 | ET: 31.91s | lr: 7.45e-05 | Val. Loss: 0.563 |  Val. Acc: 80.19% | B. Val. Loss: 0.448 |  B. Val. Acc: 80.92%\n",
      "\t | Epoch: 011 | Batch Id: 00240 | ET: 44.05s | lr: 7.21e-05 | Val. Loss: 0.528 |  Val. Acc: 80.46% | B. Val. Loss: 0.448 |  B. Val. Acc: 80.92%\n",
      "Epoch: 011 | ET: 68.07s | \t Train Loss: 0.153 | Train Acc: 93.90% \t Val. Loss: 0.518 |  Val. Acc: 80.30% \t | B. Val. Loss: 0.448 |  B. Val. Acc: 80.92%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 012 | Batch Id: 00060 | ET: 7.62s | lr: 6.95e-05 | Val. Loss: 0.553 |  Val. Acc: 79.88% | B. Val. Loss: 0.448 |  B. Val. Acc: 80.92%\n",
      "\t | Epoch: 012 | Batch Id: 00120 | ET: 19.77s | lr: 6.69e-05 | Val. Loss: 0.675 |  Val. Acc: 80.15% | B. Val. Loss: 0.448 |  B. Val. Acc: 80.92%\n",
      "\t | Epoch: 012 | Batch Id: 00180 | ET: 31.92s | lr: 6.43e-05 | Val. Loss: 0.571 |  Val. Acc: 79.10% | B. Val. Loss: 0.448 |  B. Val. Acc: 80.92%\n",
      "\t | Epoch: 012 | Batch Id: 00240 | ET: 44.06s | lr: 6.16e-05 | Val. Loss: 0.591 |  Val. Acc: 79.60% | B. Val. Loss: 0.448 |  B. Val. Acc: 80.92%\n",
      "Epoch: 012 | ET: 68.06s | \t Train Loss: 0.082 | Train Acc: 97.30% \t Val. Loss: 0.594 |  Val. Acc: 79.72% \t | B. Val. Loss: 0.448 |  B. Val. Acc: 80.92%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 013 | Batch Id: 00060 | ET: 7.62s | lr: 5.88e-05 | Val. Loss: 0.595 |  Val. Acc: 79.02% | B. Val. Loss: 0.448 |  B. Val. Acc: 80.92%\n",
      "\t | Epoch: 013 | Batch Id: 00120 | ET: 19.78s | lr: 5.61e-05 | Val. Loss: 0.827 |  Val. Acc: 80.26% | B. Val. Loss: 0.448 |  B. Val. Acc: 80.92%\n",
      "\t | Epoch: 013 | Batch Id: 00180 | ET: 31.93s | lr: 5.33e-05 | Val. Loss: 0.779 |  Val. Acc: 77.94% | B. Val. Loss: 0.448 |  B. Val. Acc: 80.92%\n",
      "\t | Epoch: 013 | Batch Id: 00240 | ET: 44.08s | lr: 5.06e-05 | Val. Loss: 0.692 |  Val. Acc: 78.48% | B. Val. Loss: 0.448 |  B. Val. Acc: 80.92%\n",
      "Epoch: 013 | ET: 68.11s | \t Train Loss: 0.060 | Train Acc: 98.27% \t Val. Loss: 0.665 |  Val. Acc: 79.80% \t | B. Val. Loss: 0.448 |  B. Val. Acc: 80.92%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 014 | Batch Id: 00060 | ET: 7.62s | lr: 4.77e-05 | Val. Loss: 0.635 |  Val. Acc: 79.37% | B. Val. Loss: 0.448 |  B. Val. Acc: 80.92%\n",
      "\t | Epoch: 014 | Batch Id: 00120 | ET: 19.77s | lr: 4.50e-05 | Val. Loss: 0.882 |  Val. Acc: 80.77% | B. Val. Loss: 0.448 |  B. Val. Acc: 80.92%\n",
      "\t | Epoch: 014 | Batch Id: 00180 | ET: 31.91s | lr: 4.23e-05 | Val. Loss: 0.937 |  Val. Acc: 79.49% | B. Val. Loss: 0.448 |  B. Val. Acc: 80.92%\n",
      "\t | Epoch: 014 | Batch Id: 00240 | ET: 44.06s | lr: 3.96e-05 | Val. Loss: 0.988 |  Val. Acc: 80.19% | B. Val. Loss: 0.448 |  B. Val. Acc: 80.92%\n",
      "Epoch: 014 | ET: 68.19s | \t Train Loss: 0.049 | Train Acc: 98.15% \t Val. Loss: 1.027 |  Val. Acc: 79.14% \t | B. Val. Loss: 0.448 |  B. Val. Acc: 80.92%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 015 | Batch Id: 00060 | ET: 7.79s | lr: 3.68e-05 | Val. Loss: 0.700 |  Val. Acc: 79.84% | B. Val. Loss: 0.448 |  B. Val. Acc: 80.92%\n",
      "\t | Epoch: 015 | Batch Id: 00120 | ET: 19.96s | lr: 3.42e-05 | Val. Loss: 0.892 |  Val. Acc: 80.22% | B. Val. Loss: 0.448 |  B. Val. Acc: 80.92%\n",
      "\t | Epoch: 015 | Batch Id: 00180 | ET: 33.76s | lr: 3.16e-05 | Val. Loss: 1.011 |  Val. Acc: 78.95% | B. Val. Loss: 0.448 |  B. Val. Acc: 80.92%\n",
      "\t | Epoch: 015 | Batch Id: 00240 | ET: 47.61s | lr: 2.91e-05 | Val. Loss: 1.070 |  Val. Acc: 80.19% | B. Val. Loss: 0.448 |  B. Val. Acc: 80.92%\n",
      "Epoch: 015 | ET: 74.03s | \t Train Loss: 0.003 | Train Acc: 99.95% \t Val. Loss: 1.068 |  Val. Acc: 80.07% \t | B. Val. Loss: 0.448 |  B. Val. Acc: 80.92%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 016 | Batch Id: 00060 | ET: 9.16s | lr: 2.66e-05 | Val. Loss: 0.735 |  Val. Acc: 78.95% | B. Val. Loss: 0.448 |  B. Val. Acc: 80.92%\n",
      "\t | Epoch: 016 | Batch Id: 00120 | ET: 23.45s | lr: 2.42e-05 | Val. Loss: 0.889 |  Val. Acc: 80.11% | B. Val. Loss: 0.448 |  B. Val. Acc: 80.92%\n",
      "\t | Epoch: 016 | Batch Id: 00180 | ET: 37.59s | lr: 2.19e-05 | Val. Loss: 0.988 |  Val. Acc: 80.54% | B. Val. Loss: 0.448 |  B. Val. Acc: 80.92%\n",
      "\t | Epoch: 016 | Batch Id: 00240 | ET: 50.38s | lr: 1.97e-05 | Val. Loss: 1.075 |  Val. Acc: 80.42% | B. Val. Loss: 0.448 |  B. Val. Acc: 80.92%\n",
      "Epoch: 016 | ET: 74.43s | \t Train Loss: 0.002 | Train Acc: 100.00% \t Val. Loss: 1.069 |  Val. Acc: 80.42% \t | B. Val. Loss: 0.448 |  B. Val. Acc: 80.92%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 017 | Batch Id: 00060 | ET: 7.62s | lr: 1.75e-05 | Val. Loss: 0.754 |  Val. Acc: 79.95% | B. Val. Loss: 0.448 |  B. Val. Acc: 80.92%\n",
      "\t | Epoch: 017 | Batch Id: 00120 | ET: 19.70s | lr: 1.55e-05 | Val. Loss: 0.848 |  Val. Acc: 80.22% | B. Val. Loss: 0.448 |  B. Val. Acc: 80.92%\n",
      "\t | Epoch: 017 | Batch Id: 00180 | ET: 31.80s | lr: 1.36e-05 | Val. Loss: 0.930 |  Val. Acc: 80.46% | B. Val. Loss: 0.448 |  B. Val. Acc: 80.92%\n",
      "\t | Epoch: 017 | Batch Id: 00240 | ET: 43.83s | lr: 1.19e-05 | Val. Loss: 0.985 |  Val. Acc: 80.57% | B. Val. Loss: 0.448 |  B. Val. Acc: 80.92%\n",
      "Epoch: 017 | ET: 67.69s | \t Train Loss: 0.002 | Train Acc: 100.00% \t Val. Loss: 0.987 |  Val. Acc: 80.65% \t | B. Val. Loss: 0.448 |  B. Val. Acc: 80.92%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 018 | Batch Id: 00060 | ET: 7.54s | lr: 1.01e-05 | Val. Loss: 0.777 |  Val. Acc: 80.03% | B. Val. Loss: 0.448 |  B. Val. Acc: 80.92%\n",
      "\t | Epoch: 018 | Batch Id: 00120 | ET: 19.58s | lr: 8.61e-06 | Val. Loss: 0.829 |  Val. Acc: 80.03% | B. Val. Loss: 0.448 |  B. Val. Acc: 80.92%\n",
      "\t | Epoch: 018 | Batch Id: 00180 | ET: 31.62s | lr: 7.21e-06 | Val. Loss: 0.876 |  Val. Acc: 79.91% | B. Val. Loss: 0.448 |  B. Val. Acc: 80.92%\n",
      "\t | Epoch: 018 | Batch Id: 00240 | ET: 43.63s | lr: 5.94e-06 | Val. Loss: 0.910 |  Val. Acc: 79.88% | B. Val. Loss: 0.448 |  B. Val. Acc: 80.92%\n",
      "Epoch: 018 | ET: 67.43s | \t Train Loss: 0.003 | Train Acc: 100.00% \t Val. Loss: 0.911 |  Val. Acc: 79.91% \t | B. Val. Loss: 0.448 |  B. Val. Acc: 80.92%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 019 | Batch Id: 00060 | ET: 7.53s | lr: 4.78e-06 | Val. Loss: 0.766 |  Val. Acc: 79.80% | B. Val. Loss: 0.448 |  B. Val. Acc: 80.92%\n",
      "\t | Epoch: 019 | Batch Id: 00120 | ET: 19.57s | lr: 3.79e-06 | Val. Loss: 0.794 |  Val. Acc: 79.91% | B. Val. Loss: 0.448 |  B. Val. Acc: 80.92%\n",
      "\t | Epoch: 019 | Batch Id: 00180 | ET: 31.60s | lr: 2.95e-06 | Val. Loss: 0.821 |  Val. Acc: 79.91% | B. Val. Loss: 0.448 |  B. Val. Acc: 80.92%\n",
      "\t | Epoch: 019 | Batch Id: 00240 | ET: 43.63s | lr: 2.26e-06 | Val. Loss: 0.842 |  Val. Acc: 79.99% | B. Val. Loss: 0.448 |  B. Val. Acc: 80.92%\n",
      "Epoch: 019 | ET: 67.49s | \t Train Loss: 0.004 | Train Acc: 100.00% \t Val. Loss: 0.843 |  Val. Acc: 79.99% \t | B. Val. Loss: 0.448 |  B. Val. Acc: 80.92%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 020 | Batch Id: 00060 | ET: 7.53s | lr: 1.70e-06 | Val. Loss: 0.779 |  Val. Acc: 80.11% | B. Val. Loss: 0.448 |  B. Val. Acc: 80.92%\n",
      "\t | Epoch: 020 | Batch Id: 00120 | ET: 19.58s | lr: 1.32e-06 | Val. Loss: 0.787 |  Val. Acc: 79.99% | B. Val. Loss: 0.448 |  B. Val. Acc: 80.92%\n",
      "\t | Epoch: 020 | Batch Id: 00180 | ET: 31.63s | lr: 1.08e-06 | Val. Loss: 0.793 |  Val. Acc: 80.11% | B. Val. Loss: 0.448 |  B. Val. Acc: 80.92%\n",
      "\t | Epoch: 020 | Batch Id: 00240 | ET: 43.74s | lr: 1.00e-06 | Val. Loss: 0.804 |  Val. Acc: 79.99% | B. Val. Loss: 0.448 |  B. Val. Acc: 80.92%\n",
      "Epoch: 020 | ET: 67.67s | \t Train Loss: 0.009 | Train Acc: 99.95% \t Val. Loss: 0.805 |  Val. Acc: 79.95% \t | B. Val. Loss: 0.448 |  B. Val. Acc: 80.92%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Number of the parameters: 2224577\n",
      "\n",
      "Training\n",
      "-----    1    ----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 001 | Batch Id: 00060 | ET: 7.60s | lr: 1.04e-05 | Val. Loss: 0.693 |  Val. Acc: 54.40% | B. Val. Loss: 0.693 |  B. Val. Acc: 54.40%\n",
      "\t | Epoch: 001 | Batch Id: 00120 | ET: 20.05s | lr: 1.15e-05 | Val. Loss: 0.693 |  Val. Acc: 50.02% | B. Val. Loss: 0.693 |  B. Val. Acc: 54.40%\n",
      "\t | Epoch: 001 | Batch Id: 00180 | ET: 32.10s | lr: 1.33e-05 | Val. Loss: 0.693 |  Val. Acc: 50.02% | B. Val. Loss: 0.693 |  B. Val. Acc: 54.40%\n",
      "\t | Epoch: 001 | Batch Id: 00240 | ET: 44.26s | lr: 1.59e-05 | Val. Loss: 0.694 |  Val. Acc: 49.98% | B. Val. Loss: 0.693 |  B. Val. Acc: 54.40%\n",
      "Epoch: 001 | ET: 68.23s | \t Train Loss: 0.693 | Train Acc: 50.01% \t Val. Loss: 0.693 |  Val. Acc: 49.98% \t | B. Val. Loss: 0.693 |  B. Val. Acc: 54.40%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 002 | Batch Id: 00060 | ET: 7.52s | lr: 1.92e-05 | Val. Loss: 0.684 |  Val. Acc: 58.20% | B. Val. Loss: 0.684 |  B. Val. Acc: 58.20%\n",
      "\t | Epoch: 002 | Batch Id: 00120 | ET: 19.67s | lr: 2.31e-05 | Val. Loss: 0.664 |  Val. Acc: 58.67% | B. Val. Loss: 0.664 |  B. Val. Acc: 58.67%\n",
      "\t | Epoch: 002 | Batch Id: 00180 | ET: 31.95s | lr: 2.74e-05 | Val. Loss: 0.586 |  Val. Acc: 68.83% | B. Val. Loss: 0.586 |  B. Val. Acc: 68.83%\n",
      "\t | Epoch: 002 | Batch Id: 00240 | ET: 44.22s | lr: 3.23e-05 | Val. Loss: 0.603 |  Val. Acc: 73.32% | B. Val. Loss: 0.586 |  B. Val. Acc: 73.32%\n",
      "Epoch: 002 | ET: 68.44s | \t Train Loss: 0.535 | Train Acc: 77.32% \t Val. Loss: 0.576 |  Val. Acc: 74.14% \t | B. Val. Loss: 0.576 |  B. Val. Acc: 74.14%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 003 | Batch Id: 00060 | ET: 7.60s | lr: 3.77e-05 | Val. Loss: 0.581 |  Val. Acc: 70.57% | B. Val. Loss: 0.576 |  B. Val. Acc: 74.14%\n",
      "\t | Epoch: 003 | Batch Id: 00120 | ET: 19.75s | lr: 4.32e-05 | Val. Loss: 0.557 |  Val. Acc: 74.60% | B. Val. Loss: 0.557 |  B. Val. Acc: 74.60%\n",
      "\t | Epoch: 003 | Batch Id: 00180 | ET: 31.97s | lr: 4.89e-05 | Val. Loss: 0.526 |  Val. Acc: 75.49% | B. Val. Loss: 0.526 |  B. Val. Acc: 75.49%\n",
      "\t | Epoch: 003 | Batch Id: 00240 | ET: 44.15s | lr: 5.48e-05 | Val. Loss: 0.507 |  Val. Acc: 75.96% | B. Val. Loss: 0.507 |  B. Val. Acc: 75.96%\n",
      "Epoch: 003 | ET: 68.34s | \t Train Loss: 0.446 | Train Acc: 80.55% \t Val. Loss: 0.496 |  Val. Acc: 77.82% \t | B. Val. Loss: 0.496 |  B. Val. Acc: 77.82%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 004 | Batch Id: 00060 | ET: 7.61s | lr: 6.08e-05 | Val. Loss: 0.516 |  Val. Acc: 75.22% | B. Val. Loss: 0.496 |  B. Val. Acc: 77.82%\n",
      "\t | Epoch: 004 | Batch Id: 00120 | ET: 19.76s | lr: 6.65e-05 | Val. Loss: 0.556 |  Val. Acc: 75.18% | B. Val. Loss: 0.496 |  B. Val. Acc: 77.82%\n",
      "\t | Epoch: 004 | Batch Id: 00180 | ET: 31.90s | lr: 7.21e-05 | Val. Loss: 0.507 |  Val. Acc: 77.36% | B. Val. Loss: 0.496 |  B. Val. Acc: 77.82%\n",
      "\t | Epoch: 004 | Batch Id: 00240 | ET: 44.04s | lr: 7.73e-05 | Val. Loss: 0.471 |  Val. Acc: 78.60% | B. Val. Loss: 0.471 |  B. Val. Acc: 78.60%\n",
      "Epoch: 004 | ET: 68.20s | \t Train Loss: 0.417 | Train Acc: 81.70% \t Val. Loss: 0.474 |  Val. Acc: 77.63% \t | B. Val. Loss: 0.471 |  B. Val. Acc: 78.60%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 005 | Batch Id: 00060 | ET: 7.56s | lr: 8.23e-05 | Val. Loss: 0.473 |  Val. Acc: 78.09% | B. Val. Loss: 0.471 |  B. Val. Acc: 78.60%\n",
      "\t | Epoch: 005 | Batch Id: 00120 | ET: 19.68s | lr: 8.67e-05 | Val. Loss: 0.456 |  Val. Acc: 78.95% | B. Val. Loss: 0.456 |  B. Val. Acc: 78.95%\n",
      "\t | Epoch: 005 | Batch Id: 00180 | ET: 31.96s | lr: 9.06e-05 | Val. Loss: 0.467 |  Val. Acc: 78.21% | B. Val. Loss: 0.456 |  B. Val. Acc: 78.95%\n",
      "\t | Epoch: 005 | Batch Id: 00240 | ET: 44.02s | lr: 9.39e-05 | Val. Loss: 0.431 |  Val. Acc: 80.22% | B. Val. Loss: 0.431 |  B. Val. Acc: 80.22%\n",
      "Epoch: 005 | ET: 68.26s | \t Train Loss: 0.356 | Train Acc: 84.71% \t Val. Loss: 0.428 |  Val. Acc: 80.30% \t | B. Val. Loss: 0.428 |  B. Val. Acc: 80.30%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 006 | Batch Id: 00060 | ET: 7.60s | lr: 9.66e-05 | Val. Loss: 0.440 |  Val. Acc: 79.18% | B. Val. Loss: 0.428 |  B. Val. Acc: 80.30%\n",
      "\t | Epoch: 006 | Batch Id: 00120 | ET: 19.74s | lr: 9.84e-05 | Val. Loss: 0.446 |  Val. Acc: 80.26% | B. Val. Loss: 0.428 |  B. Val. Acc: 80.30%\n",
      "\t | Epoch: 006 | Batch Id: 00180 | ET: 31.88s | lr: 9.96e-05 | Val. Loss: 0.432 |  Val. Acc: 80.54% | B. Val. Loss: 0.428 |  B. Val. Acc: 80.54%\n",
      "\t | Epoch: 006 | Batch Id: 00240 | ET: 44.17s | lr: 1.00e-04 | Val. Loss: 0.432 |  Val. Acc: 80.81% | B. Val. Loss: 0.428 |  B. Val. Acc: 80.81%\n",
      "Epoch: 006 | ET: 68.29s | \t Train Loss: 0.358 | Train Acc: 84.57% \t Val. Loss: 0.451 |  Val. Acc: 80.61% \t | B. Val. Loss: 0.428 |  B. Val. Acc: 80.81%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 007 | Batch Id: 00060 | ET: 7.54s | lr: 9.99e-05 | Val. Loss: 0.404 |  Val. Acc: 81.27% | B. Val. Loss: 0.404 |  B. Val. Acc: 81.27%\n",
      "\t | Epoch: 007 | Batch Id: 00120 | ET: 19.72s | lr: 9.97e-05 | Val. Loss: 0.409 |  Val. Acc: 82.01% | B. Val. Loss: 0.404 |  B. Val. Acc: 82.01%\n",
      "\t | Epoch: 007 | Batch Id: 00180 | ET: 31.89s | lr: 9.93e-05 | Val. Loss: 0.396 |  Val. Acc: 82.32% | B. Val. Loss: 0.396 |  B. Val. Acc: 82.32%\n",
      "\t | Epoch: 007 | Batch Id: 00240 | ET: 44.10s | lr: 9.88e-05 | Val. Loss: 0.392 |  Val. Acc: 83.06% | B. Val. Loss: 0.392 |  B. Val. Acc: 83.06%\n",
      "Epoch: 007 | ET: 68.23s | \t Train Loss: 0.257 | Train Acc: 89.22% \t Val. Loss: 0.386 |  Val. Acc: 83.06% \t | B. Val. Loss: 0.386 |  B. Val. Acc: 83.06%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 008 | Batch Id: 00060 | ET: 7.52s | lr: 9.81e-05 | Val. Loss: 0.399 |  Val. Acc: 82.40% | B. Val. Loss: 0.386 |  B. Val. Acc: 83.06%\n",
      "\t | Epoch: 008 | Batch Id: 00120 | ET: 19.53s | lr: 9.72e-05 | Val. Loss: 0.395 |  Val. Acc: 83.06% | B. Val. Loss: 0.386 |  B. Val. Acc: 83.06%\n",
      "\t | Epoch: 008 | Batch Id: 00180 | ET: 31.59s | lr: 9.63e-05 | Val. Loss: 0.429 |  Val. Acc: 82.44% | B. Val. Loss: 0.386 |  B. Val. Acc: 83.06%\n",
      "\t | Epoch: 008 | Batch Id: 00240 | ET: 43.52s | lr: 9.51e-05 | Val. Loss: 0.388 |  Val. Acc: 83.79% | B. Val. Loss: 0.386 |  B. Val. Acc: 83.79%\n",
      "Epoch: 008 | ET: 67.76s | \t Train Loss: 0.226 | Train Acc: 90.64% \t Val. Loss: 0.371 |  Val. Acc: 84.30% \t | B. Val. Loss: 0.371 |  B. Val. Acc: 84.30%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 009 | Batch Id: 00060 | ET: 7.63s | lr: 9.38e-05 | Val. Loss: 0.375 |  Val. Acc: 83.75% | B. Val. Loss: 0.371 |  B. Val. Acc: 84.30%\n",
      "\t | Epoch: 009 | Batch Id: 00120 | ET: 19.81s | lr: 9.24e-05 | Val. Loss: 0.368 |  Val. Acc: 83.87% | B. Val. Loss: 0.368 |  B. Val. Acc: 84.30%\n",
      "\t | Epoch: 009 | Batch Id: 00180 | ET: 31.97s | lr: 9.09e-05 | Val. Loss: 0.368 |  Val. Acc: 83.99% | B. Val. Loss: 0.368 |  B. Val. Acc: 84.30%\n",
      "\t | Epoch: 009 | Batch Id: 00240 | ET: 44.14s | lr: 8.93e-05 | Val. Loss: 0.382 |  Val. Acc: 83.37% | B. Val. Loss: 0.368 |  B. Val. Acc: 84.30%\n",
      "Epoch: 009 | ET: 68.17s | \t Train Loss: 0.196 | Train Acc: 91.68% \t Val. Loss: 0.388 |  Val. Acc: 83.13% \t | B. Val. Loss: 0.368 |  B. Val. Acc: 84.30%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 010 | Batch Id: 00060 | ET: 7.63s | lr: 8.74e-05 | Val. Loss: 0.406 |  Val. Acc: 84.18% | B. Val. Loss: 0.368 |  B. Val. Acc: 84.30%\n",
      "\t | Epoch: 010 | Batch Id: 00120 | ET: 19.79s | lr: 8.55e-05 | Val. Loss: 0.385 |  Val. Acc: 83.17% | B. Val. Loss: 0.368 |  B. Val. Acc: 84.30%\n",
      "\t | Epoch: 010 | Batch Id: 00180 | ET: 31.96s | lr: 8.35e-05 | Val. Loss: 0.413 |  Val. Acc: 83.87% | B. Val. Loss: 0.368 |  B. Val. Acc: 84.30%\n",
      "\t | Epoch: 010 | Batch Id: 00240 | ET: 44.12s | lr: 8.14e-05 | Val. Loss: 0.448 |  Val. Acc: 82.36% | B. Val. Loss: 0.368 |  B. Val. Acc: 84.30%\n",
      "Epoch: 010 | ET: 68.16s | \t Train Loss: 0.177 | Train Acc: 92.28% \t Val. Loss: 0.426 |  Val. Acc: 82.75% \t | B. Val. Loss: 0.368 |  B. Val. Acc: 84.30%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 011 | Batch Id: 00060 | ET: 7.63s | lr: 7.92e-05 | Val. Loss: 0.400 |  Val. Acc: 83.99% | B. Val. Loss: 0.368 |  B. Val. Acc: 84.30%\n",
      "\t | Epoch: 011 | Batch Id: 00120 | ET: 19.80s | lr: 7.69e-05 | Val. Loss: 0.524 |  Val. Acc: 79.99% | B. Val. Loss: 0.368 |  B. Val. Acc: 84.30%\n",
      "\t | Epoch: 011 | Batch Id: 00180 | ET: 31.96s | lr: 7.45e-05 | Val. Loss: 0.486 |  Val. Acc: 81.70% | B. Val. Loss: 0.368 |  B. Val. Acc: 84.30%\n",
      "\t | Epoch: 011 | Batch Id: 00240 | ET: 44.13s | lr: 7.21e-05 | Val. Loss: 0.499 |  Val. Acc: 83.95% | B. Val. Loss: 0.368 |  B. Val. Acc: 84.30%\n",
      "Epoch: 011 | ET: 68.18s | \t Train Loss: 0.099 | Train Acc: 96.16% \t Val. Loss: 0.513 |  Val. Acc: 83.60% \t | B. Val. Loss: 0.368 |  B. Val. Acc: 84.30%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 012 | Batch Id: 00060 | ET: 7.64s | lr: 6.95e-05 | Val. Loss: 0.477 |  Val. Acc: 82.86% | B. Val. Loss: 0.368 |  B. Val. Acc: 84.30%\n",
      "\t | Epoch: 012 | Batch Id: 00120 | ET: 19.80s | lr: 6.69e-05 | Val. Loss: 0.534 |  Val. Acc: 82.44% | B. Val. Loss: 0.368 |  B. Val. Acc: 84.30%\n",
      "\t | Epoch: 012 | Batch Id: 00180 | ET: 31.96s | lr: 6.43e-05 | Val. Loss: 0.504 |  Val. Acc: 82.63% | B. Val. Loss: 0.368 |  B. Val. Acc: 84.30%\n",
      "\t | Epoch: 012 | Batch Id: 00240 | ET: 44.11s | lr: 6.16e-05 | Val. Loss: 0.501 |  Val. Acc: 82.86% | B. Val. Loss: 0.368 |  B. Val. Acc: 84.30%\n",
      "Epoch: 012 | ET: 68.17s | \t Train Loss: 0.075 | Train Acc: 97.49% \t Val. Loss: 0.509 |  Val. Acc: 82.55% \t | B. Val. Loss: 0.368 |  B. Val. Acc: 84.30%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 013 | Batch Id: 00060 | ET: 7.63s | lr: 5.88e-05 | Val. Loss: 0.505 |  Val. Acc: 82.55% | B. Val. Loss: 0.368 |  B. Val. Acc: 84.30%\n",
      "\t | Epoch: 013 | Batch Id: 00120 | ET: 19.78s | lr: 5.61e-05 | Val. Loss: 0.706 |  Val. Acc: 83.87% | B. Val. Loss: 0.368 |  B. Val. Acc: 84.30%\n",
      "\t | Epoch: 013 | Batch Id: 00180 | ET: 31.94s | lr: 5.33e-05 | Val. Loss: 0.793 |  Val. Acc: 83.17% | B. Val. Loss: 0.368 |  B. Val. Acc: 84.30%\n",
      "\t | Epoch: 013 | Batch Id: 00240 | ET: 44.06s | lr: 5.06e-05 | Val. Loss: 0.623 |  Val. Acc: 82.75% | B. Val. Loss: 0.368 |  B. Val. Acc: 84.30%\n",
      "Epoch: 013 | ET: 67.97s | \t Train Loss: 0.037 | Train Acc: 98.88% \t Val. Loss: 0.603 |  Val. Acc: 82.51% \t | B. Val. Loss: 0.368 |  B. Val. Acc: 84.30%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 014 | Batch Id: 00060 | ET: 7.57s | lr: 4.77e-05 | Val. Loss: 0.535 |  Val. Acc: 83.13% | B. Val. Loss: 0.368 |  B. Val. Acc: 84.30%\n",
      "\t | Epoch: 014 | Batch Id: 00120 | ET: 19.63s | lr: 4.50e-05 | Val. Loss: 0.738 |  Val. Acc: 84.53% | B. Val. Loss: 0.368 |  B. Val. Acc: 84.53%\n",
      "\t | Epoch: 014 | Batch Id: 00180 | ET: 31.82s | lr: 4.23e-05 | Val. Loss: 0.755 |  Val. Acc: 82.75% | B. Val. Loss: 0.368 |  B. Val. Acc: 84.53%\n",
      "\t | Epoch: 014 | Batch Id: 00240 | ET: 43.89s | lr: 3.96e-05 | Val. Loss: 0.767 |  Val. Acc: 84.06% | B. Val. Loss: 0.368 |  B. Val. Acc: 84.53%\n",
      "Epoch: 014 | ET: 67.71s | \t Train Loss: 0.016 | Train Acc: 99.56% \t Val. Loss: 0.750 |  Val. Acc: 84.22% \t | B. Val. Loss: 0.368 |  B. Val. Acc: 84.53%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 015 | Batch Id: 00060 | ET: 7.54s | lr: 3.68e-05 | Val. Loss: 0.556 |  Val. Acc: 83.68% | B. Val. Loss: 0.368 |  B. Val. Acc: 84.53%\n",
      "\t | Epoch: 015 | Batch Id: 00120 | ET: 19.56s | lr: 3.42e-05 | Val. Loss: 0.694 |  Val. Acc: 84.14% | B. Val. Loss: 0.368 |  B. Val. Acc: 84.53%\n",
      "\t | Epoch: 015 | Batch Id: 00180 | ET: 31.57s | lr: 3.16e-05 | Val. Loss: 0.750 |  Val. Acc: 83.71% | B. Val. Loss: 0.368 |  B. Val. Acc: 84.53%\n",
      "\t | Epoch: 015 | Batch Id: 00240 | ET: 43.60s | lr: 2.91e-05 | Val. Loss: 0.824 |  Val. Acc: 84.02% | B. Val. Loss: 0.368 |  B. Val. Acc: 84.53%\n",
      "Epoch: 015 | ET: 67.42s | \t Train Loss: 0.002 | Train Acc: 99.99% \t Val. Loss: 0.822 |  Val. Acc: 84.41% \t | B. Val. Loss: 0.368 |  B. Val. Acc: 84.53%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 016 | Batch Id: 00060 | ET: 7.52s | lr: 2.66e-05 | Val. Loss: 0.608 |  Val. Acc: 83.02% | B. Val. Loss: 0.368 |  B. Val. Acc: 84.53%\n",
      "\t | Epoch: 016 | Batch Id: 00120 | ET: 19.54s | lr: 2.42e-05 | Val. Loss: 0.717 |  Val. Acc: 83.68% | B. Val. Loss: 0.368 |  B. Val. Acc: 84.53%\n",
      "\t | Epoch: 016 | Batch Id: 00180 | ET: 31.54s | lr: 2.19e-05 | Val. Loss: 0.797 |  Val. Acc: 83.91% | B. Val. Loss: 0.368 |  B. Val. Acc: 84.53%\n",
      "\t | Epoch: 016 | Batch Id: 00240 | ET: 43.56s | lr: 1.97e-05 | Val. Loss: 0.871 |  Val. Acc: 83.99% | B. Val. Loss: 0.368 |  B. Val. Acc: 84.53%\n",
      "Epoch: 016 | ET: 67.11s | \t Train Loss: 0.001 | Train Acc: 100.00% \t Val. Loss: 0.871 |  Val. Acc: 84.06% \t | B. Val. Loss: 0.368 |  B. Val. Acc: 84.53%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 017 | Batch Id: 00060 | ET: 7.51s | lr: 1.75e-05 | Val. Loss: 0.588 |  Val. Acc: 83.75% | B. Val. Loss: 0.368 |  B. Val. Acc: 84.53%\n",
      "\t | Epoch: 017 | Batch Id: 00120 | ET: 19.56s | lr: 1.55e-05 | Val. Loss: 0.668 |  Val. Acc: 83.99% | B. Val. Loss: 0.368 |  B. Val. Acc: 84.53%\n",
      "\t | Epoch: 017 | Batch Id: 00180 | ET: 31.62s | lr: 1.36e-05 | Val. Loss: 0.727 |  Val. Acc: 83.91% | B. Val. Loss: 0.368 |  B. Val. Acc: 84.53%\n",
      "\t | Epoch: 017 | Batch Id: 00240 | ET: 43.67s | lr: 1.19e-05 | Val. Loss: 0.760 |  Val. Acc: 84.26% | B. Val. Loss: 0.368 |  B. Val. Acc: 84.53%\n",
      "Epoch: 017 | ET: 67.52s | \t Train Loss: 0.002 | Train Acc: 100.00% \t Val. Loss: 0.761 |  Val. Acc: 84.26% \t | B. Val. Loss: 0.368 |  B. Val. Acc: 84.53%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 018 | Batch Id: 00060 | ET: 7.54s | lr: 1.01e-05 | Val. Loss: 0.617 |  Val. Acc: 83.25% | B. Val. Loss: 0.368 |  B. Val. Acc: 84.53%\n",
      "\t | Epoch: 018 | Batch Id: 00120 | ET: 19.59s | lr: 8.61e-06 | Val. Loss: 0.660 |  Val. Acc: 83.87% | B. Val. Loss: 0.368 |  B. Val. Acc: 84.53%\n",
      "\t | Epoch: 018 | Batch Id: 00180 | ET: 31.47s | lr: 7.21e-06 | Val. Loss: 0.696 |  Val. Acc: 83.83% | B. Val. Loss: 0.368 |  B. Val. Acc: 84.53%\n",
      "\t | Epoch: 018 | Batch Id: 00240 | ET: 43.34s | lr: 5.94e-06 | Val. Loss: 0.716 |  Val. Acc: 83.87% | B. Val. Loss: 0.368 |  B. Val. Acc: 84.53%\n",
      "Epoch: 018 | ET: 66.85s | \t Train Loss: 0.003 | Train Acc: 99.97% \t Val. Loss: 0.716 |  Val. Acc: 83.95% \t | B. Val. Loss: 0.368 |  B. Val. Acc: 84.53%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 019 | Batch Id: 00060 | ET: 7.44s | lr: 4.78e-06 | Val. Loss: 0.614 |  Val. Acc: 83.64% | B. Val. Loss: 0.368 |  B. Val. Acc: 84.53%\n",
      "\t | Epoch: 019 | Batch Id: 00120 | ET: 19.45s | lr: 3.79e-06 | Val. Loss: 0.626 |  Val. Acc: 83.79% | B. Val. Loss: 0.368 |  B. Val. Acc: 84.53%\n",
      "\t | Epoch: 019 | Batch Id: 00180 | ET: 31.52s | lr: 2.95e-06 | Val. Loss: 0.644 |  Val. Acc: 83.91% | B. Val. Loss: 0.368 |  B. Val. Acc: 84.53%\n",
      "\t | Epoch: 019 | Batch Id: 00240 | ET: 43.59s | lr: 2.26e-06 | Val. Loss: 0.656 |  Val. Acc: 83.91% | B. Val. Loss: 0.368 |  B. Val. Acc: 84.53%\n",
      "Epoch: 019 | ET: 67.29s | \t Train Loss: 0.003 | Train Acc: 100.00% \t Val. Loss: 0.656 |  Val. Acc: 83.95% \t | B. Val. Loss: 0.368 |  B. Val. Acc: 84.53%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 020 | Batch Id: 00060 | ET: 7.44s | lr: 1.70e-06 | Val. Loss: 0.620 |  Val. Acc: 83.95% | B. Val. Loss: 0.368 |  B. Val. Acc: 84.53%\n",
      "\t | Epoch: 020 | Batch Id: 00120 | ET: 19.31s | lr: 1.32e-06 | Val. Loss: 0.626 |  Val. Acc: 83.83% | B. Val. Loss: 0.368 |  B. Val. Acc: 84.53%\n",
      "\t | Epoch: 020 | Batch Id: 00180 | ET: 31.18s | lr: 1.08e-06 | Val. Loss: 0.633 |  Val. Acc: 83.99% | B. Val. Loss: 0.368 |  B. Val. Acc: 84.53%\n",
      "\t | Epoch: 020 | Batch Id: 00240 | ET: 43.05s | lr: 1.00e-06 | Val. Loss: 0.640 |  Val. Acc: 84.06% | B. Val. Loss: 0.368 |  B. Val. Acc: 84.53%\n",
      "Epoch: 020 | ET: 66.81s | \t Train Loss: 0.004 | Train Acc: 100.00% \t Val. Loss: 0.640 |  Val. Acc: 84.06% \t | B. Val. Loss: 0.368 |  B. Val. Acc: 84.53%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Number of the parameters: 2224577\n",
      "\n",
      "Training\n",
      "-----    2    ----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 001 | Batch Id: 00060 | ET: 7.57s | lr: 1.04e-05 | Val. Loss: 0.693 |  Val. Acc: 50.02% | B. Val. Loss: 0.693 |  B. Val. Acc: 50.02%\n",
      "\t | Epoch: 001 | Batch Id: 00120 | ET: 19.77s | lr: 1.15e-05 | Val. Loss: 0.693 |  Val. Acc: 50.02% | B. Val. Loss: 0.693 |  B. Val. Acc: 50.02%\n",
      "\t | Epoch: 001 | Batch Id: 00180 | ET: 31.85s | lr: 1.33e-05 | Val. Loss: 0.692 |  Val. Acc: 55.37% | B. Val. Loss: 0.692 |  B. Val. Acc: 55.37%\n",
      "\t | Epoch: 001 | Batch Id: 00240 | ET: 44.01s | lr: 1.59e-05 | Val. Loss: 0.692 |  Val. Acc: 49.98% | B. Val. Loss: 0.692 |  B. Val. Acc: 55.37%\n",
      "Epoch: 001 | ET: 68.01s | \t Train Loss: 0.692 | Train Acc: 58.76% \t Val. Loss: 0.692 |  Val. Acc: 59.13% \t | B. Val. Loss: 0.692 |  B. Val. Acc: 59.13%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 002 | Batch Id: 00060 | ET: 7.57s | lr: 1.92e-05 | Val. Loss: 0.674 |  Val. Acc: 58.51% | B. Val. Loss: 0.674 |  B. Val. Acc: 59.13%\n",
      "\t | Epoch: 002 | Batch Id: 00120 | ET: 19.64s | lr: 2.31e-05 | Val. Loss: 0.651 |  Val. Acc: 63.40% | B. Val. Loss: 0.651 |  B. Val. Acc: 63.40%\n",
      "\t | Epoch: 002 | Batch Id: 00180 | ET: 31.80s | lr: 2.74e-05 | Val. Loss: 0.621 |  Val. Acc: 67.12% | B. Val. Loss: 0.621 |  B. Val. Acc: 67.12%\n",
      "\t | Epoch: 002 | Batch Id: 00240 | ET: 43.96s | lr: 3.23e-05 | Val. Loss: 0.532 |  Val. Acc: 74.14% | B. Val. Loss: 0.532 |  B. Val. Acc: 74.14%\n",
      "Epoch: 002 | ET: 68.03s | \t Train Loss: 0.487 | Train Acc: 77.02% \t Val. Loss: 0.534 |  Val. Acc: 74.18% \t | B. Val. Loss: 0.532 |  B. Val. Acc: 74.18%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 003 | Batch Id: 00060 | ET: 7.52s | lr: 3.77e-05 | Val. Loss: 0.522 |  Val. Acc: 74.64% | B. Val. Loss: 0.522 |  B. Val. Acc: 74.64%\n",
      "\t | Epoch: 003 | Batch Id: 00120 | ET: 19.69s | lr: 4.32e-05 | Val. Loss: 0.523 |  Val. Acc: 75.07% | B. Val. Loss: 0.522 |  B. Val. Acc: 75.07%\n",
      "\t | Epoch: 003 | Batch Id: 00180 | ET: 31.84s | lr: 4.89e-05 | Val. Loss: 0.499 |  Val. Acc: 75.30% | B. Val. Loss: 0.499 |  B. Val. Acc: 75.30%\n",
      "\t | Epoch: 003 | Batch Id: 00240 | ET: 44.04s | lr: 5.48e-05 | Val. Loss: 0.479 |  Val. Acc: 77.67% | B. Val. Loss: 0.479 |  B. Val. Acc: 77.67%\n",
      "Epoch: 003 | ET: 67.91s | \t Train Loss: 0.437 | Train Acc: 79.37% \t Val. Loss: 0.487 |  Val. Acc: 76.66% \t | B. Val. Loss: 0.479 |  B. Val. Acc: 77.67%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 004 | Batch Id: 00060 | ET: 7.51s | lr: 6.08e-05 | Val. Loss: 0.485 |  Val. Acc: 76.97% | B. Val. Loss: 0.479 |  B. Val. Acc: 77.67%\n",
      "\t | Epoch: 004 | Batch Id: 00120 | ET: 19.58s | lr: 6.65e-05 | Val. Loss: 0.473 |  Val. Acc: 77.51% | B. Val. Loss: 0.473 |  B. Val. Acc: 77.67%\n",
      "\t | Epoch: 004 | Batch Id: 00180 | ET: 31.65s | lr: 7.21e-05 | Val. Loss: 0.498 |  Val. Acc: 76.70% | B. Val. Loss: 0.473 |  B. Val. Acc: 77.67%\n",
      "\t | Epoch: 004 | Batch Id: 00240 | ET: 43.74s | lr: 7.73e-05 | Val. Loss: 0.461 |  Val. Acc: 78.56% | B. Val. Loss: 0.461 |  B. Val. Acc: 78.56%\n",
      "Epoch: 004 | ET: 67.89s | \t Train Loss: 0.408 | Train Acc: 81.63% \t Val. Loss: 0.460 |  Val. Acc: 78.71% \t | B. Val. Loss: 0.460 |  B. Val. Acc: 78.71%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 005 | Batch Id: 00060 | ET: 7.52s | lr: 8.23e-05 | Val. Loss: 0.463 |  Val. Acc: 78.21% | B. Val. Loss: 0.460 |  B. Val. Acc: 78.71%\n",
      "\t | Epoch: 005 | Batch Id: 00120 | ET: 19.51s | lr: 8.67e-05 | Val. Loss: 0.459 |  Val. Acc: 78.75% | B. Val. Loss: 0.459 |  B. Val. Acc: 78.75%\n",
      "\t | Epoch: 005 | Batch Id: 00180 | ET: 31.64s | lr: 9.06e-05 | Val. Loss: 0.459 |  Val. Acc: 78.32% | B. Val. Loss: 0.459 |  B. Val. Acc: 78.75%\n",
      "\t | Epoch: 005 | Batch Id: 00240 | ET: 43.74s | lr: 9.39e-05 | Val. Loss: 0.448 |  Val. Acc: 79.18% | B. Val. Loss: 0.448 |  B. Val. Acc: 79.18%\n",
      "Epoch: 005 | ET: 68.04s | \t Train Loss: 0.382 | Train Acc: 83.03% \t Val. Loss: 0.449 |  Val. Acc: 79.26% \t | B. Val. Loss: 0.448 |  B. Val. Acc: 79.26%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 006 | Batch Id: 00060 | ET: 7.61s | lr: 9.66e-05 | Val. Loss: 0.452 |  Val. Acc: 78.98% | B. Val. Loss: 0.448 |  B. Val. Acc: 79.26%\n",
      "\t | Epoch: 006 | Batch Id: 00120 | ET: 19.74s | lr: 9.84e-05 | Val. Loss: 0.455 |  Val. Acc: 79.02% | B. Val. Loss: 0.448 |  B. Val. Acc: 79.26%\n",
      "\t | Epoch: 006 | Batch Id: 00180 | ET: 31.80s | lr: 9.96e-05 | Val. Loss: 0.459 |  Val. Acc: 79.60% | B. Val. Loss: 0.448 |  B. Val. Acc: 79.60%\n",
      "\t | Epoch: 006 | Batch Id: 00240 | ET: 44.00s | lr: 1.00e-04 | Val. Loss: 0.458 |  Val. Acc: 78.56% | B. Val. Loss: 0.448 |  B. Val. Acc: 79.60%\n",
      "Epoch: 006 | ET: 67.96s | \t Train Loss: 0.369 | Train Acc: 84.45% \t Val. Loss: 0.446 |  Val. Acc: 78.91% \t | B. Val. Loss: 0.446 |  B. Val. Acc: 79.60%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 007 | Batch Id: 00060 | ET: 7.56s | lr: 9.99e-05 | Val. Loss: 0.463 |  Val. Acc: 79.29% | B. Val. Loss: 0.446 |  B. Val. Acc: 79.60%\n",
      "\t | Epoch: 007 | Batch Id: 00120 | ET: 19.71s | lr: 9.97e-05 | Val. Loss: 0.462 |  Val. Acc: 80.15% | B. Val. Loss: 0.446 |  B. Val. Acc: 80.15%\n",
      "\t | Epoch: 007 | Batch Id: 00180 | ET: 32.00s | lr: 9.93e-05 | Val. Loss: 0.447 |  Val. Acc: 79.22% | B. Val. Loss: 0.446 |  B. Val. Acc: 80.15%\n",
      "\t | Epoch: 007 | Batch Id: 00240 | ET: 44.16s | lr: 9.88e-05 | Val. Loss: 0.444 |  Val. Acc: 79.76% | B. Val. Loss: 0.444 |  B. Val. Acc: 80.15%\n",
      "Epoch: 007 | ET: 68.22s | \t Train Loss: 0.329 | Train Acc: 85.54% \t Val. Loss: 0.439 |  Val. Acc: 79.91% \t | B. Val. Loss: 0.439 |  B. Val. Acc: 80.15%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 008 | Batch Id: 00060 | ET: 7.61s | lr: 9.81e-05 | Val. Loss: 0.473 |  Val. Acc: 78.21% | B. Val. Loss: 0.439 |  B. Val. Acc: 80.15%\n",
      "\t | Epoch: 008 | Batch Id: 00120 | ET: 19.77s | lr: 9.72e-05 | Val. Loss: 0.491 |  Val. Acc: 79.57% | B. Val. Loss: 0.439 |  B. Val. Acc: 80.15%\n",
      "\t | Epoch: 008 | Batch Id: 00180 | ET: 31.94s | lr: 9.63e-05 | Val. Loss: 0.440 |  Val. Acc: 79.76% | B. Val. Loss: 0.439 |  B. Val. Acc: 80.15%\n",
      "\t | Epoch: 008 | Batch Id: 00240 | ET: 44.12s | lr: 9.51e-05 | Val. Loss: 0.425 |  Val. Acc: 81.31% | B. Val. Loss: 0.425 |  B. Val. Acc: 81.31%\n",
      "Epoch: 008 | ET: 68.24s | \t Train Loss: 0.287 | Train Acc: 87.23% \t Val. Loss: 0.436 |  Val. Acc: 81.08% \t | B. Val. Loss: 0.425 |  B. Val. Acc: 81.31%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 009 | Batch Id: 00060 | ET: 7.56s | lr: 9.38e-05 | Val. Loss: 0.454 |  Val. Acc: 81.00% | B. Val. Loss: 0.425 |  B. Val. Acc: 81.31%\n",
      "\t | Epoch: 009 | Batch Id: 00120 | ET: 19.63s | lr: 9.24e-05 | Val. Loss: 0.449 |  Val. Acc: 80.96% | B. Val. Loss: 0.425 |  B. Val. Acc: 81.31%\n",
      "\t | Epoch: 009 | Batch Id: 00180 | ET: 31.71s | lr: 9.09e-05 | Val. Loss: 0.432 |  Val. Acc: 79.84% | B. Val. Loss: 0.425 |  B. Val. Acc: 81.31%\n",
      "\t | Epoch: 009 | Batch Id: 00240 | ET: 43.77s | lr: 8.93e-05 | Val. Loss: 0.459 |  Val. Acc: 79.60% | B. Val. Loss: 0.425 |  B. Val. Acc: 81.31%\n",
      "Epoch: 009 | ET: 67.85s | \t Train Loss: 0.270 | Train Acc: 88.87% \t Val. Loss: 0.460 |  Val. Acc: 79.06% \t | B. Val. Loss: 0.425 |  B. Val. Acc: 81.31%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 010 | Batch Id: 00060 | ET: 7.61s | lr: 8.74e-05 | Val. Loss: 0.482 |  Val. Acc: 79.95% | B. Val. Loss: 0.425 |  B. Val. Acc: 81.31%\n",
      "\t | Epoch: 010 | Batch Id: 00120 | ET: 19.76s | lr: 8.55e-05 | Val. Loss: 0.475 |  Val. Acc: 81.27% | B. Val. Loss: 0.425 |  B. Val. Acc: 81.31%\n",
      "\t | Epoch: 010 | Batch Id: 00180 | ET: 31.92s | lr: 8.35e-05 | Val. Loss: 0.469 |  Val. Acc: 80.30% | B. Val. Loss: 0.425 |  B. Val. Acc: 81.31%\n",
      "\t | Epoch: 010 | Batch Id: 00240 | ET: 44.08s | lr: 8.14e-05 | Val. Loss: 0.465 |  Val. Acc: 79.95% | B. Val. Loss: 0.425 |  B. Val. Acc: 81.31%\n",
      "Epoch: 010 | ET: 68.15s | \t Train Loss: 0.233 | Train Acc: 89.85% \t Val. Loss: 0.468 |  Val. Acc: 81.27% \t | B. Val. Loss: 0.425 |  B. Val. Acc: 81.31%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 011 | Batch Id: 00060 | ET: 7.60s | lr: 7.92e-05 | Val. Loss: 0.495 |  Val. Acc: 79.91% | B. Val. Loss: 0.425 |  B. Val. Acc: 81.31%\n",
      "\t | Epoch: 011 | Batch Id: 00120 | ET: 19.73s | lr: 7.69e-05 | Val. Loss: 0.478 |  Val. Acc: 81.62% | B. Val. Loss: 0.425 |  B. Val. Acc: 81.62%\n",
      "\t | Epoch: 011 | Batch Id: 00180 | ET: 31.93s | lr: 7.45e-05 | Val. Loss: 0.581 |  Val. Acc: 77.70% | B. Val. Loss: 0.425 |  B. Val. Acc: 81.62%\n",
      "\t | Epoch: 011 | Batch Id: 00240 | ET: 43.99s | lr: 7.21e-05 | Val. Loss: 0.510 |  Val. Acc: 77.08% | B. Val. Loss: 0.425 |  B. Val. Acc: 81.62%\n",
      "Epoch: 011 | ET: 68.04s | \t Train Loss: 0.176 | Train Acc: 94.09% \t Val. Loss: 0.493 |  Val. Acc: 76.85% \t | B. Val. Loss: 0.425 |  B. Val. Acc: 81.62%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 012 | Batch Id: 00060 | ET: 7.61s | lr: 6.95e-05 | Val. Loss: 0.522 |  Val. Acc: 79.68% | B. Val. Loss: 0.425 |  B. Val. Acc: 81.62%\n",
      "\t | Epoch: 012 | Batch Id: 00120 | ET: 19.76s | lr: 6.69e-05 | Val. Loss: 0.514 |  Val. Acc: 81.27% | B. Val. Loss: 0.425 |  B. Val. Acc: 81.62%\n",
      "\t | Epoch: 012 | Batch Id: 00180 | ET: 31.92s | lr: 6.43e-05 | Val. Loss: 0.576 |  Val. Acc: 79.72% | B. Val. Loss: 0.425 |  B. Val. Acc: 81.62%\n",
      "\t | Epoch: 012 | Batch Id: 00240 | ET: 44.08s | lr: 6.16e-05 | Val. Loss: 0.593 |  Val. Acc: 80.38% | B. Val. Loss: 0.425 |  B. Val. Acc: 81.62%\n",
      "Epoch: 012 | ET: 68.16s | \t Train Loss: 0.080 | Train Acc: 97.30% \t Val. Loss: 0.575 |  Val. Acc: 80.11% \t | B. Val. Loss: 0.425 |  B. Val. Acc: 81.62%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 013 | Batch Id: 00060 | ET: 7.60s | lr: 5.88e-05 | Val. Loss: 0.556 |  Val. Acc: 79.45% | B. Val. Loss: 0.425 |  B. Val. Acc: 81.62%\n",
      "\t | Epoch: 013 | Batch Id: 00120 | ET: 19.68s | lr: 5.61e-05 | Val. Loss: 0.751 |  Val. Acc: 80.30% | B. Val. Loss: 0.425 |  B. Val. Acc: 81.62%\n",
      "\t | Epoch: 013 | Batch Id: 00180 | ET: 31.76s | lr: 5.33e-05 | Val. Loss: 0.688 |  Val. Acc: 80.42% | B. Val. Loss: 0.425 |  B. Val. Acc: 81.62%\n",
      "\t | Epoch: 013 | Batch Id: 00240 | ET: 43.83s | lr: 5.06e-05 | Val. Loss: 0.686 |  Val. Acc: 80.34% | B. Val. Loss: 0.425 |  B. Val. Acc: 81.62%\n",
      "Epoch: 013 | ET: 67.75s | \t Train Loss: 0.060 | Train Acc: 98.01% \t Val. Loss: 0.680 |  Val. Acc: 80.73% \t | B. Val. Loss: 0.425 |  B. Val. Acc: 81.62%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 014 | Batch Id: 00060 | ET: 7.56s | lr: 4.77e-05 | Val. Loss: 0.606 |  Val. Acc: 78.98% | B. Val. Loss: 0.425 |  B. Val. Acc: 81.62%\n",
      "\t | Epoch: 014 | Batch Id: 00120 | ET: 19.63s | lr: 4.50e-05 | Val. Loss: 0.806 |  Val. Acc: 80.07% | B. Val. Loss: 0.425 |  B. Val. Acc: 81.62%\n",
      "\t | Epoch: 014 | Batch Id: 00180 | ET: 31.63s | lr: 4.23e-05 | Val. Loss: 0.918 |  Val. Acc: 79.68% | B. Val. Loss: 0.425 |  B. Val. Acc: 81.62%\n",
      "\t | Epoch: 014 | Batch Id: 00240 | ET: 43.65s | lr: 3.96e-05 | Val. Loss: 1.033 |  Val. Acc: 76.00% | B. Val. Loss: 0.425 |  B. Val. Acc: 81.62%\n",
      "Epoch: 014 | ET: 67.60s | \t Train Loss: 0.046 | Train Acc: 98.44% \t Val. Loss: 0.902 |  Val. Acc: 77.90% \t | B. Val. Loss: 0.425 |  B. Val. Acc: 81.62%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 015 | Batch Id: 00060 | ET: 7.59s | lr: 3.68e-05 | Val. Loss: 0.662 |  Val. Acc: 79.57% | B. Val. Loss: 0.425 |  B. Val. Acc: 81.62%\n",
      "\t | Epoch: 015 | Batch Id: 00120 | ET: 19.76s | lr: 3.42e-05 | Val. Loss: 0.838 |  Val. Acc: 80.81% | B. Val. Loss: 0.425 |  B. Val. Acc: 81.62%\n",
      "\t | Epoch: 015 | Batch Id: 00180 | ET: 31.93s | lr: 3.16e-05 | Val. Loss: 0.937 |  Val. Acc: 80.42% | B. Val. Loss: 0.425 |  B. Val. Acc: 81.62%\n",
      "\t | Epoch: 015 | Batch Id: 00240 | ET: 44.08s | lr: 2.91e-05 | Val. Loss: 1.000 |  Val. Acc: 79.99% | B. Val. Loss: 0.425 |  B. Val. Acc: 81.62%\n",
      "Epoch: 015 | ET: 68.00s | \t Train Loss: 0.003 | Train Acc: 99.97% \t Val. Loss: 1.007 |  Val. Acc: 79.99% \t | B. Val. Loss: 0.425 |  B. Val. Acc: 81.62%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 016 | Batch Id: 00060 | ET: 7.65s | lr: 2.66e-05 | Val. Loss: 0.705 |  Val. Acc: 79.22% | B. Val. Loss: 0.425 |  B. Val. Acc: 81.62%\n",
      "\t | Epoch: 016 | Batch Id: 00120 | ET: 19.84s | lr: 2.42e-05 | Val. Loss: 0.847 |  Val. Acc: 79.88% | B. Val. Loss: 0.425 |  B. Val. Acc: 81.62%\n",
      "\t | Epoch: 016 | Batch Id: 00180 | ET: 32.03s | lr: 2.19e-05 | Val. Loss: 0.943 |  Val. Acc: 80.42% | B. Val. Loss: 0.425 |  B. Val. Acc: 81.62%\n",
      "\t | Epoch: 016 | Batch Id: 00240 | ET: 44.10s | lr: 1.97e-05 | Val. Loss: 1.003 |  Val. Acc: 80.07% | B. Val. Loss: 0.425 |  B. Val. Acc: 81.62%\n",
      "Epoch: 016 | ET: 67.95s | \t Train Loss: 0.002 | Train Acc: 100.00% \t Val. Loss: 1.002 |  Val. Acc: 80.19% \t | B. Val. Loss: 0.425 |  B. Val. Acc: 81.62%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 017 | Batch Id: 00060 | ET: 7.56s | lr: 1.75e-05 | Val. Loss: 0.730 |  Val. Acc: 79.88% | B. Val. Loss: 0.425 |  B. Val. Acc: 81.62%\n",
      "\t | Epoch: 017 | Batch Id: 00120 | ET: 19.63s | lr: 1.55e-05 | Val. Loss: 0.820 |  Val. Acc: 80.96% | B. Val. Loss: 0.425 |  B. Val. Acc: 81.62%\n",
      "\t | Epoch: 017 | Batch Id: 00180 | ET: 31.74s | lr: 1.36e-05 | Val. Loss: 0.886 |  Val. Acc: 80.34% | B. Val. Loss: 0.425 |  B. Val. Acc: 81.62%\n",
      "\t | Epoch: 017 | Batch Id: 00240 | ET: 43.91s | lr: 1.19e-05 | Val. Loss: 0.925 |  Val. Acc: 80.73% | B. Val. Loss: 0.425 |  B. Val. Acc: 81.62%\n",
      "Epoch: 017 | ET: 67.79s | \t Train Loss: 0.003 | Train Acc: 99.99% \t Val. Loss: 0.927 |  Val. Acc: 80.69% \t | B. Val. Loss: 0.425 |  B. Val. Acc: 81.62%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 018 | Batch Id: 00060 | ET: 7.54s | lr: 1.01e-05 | Val. Loss: 0.747 |  Val. Acc: 80.15% | B. Val. Loss: 0.425 |  B. Val. Acc: 81.62%\n",
      "\t | Epoch: 018 | Batch Id: 00120 | ET: 19.68s | lr: 8.61e-06 | Val. Loss: 0.780 |  Val. Acc: 80.03% | B. Val. Loss: 0.425 |  B. Val. Acc: 81.62%\n",
      "\t | Epoch: 018 | Batch Id: 00180 | ET: 31.85s | lr: 7.21e-06 | Val. Loss: 0.820 |  Val. Acc: 80.15% | B. Val. Loss: 0.425 |  B. Val. Acc: 81.62%\n",
      "\t | Epoch: 018 | Batch Id: 00240 | ET: 44.01s | lr: 5.94e-06 | Val. Loss: 0.852 |  Val. Acc: 80.03% | B. Val. Loss: 0.425 |  B. Val. Acc: 81.62%\n",
      "Epoch: 018 | ET: 68.09s | \t Train Loss: 0.003 | Train Acc: 100.00% \t Val. Loss: 0.853 |  Val. Acc: 80.03% \t | B. Val. Loss: 0.425 |  B. Val. Acc: 81.62%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 019 | Batch Id: 00060 | ET: 7.62s | lr: 4.78e-06 | Val. Loss: 0.736 |  Val. Acc: 79.72% | B. Val. Loss: 0.425 |  B. Val. Acc: 81.62%\n",
      "\t | Epoch: 019 | Batch Id: 00120 | ET: 19.77s | lr: 3.79e-06 | Val. Loss: 0.753 |  Val. Acc: 79.80% | B. Val. Loss: 0.425 |  B. Val. Acc: 81.62%\n",
      "\t | Epoch: 019 | Batch Id: 00180 | ET: 31.96s | lr: 2.95e-06 | Val. Loss: 0.773 |  Val. Acc: 79.91% | B. Val. Loss: 0.425 |  B. Val. Acc: 81.62%\n",
      "\t | Epoch: 019 | Batch Id: 00240 | ET: 44.12s | lr: 2.26e-06 | Val. Loss: 0.789 |  Val. Acc: 80.03% | B. Val. Loss: 0.425 |  B. Val. Acc: 81.62%\n",
      "Epoch: 019 | ET: 68.19s | \t Train Loss: 0.004 | Train Acc: 100.00% \t Val. Loss: 0.789 |  Val. Acc: 80.03% \t | B. Val. Loss: 0.425 |  B. Val. Acc: 81.62%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 020 | Batch Id: 00060 | ET: 7.62s | lr: 1.70e-06 | Val. Loss: 0.750 |  Val. Acc: 79.29% | B. Val. Loss: 0.425 |  B. Val. Acc: 81.62%\n",
      "\t | Epoch: 020 | Batch Id: 00120 | ET: 19.78s | lr: 1.32e-06 | Val. Loss: 0.759 |  Val. Acc: 79.37% | B. Val. Loss: 0.425 |  B. Val. Acc: 81.62%\n",
      "\t | Epoch: 020 | Batch Id: 00180 | ET: 31.95s | lr: 1.08e-06 | Val. Loss: 0.768 |  Val. Acc: 79.37% | B. Val. Loss: 0.425 |  B. Val. Acc: 81.62%\n",
      "\t | Epoch: 020 | Batch Id: 00240 | ET: 44.11s | lr: 1.00e-06 | Val. Loss: 0.775 |  Val. Acc: 79.45% | B. Val. Loss: 0.425 |  B. Val. Acc: 81.62%\n",
      "Epoch: 020 | ET: 67.99s | \t Train Loss: 0.005 | Train Acc: 100.00% \t Val. Loss: 0.776 |  Val. Acc: 79.45% \t | B. Val. Loss: 0.425 |  B. Val. Acc: 81.62%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Number of the parameters: 2224577\n",
      "\n",
      "Training\n",
      "-----    3    ----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 001 | Batch Id: 00060 | ET: 7.57s | lr: 1.04e-05 | Val. Loss: 0.697 |  Val. Acc: 50.02% | B. Val. Loss: 0.697 |  B. Val. Acc: 50.02%\n",
      "\t | Epoch: 001 | Batch Id: 00120 | ET: 19.81s | lr: 1.15e-05 | Val. Loss: 0.693 |  Val. Acc: 50.21% | B. Val. Loss: 0.693 |  B. Val. Acc: 50.21%\n",
      "\t | Epoch: 001 | Batch Id: 00180 | ET: 32.11s | lr: 1.33e-05 | Val. Loss: 0.693 |  Val. Acc: 49.98% | B. Val. Loss: 0.693 |  B. Val. Acc: 50.21%\n",
      "\t | Epoch: 001 | Batch Id: 00240 | ET: 44.28s | lr: 1.59e-05 | Val. Loss: 0.694 |  Val. Acc: 50.02% | B. Val. Loss: 0.693 |  B. Val. Acc: 50.21%\n",
      "Epoch: 001 | ET: 68.35s | \t Train Loss: 0.695 | Train Acc: 49.99% \t Val. Loss: 0.695 |  Val. Acc: 50.02% \t | B. Val. Loss: 0.693 |  B. Val. Acc: 50.21%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 002 | Batch Id: 00060 | ET: 7.63s | lr: 1.92e-05 | Val. Loss: 0.671 |  Val. Acc: 61.19% | B. Val. Loss: 0.671 |  B. Val. Acc: 61.19%\n",
      "\t | Epoch: 002 | Batch Id: 00120 | ET: 19.92s | lr: 2.31e-05 | Val. Loss: 0.663 |  Val. Acc: 61.15% | B. Val. Loss: 0.663 |  B. Val. Acc: 61.19%\n",
      "\t | Epoch: 002 | Batch Id: 00180 | ET: 32.10s | lr: 2.74e-05 | Val. Loss: 0.661 |  Val. Acc: 60.76% | B. Val. Loss: 0.661 |  B. Val. Acc: 61.19%\n",
      "\t | Epoch: 002 | Batch Id: 00240 | ET: 44.27s | lr: 3.23e-05 | Val. Loss: 0.702 |  Val. Acc: 59.95% | B. Val. Loss: 0.661 |  B. Val. Acc: 61.19%\n",
      "Epoch: 002 | ET: 68.33s | \t Train Loss: 0.687 | Train Acc: 61.19% \t Val. Loss: 0.710 |  Val. Acc: 59.48% \t | B. Val. Loss: 0.661 |  B. Val. Acc: 61.19%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 003 | Batch Id: 00060 | ET: 7.64s | lr: 3.77e-05 | Val. Loss: 0.654 |  Val. Acc: 62.97% | B. Val. Loss: 0.654 |  B. Val. Acc: 62.97%\n",
      "\t | Epoch: 003 | Batch Id: 00120 | ET: 19.92s | lr: 4.32e-05 | Val. Loss: 0.679 |  Val. Acc: 59.33% | B. Val. Loss: 0.654 |  B. Val. Acc: 62.97%\n",
      "\t | Epoch: 003 | Batch Id: 00180 | ET: 32.09s | lr: 4.89e-05 | Val. Loss: 0.624 |  Val. Acc: 65.99% | B. Val. Loss: 0.624 |  B. Val. Acc: 65.99%\n",
      "\t | Epoch: 003 | Batch Id: 00240 | ET: 44.37s | lr: 5.48e-05 | Val. Loss: 0.607 |  Val. Acc: 69.25% | B. Val. Loss: 0.607 |  B. Val. Acc: 69.25%\n",
      "Epoch: 003 | ET: 68.64s | \t Train Loss: 0.489 | Train Acc: 76.75% \t Val. Loss: 0.539 |  Val. Acc: 73.40% \t | B. Val. Loss: 0.539 |  B. Val. Acc: 73.40%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 004 | Batch Id: 00060 | ET: 7.63s | lr: 6.08e-05 | Val. Loss: 0.526 |  Val. Acc: 72.94% | B. Val. Loss: 0.526 |  B. Val. Acc: 73.40%\n",
      "\t | Epoch: 004 | Batch Id: 00120 | ET: 19.81s | lr: 6.65e-05 | Val. Loss: 0.492 |  Val. Acc: 77.32% | B. Val. Loss: 0.492 |  B. Val. Acc: 77.32%\n",
      "\t | Epoch: 004 | Batch Id: 00180 | ET: 32.10s | lr: 7.21e-05 | Val. Loss: 0.496 |  Val. Acc: 77.36% | B. Val. Loss: 0.492 |  B. Val. Acc: 77.36%\n",
      "\t | Epoch: 004 | Batch Id: 00240 | ET: 44.38s | lr: 7.73e-05 | Val. Loss: 0.474 |  Val. Acc: 78.87% | B. Val. Loss: 0.474 |  B. Val. Acc: 78.87%\n",
      "Epoch: 004 | ET: 68.55s | \t Train Loss: 0.431 | Train Acc: 80.15% \t Val. Loss: 0.488 |  Val. Acc: 78.29% \t | B. Val. Loss: 0.474 |  B. Val. Acc: 78.87%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 005 | Batch Id: 00060 | ET: 7.60s | lr: 8.23e-05 | Val. Loss: 0.467 |  Val. Acc: 78.44% | B. Val. Loss: 0.467 |  B. Val. Acc: 78.87%\n",
      "\t | Epoch: 005 | Batch Id: 00120 | ET: 19.73s | lr: 8.67e-05 | Val. Loss: 0.479 |  Val. Acc: 76.89% | B. Val. Loss: 0.467 |  B. Val. Acc: 78.87%\n",
      "\t | Epoch: 005 | Batch Id: 00180 | ET: 31.89s | lr: 9.06e-05 | Val. Loss: 0.471 |  Val. Acc: 78.21% | B. Val. Loss: 0.467 |  B. Val. Acc: 78.87%\n",
      "\t | Epoch: 005 | Batch Id: 00240 | ET: 44.07s | lr: 9.39e-05 | Val. Loss: 0.444 |  Val. Acc: 79.37% | B. Val. Loss: 0.444 |  B. Val. Acc: 79.37%\n",
      "Epoch: 005 | ET: 68.16s | \t Train Loss: 0.383 | Train Acc: 82.69% \t Val. Loss: 0.458 |  Val. Acc: 79.14% \t | B. Val. Loss: 0.444 |  B. Val. Acc: 79.37%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 006 | Batch Id: 00060 | ET: 7.54s | lr: 9.66e-05 | Val. Loss: 0.456 |  Val. Acc: 78.60% | B. Val. Loss: 0.444 |  B. Val. Acc: 79.37%\n",
      "\t | Epoch: 006 | Batch Id: 00120 | ET: 19.52s | lr: 9.84e-05 | Val. Loss: 0.445 |  Val. Acc: 79.18% | B. Val. Loss: 0.444 |  B. Val. Acc: 79.37%\n",
      "\t | Epoch: 006 | Batch Id: 00180 | ET: 31.51s | lr: 9.96e-05 | Val. Loss: 0.584 |  Val. Acc: 74.68% | B. Val. Loss: 0.444 |  B. Val. Acc: 79.37%\n",
      "\t | Epoch: 006 | Batch Id: 00240 | ET: 43.59s | lr: 1.00e-04 | Val. Loss: 0.439 |  Val. Acc: 80.50% | B. Val. Loss: 0.439 |  B. Val. Acc: 80.50%\n",
      "Epoch: 006 | ET: 67.42s | \t Train Loss: 0.352 | Train Acc: 84.36% \t Val. Loss: 0.433 |  Val. Acc: 79.72% \t | B. Val. Loss: 0.433 |  B. Val. Acc: 80.50%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 007 | Batch Id: 00060 | ET: 7.61s | lr: 9.99e-05 | Val. Loss: 0.436 |  Val. Acc: 80.46% | B. Val. Loss: 0.433 |  B. Val. Acc: 80.50%\n",
      "\t | Epoch: 007 | Batch Id: 00120 | ET: 19.75s | lr: 9.97e-05 | Val. Loss: 0.422 |  Val. Acc: 80.81% | B. Val. Loss: 0.422 |  B. Val. Acc: 80.81%\n",
      "\t | Epoch: 007 | Batch Id: 00180 | ET: 32.01s | lr: 9.93e-05 | Val. Loss: 0.427 |  Val. Acc: 80.96% | B. Val. Loss: 0.422 |  B. Val. Acc: 80.96%\n",
      "\t | Epoch: 007 | Batch Id: 00240 | ET: 44.26s | lr: 9.88e-05 | Val. Loss: 0.429 |  Val. Acc: 81.04% | B. Val. Loss: 0.422 |  B. Val. Acc: 81.04%\n",
      "Epoch: 007 | ET: 68.21s | \t Train Loss: 0.309 | Train Acc: 86.92% \t Val. Loss: 0.429 |  Val. Acc: 80.61% \t | B. Val. Loss: 0.422 |  B. Val. Acc: 81.04%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 008 | Batch Id: 00060 | ET: 7.53s | lr: 9.81e-05 | Val. Loss: 0.440 |  Val. Acc: 80.30% | B. Val. Loss: 0.422 |  B. Val. Acc: 81.04%\n",
      "\t | Epoch: 008 | Batch Id: 00120 | ET: 19.60s | lr: 9.72e-05 | Val. Loss: 0.426 |  Val. Acc: 80.11% | B. Val. Loss: 0.422 |  B. Val. Acc: 81.04%\n",
      "\t | Epoch: 008 | Batch Id: 00180 | ET: 31.75s | lr: 9.63e-05 | Val. Loss: 0.400 |  Val. Acc: 81.97% | B. Val. Loss: 0.400 |  B. Val. Acc: 81.97%\n",
      "\t | Epoch: 008 | Batch Id: 00240 | ET: 44.02s | lr: 9.51e-05 | Val. Loss: 0.395 |  Val. Acc: 82.55% | B. Val. Loss: 0.395 |  B. Val. Acc: 82.55%\n",
      "Epoch: 008 | ET: 68.17s | \t Train Loss: 0.263 | Train Acc: 89.40% \t Val. Loss: 0.404 |  Val. Acc: 81.70% \t | B. Val. Loss: 0.395 |  B. Val. Acc: 82.55%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 009 | Batch Id: 00060 | ET: 7.63s | lr: 9.38e-05 | Val. Loss: 0.422 |  Val. Acc: 82.20% | B. Val. Loss: 0.395 |  B. Val. Acc: 82.55%\n",
      "\t | Epoch: 009 | Batch Id: 00120 | ET: 19.81s | lr: 9.24e-05 | Val. Loss: 0.421 |  Val. Acc: 82.24% | B. Val. Loss: 0.395 |  B. Val. Acc: 82.55%\n",
      "\t | Epoch: 009 | Batch Id: 00180 | ET: 31.97s | lr: 9.09e-05 | Val. Loss: 0.440 |  Val. Acc: 82.67% | B. Val. Loss: 0.395 |  B. Val. Acc: 82.67%\n",
      "\t | Epoch: 009 | Batch Id: 00240 | ET: 44.29s | lr: 8.93e-05 | Val. Loss: 0.437 |  Val. Acc: 82.55% | B. Val. Loss: 0.395 |  B. Val. Acc: 82.67%\n",
      "Epoch: 009 | ET: 68.22s | \t Train Loss: 0.228 | Train Acc: 90.66% \t Val. Loss: 0.480 |  Val. Acc: 81.35% \t | B. Val. Loss: 0.395 |  B. Val. Acc: 82.67%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 010 | Batch Id: 00060 | ET: 7.64s | lr: 8.74e-05 | Val. Loss: 0.446 |  Val. Acc: 81.50% | B. Val. Loss: 0.395 |  B. Val. Acc: 82.67%\n",
      "\t | Epoch: 010 | Batch Id: 00120 | ET: 19.81s | lr: 8.55e-05 | Val. Loss: 0.463 |  Val. Acc: 79.68% | B. Val. Loss: 0.395 |  B. Val. Acc: 82.67%\n",
      "\t | Epoch: 010 | Batch Id: 00180 | ET: 31.98s | lr: 8.35e-05 | Val. Loss: 0.441 |  Val. Acc: 82.55% | B. Val. Loss: 0.395 |  B. Val. Acc: 82.67%\n",
      "\t | Epoch: 010 | Batch Id: 00240 | ET: 44.15s | lr: 8.14e-05 | Val. Loss: 0.428 |  Val. Acc: 81.74% | B. Val. Loss: 0.395 |  B. Val. Acc: 82.67%\n",
      "Epoch: 010 | ET: 68.21s | \t Train Loss: 0.165 | Train Acc: 94.22% \t Val. Loss: 0.426 |  Val. Acc: 81.85% \t | B. Val. Loss: 0.395 |  B. Val. Acc: 82.67%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 011 | Batch Id: 00060 | ET: 7.63s | lr: 7.92e-05 | Val. Loss: 0.476 |  Val. Acc: 81.39% | B. Val. Loss: 0.395 |  B. Val. Acc: 82.67%\n",
      "\t | Epoch: 011 | Batch Id: 00120 | ET: 19.81s | lr: 7.69e-05 | Val. Loss: 0.467 |  Val. Acc: 82.20% | B. Val. Loss: 0.395 |  B. Val. Acc: 82.67%\n",
      "\t | Epoch: 011 | Batch Id: 00180 | ET: 31.98s | lr: 7.45e-05 | Val. Loss: 0.445 |  Val. Acc: 81.74% | B. Val. Loss: 0.395 |  B. Val. Acc: 82.67%\n",
      "\t | Epoch: 011 | Batch Id: 00240 | ET: 44.16s | lr: 7.21e-05 | Val. Loss: 0.510 |  Val. Acc: 81.58% | B. Val. Loss: 0.395 |  B. Val. Acc: 82.67%\n",
      "Epoch: 011 | ET: 68.24s | \t Train Loss: 0.094 | Train Acc: 96.98% \t Val. Loss: 0.509 |  Val. Acc: 81.70% \t | B. Val. Loss: 0.395 |  B. Val. Acc: 82.67%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 012 | Batch Id: 00060 | ET: 7.63s | lr: 6.95e-05 | Val. Loss: 0.518 |  Val. Acc: 81.70% | B. Val. Loss: 0.395 |  B. Val. Acc: 82.67%\n",
      "\t | Epoch: 012 | Batch Id: 00120 | ET: 19.80s | lr: 6.69e-05 | Val. Loss: 0.674 |  Val. Acc: 81.70% | B. Val. Loss: 0.395 |  B. Val. Acc: 82.67%\n",
      "\t | Epoch: 012 | Batch Id: 00180 | ET: 31.88s | lr: 6.43e-05 | Val. Loss: 0.628 |  Val. Acc: 81.19% | B. Val. Loss: 0.395 |  B. Val. Acc: 82.67%\n",
      "\t | Epoch: 012 | Batch Id: 00240 | ET: 44.04s | lr: 6.16e-05 | Val. Loss: 0.601 |  Val. Acc: 82.16% | B. Val. Loss: 0.395 |  B. Val. Acc: 82.67%\n",
      "Epoch: 012 | ET: 67.98s | \t Train Loss: 0.075 | Train Acc: 97.51% \t Val. Loss: 0.603 |  Val. Acc: 82.09% \t | B. Val. Loss: 0.395 |  B. Val. Acc: 82.67%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 013 | Batch Id: 00060 | ET: 7.63s | lr: 5.88e-05 | Val. Loss: 0.528 |  Val. Acc: 81.19% | B. Val. Loss: 0.395 |  B. Val. Acc: 82.67%\n",
      "\t | Epoch: 013 | Batch Id: 00120 | ET: 19.80s | lr: 5.61e-05 | Val. Loss: 0.725 |  Val. Acc: 81.62% | B. Val. Loss: 0.395 |  B. Val. Acc: 82.67%\n",
      "\t | Epoch: 013 | Batch Id: 00180 | ET: 31.96s | lr: 5.33e-05 | Val. Loss: 0.751 |  Val. Acc: 80.96% | B. Val. Loss: 0.395 |  B. Val. Acc: 82.67%\n",
      "\t | Epoch: 013 | Batch Id: 00240 | ET: 44.09s | lr: 5.06e-05 | Val. Loss: 0.691 |  Val. Acc: 81.31% | B. Val. Loss: 0.395 |  B. Val. Acc: 82.67%\n",
      "Epoch: 013 | ET: 68.11s | \t Train Loss: 0.030 | Train Acc: 99.04% \t Val. Loss: 0.691 |  Val. Acc: 81.43% \t | B. Val. Loss: 0.395 |  B. Val. Acc: 82.67%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 014 | Batch Id: 00060 | ET: 7.63s | lr: 4.77e-05 | Val. Loss: 0.551 |  Val. Acc: 82.24% | B. Val. Loss: 0.395 |  B. Val. Acc: 82.67%\n",
      "\t | Epoch: 014 | Batch Id: 00120 | ET: 19.78s | lr: 4.50e-05 | Val. Loss: 0.727 |  Val. Acc: 81.31% | B. Val. Loss: 0.395 |  B. Val. Acc: 82.67%\n",
      "\t | Epoch: 014 | Batch Id: 00180 | ET: 31.84s | lr: 4.23e-05 | Val. Loss: 0.743 |  Val. Acc: 82.20% | B. Val. Loss: 0.395 |  B. Val. Acc: 82.67%\n",
      "\t | Epoch: 014 | Batch Id: 00240 | ET: 44.01s | lr: 3.96e-05 | Val. Loss: 0.853 |  Val. Acc: 82.44% | B. Val. Loss: 0.395 |  B. Val. Acc: 82.67%\n",
      "Epoch: 014 | ET: 68.01s | \t Train Loss: 0.010 | Train Acc: 99.74% \t Val. Loss: 0.843 |  Val. Acc: 82.24% \t | B. Val. Loss: 0.395 |  B. Val. Acc: 82.67%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 015 | Batch Id: 00060 | ET: 7.57s | lr: 3.68e-05 | Val. Loss: 0.602 |  Val. Acc: 82.32% | B. Val. Loss: 0.395 |  B. Val. Acc: 82.67%\n",
      "\t | Epoch: 015 | Batch Id: 00120 | ET: 19.73s | lr: 3.42e-05 | Val. Loss: 0.747 |  Val. Acc: 82.40% | B. Val. Loss: 0.395 |  B. Val. Acc: 82.67%\n",
      "\t | Epoch: 015 | Batch Id: 00180 | ET: 31.91s | lr: 3.16e-05 | Val. Loss: 0.864 |  Val. Acc: 81.62% | B. Val. Loss: 0.395 |  B. Val. Acc: 82.67%\n",
      "\t | Epoch: 015 | Batch Id: 00240 | ET: 44.08s | lr: 2.91e-05 | Val. Loss: 0.929 |  Val. Acc: 82.24% | B. Val. Loss: 0.395 |  B. Val. Acc: 82.67%\n",
      "Epoch: 015 | ET: 68.12s | \t Train Loss: 0.002 | Train Acc: 99.97% \t Val. Loss: 0.930 |  Val. Acc: 82.09% \t | B. Val. Loss: 0.395 |  B. Val. Acc: 82.67%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 016 | Batch Id: 00060 | ET: 7.64s | lr: 2.66e-05 | Val. Loss: 0.644 |  Val. Acc: 82.01% | B. Val. Loss: 0.395 |  B. Val. Acc: 82.67%\n",
      "\t | Epoch: 016 | Batch Id: 00120 | ET: 19.79s | lr: 2.42e-05 | Val. Loss: 0.754 |  Val. Acc: 82.32% | B. Val. Loss: 0.395 |  B. Val. Acc: 82.67%\n",
      "\t | Epoch: 016 | Batch Id: 00180 | ET: 31.91s | lr: 2.19e-05 | Val. Loss: 0.834 |  Val. Acc: 82.59% | B. Val. Loss: 0.395 |  B. Val. Acc: 82.67%\n",
      "\t | Epoch: 016 | Batch Id: 00240 | ET: 44.01s | lr: 1.97e-05 | Val. Loss: 0.883 |  Val. Acc: 82.82% | B. Val. Loss: 0.395 |  B. Val. Acc: 82.82%\n",
      "Epoch: 016 | ET: 68.23s | \t Train Loss: 0.001 | Train Acc: 100.00% \t Val. Loss: 0.885 |  Val. Acc: 82.78% \t | B. Val. Loss: 0.395 |  B. Val. Acc: 82.82%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 017 | Batch Id: 00060 | ET: 7.63s | lr: 1.75e-05 | Val. Loss: 0.654 |  Val. Acc: 81.85% | B. Val. Loss: 0.395 |  B. Val. Acc: 82.82%\n",
      "\t | Epoch: 017 | Batch Id: 00120 | ET: 19.68s | lr: 1.55e-05 | Val. Loss: 0.733 |  Val. Acc: 82.36% | B. Val. Loss: 0.395 |  B. Val. Acc: 82.82%\n",
      "\t | Epoch: 017 | Batch Id: 00180 | ET: 31.72s | lr: 1.36e-05 | Val. Loss: 0.787 |  Val. Acc: 81.97% | B. Val. Loss: 0.395 |  B. Val. Acc: 82.82%\n",
      "\t | Epoch: 017 | Batch Id: 00240 | ET: 43.76s | lr: 1.19e-05 | Val. Loss: 0.828 |  Val. Acc: 82.24% | B. Val. Loss: 0.395 |  B. Val. Acc: 82.82%\n",
      "Epoch: 017 | ET: 67.62s | \t Train Loss: 0.002 | Train Acc: 100.00% \t Val. Loss: 0.829 |  Val. Acc: 82.24% \t | B. Val. Loss: 0.395 |  B. Val. Acc: 82.82%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 018 | Batch Id: 00060 | ET: 7.55s | lr: 1.01e-05 | Val. Loss: 0.663 |  Val. Acc: 82.01% | B. Val. Loss: 0.395 |  B. Val. Acc: 82.82%\n",
      "\t | Epoch: 018 | Batch Id: 00120 | ET: 19.60s | lr: 8.61e-06 | Val. Loss: 0.697 |  Val. Acc: 82.24% | B. Val. Loss: 0.395 |  B. Val. Acc: 82.82%\n",
      "\t | Epoch: 018 | Batch Id: 00180 | ET: 31.65s | lr: 7.21e-06 | Val. Loss: 0.733 |  Val. Acc: 82.32% | B. Val. Loss: 0.395 |  B. Val. Acc: 82.82%\n",
      "\t | Epoch: 018 | Batch Id: 00240 | ET: 43.67s | lr: 5.94e-06 | Val. Loss: 0.759 |  Val. Acc: 82.36% | B. Val. Loss: 0.395 |  B. Val. Acc: 82.82%\n",
      "Epoch: 018 | ET: 67.57s | \t Train Loss: 0.002 | Train Acc: 100.00% \t Val. Loss: 0.760 |  Val. Acc: 82.36% \t | B. Val. Loss: 0.395 |  B. Val. Acc: 82.82%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 019 | Batch Id: 00060 | ET: 7.52s | lr: 4.78e-06 | Val. Loss: 0.671 |  Val. Acc: 81.62% | B. Val. Loss: 0.395 |  B. Val. Acc: 82.82%\n",
      "\t | Epoch: 019 | Batch Id: 00120 | ET: 19.63s | lr: 3.79e-06 | Val. Loss: 0.691 |  Val. Acc: 81.85% | B. Val. Loss: 0.395 |  B. Val. Acc: 82.82%\n",
      "\t | Epoch: 019 | Batch Id: 00180 | ET: 31.78s | lr: 2.95e-06 | Val. Loss: 0.710 |  Val. Acc: 81.85% | B. Val. Loss: 0.395 |  B. Val. Acc: 82.82%\n",
      "\t | Epoch: 019 | Batch Id: 00240 | ET: 43.92s | lr: 2.26e-06 | Val. Loss: 0.724 |  Val. Acc: 81.93% | B. Val. Loss: 0.395 |  B. Val. Acc: 82.82%\n",
      "Epoch: 019 | ET: 67.99s | \t Train Loss: 0.003 | Train Acc: 100.00% \t Val. Loss: 0.724 |  Val. Acc: 81.93% \t | B. Val. Loss: 0.395 |  B. Val. Acc: 82.82%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 020 | Batch Id: 00060 | ET: 7.61s | lr: 1.70e-06 | Val. Loss: 0.692 |  Val. Acc: 82.05% | B. Val. Loss: 0.395 |  B. Val. Acc: 82.82%\n",
      "\t | Epoch: 020 | Batch Id: 00120 | ET: 19.69s | lr: 1.32e-06 | Val. Loss: 0.691 |  Val. Acc: 82.12% | B. Val. Loss: 0.395 |  B. Val. Acc: 82.82%\n",
      "\t | Epoch: 020 | Batch Id: 00180 | ET: 31.78s | lr: 1.08e-06 | Val. Loss: 0.696 |  Val. Acc: 82.16% | B. Val. Loss: 0.395 |  B. Val. Acc: 82.82%\n",
      "\t | Epoch: 020 | Batch Id: 00240 | ET: 43.93s | lr: 1.00e-06 | Val. Loss: 0.702 |  Val. Acc: 82.09% | B. Val. Loss: 0.395 |  B. Val. Acc: 82.82%\n",
      "Epoch: 020 | ET: 68.01s | \t Train Loss: 0.004 | Train Acc: 100.00% \t Val. Loss: 0.702 |  Val. Acc: 82.09% \t | B. Val. Loss: 0.395 |  B. Val. Acc: 82.82%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Number of the parameters: 2224577\n",
      "\n",
      "Training\n",
      "-----    4    ----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 001 | Batch Id: 00060 | ET: 7.62s | lr: 1.04e-05 | Val. Loss: 0.702 |  Val. Acc: 49.98% | B. Val. Loss: 0.702 |  B. Val. Acc: 49.98%\n",
      "\t | Epoch: 001 | Batch Id: 00120 | ET: 19.88s | lr: 1.15e-05 | Val. Loss: 0.693 |  Val. Acc: 50.02% | B. Val. Loss: 0.693 |  B. Val. Acc: 50.02%\n",
      "\t | Epoch: 001 | Batch Id: 00180 | ET: 32.15s | lr: 1.33e-05 | Val. Loss: 0.694 |  Val. Acc: 50.02% | B. Val. Loss: 0.693 |  B. Val. Acc: 50.02%\n",
      "\t | Epoch: 001 | Batch Id: 00240 | ET: 44.44s | lr: 1.59e-05 | Val. Loss: 0.692 |  Val. Acc: 50.02% | B. Val. Loss: 0.692 |  B. Val. Acc: 50.02%\n",
      "Epoch: 001 | ET: 68.67s | \t Train Loss: 0.692 | Train Acc: 50.07% \t Val. Loss: 0.692 |  Val. Acc: 50.06% \t | B. Val. Loss: 0.692 |  B. Val. Acc: 50.06%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 002 | Batch Id: 00060 | ET: 7.65s | lr: 1.92e-05 | Val. Loss: 0.679 |  Val. Acc: 57.62% | B. Val. Loss: 0.679 |  B. Val. Acc: 57.62%\n",
      "\t | Epoch: 002 | Batch Id: 00120 | ET: 19.95s | lr: 2.31e-05 | Val. Loss: 0.661 |  Val. Acc: 61.61% | B. Val. Loss: 0.661 |  B. Val. Acc: 61.61%\n",
      "\t | Epoch: 002 | Batch Id: 00180 | ET: 32.24s | lr: 2.74e-05 | Val. Loss: 0.642 |  Val. Acc: 65.06% | B. Val. Loss: 0.642 |  B. Val. Acc: 65.06%\n",
      "\t | Epoch: 002 | Batch Id: 00240 | ET: 44.55s | lr: 3.23e-05 | Val. Loss: 0.598 |  Val. Acc: 65.18% | B. Val. Loss: 0.598 |  B. Val. Acc: 65.18%\n",
      "Epoch: 002 | ET: 68.90s | \t Train Loss: 0.555 | Train Acc: 70.18% \t Val. Loss: 0.590 |  Val. Acc: 66.42% \t | B. Val. Loss: 0.590 |  B. Val. Acc: 66.42%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 003 | Batch Id: 00060 | ET: 7.56s | lr: 3.77e-05 | Val. Loss: 0.564 |  Val. Acc: 70.41% | B. Val. Loss: 0.564 |  B. Val. Acc: 70.41%\n",
      "\t | Epoch: 003 | Batch Id: 00120 | ET: 19.74s | lr: 4.32e-05 | Val. Loss: 0.550 |  Val. Acc: 72.94% | B. Val. Loss: 0.550 |  B. Val. Acc: 72.94%\n",
      "\t | Epoch: 003 | Batch Id: 00180 | ET: 32.01s | lr: 4.89e-05 | Val. Loss: 0.484 |  Val. Acc: 77.28% | B. Val. Loss: 0.484 |  B. Val. Acc: 77.28%\n",
      "\t | Epoch: 003 | Batch Id: 00240 | ET: 44.18s | lr: 5.48e-05 | Val. Loss: 0.476 |  Val. Acc: 78.01% | B. Val. Loss: 0.476 |  B. Val. Acc: 78.01%\n",
      "Epoch: 003 | ET: 68.24s | \t Train Loss: 0.426 | Train Acc: 80.59% \t Val. Loss: 0.477 |  Val. Acc: 77.86% \t | B. Val. Loss: 0.476 |  B. Val. Acc: 78.01%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 004 | Batch Id: 00060 | ET: 7.64s | lr: 6.08e-05 | Val. Loss: 0.484 |  Val. Acc: 77.59% | B. Val. Loss: 0.476 |  B. Val. Acc: 78.01%\n",
      "\t | Epoch: 004 | Batch Id: 00120 | ET: 19.84s | lr: 6.65e-05 | Val. Loss: 0.503 |  Val. Acc: 74.14% | B. Val. Loss: 0.476 |  B. Val. Acc: 78.01%\n",
      "\t | Epoch: 004 | Batch Id: 00180 | ET: 32.03s | lr: 7.21e-05 | Val. Loss: 0.462 |  Val. Acc: 78.75% | B. Val. Loss: 0.462 |  B. Val. Acc: 78.75%\n",
      "\t | Epoch: 004 | Batch Id: 00240 | ET: 44.34s | lr: 7.73e-05 | Val. Loss: 0.458 |  Val. Acc: 78.83% | B. Val. Loss: 0.458 |  B. Val. Acc: 78.83%\n",
      "Epoch: 004 | ET: 68.70s | \t Train Loss: 0.388 | Train Acc: 82.59% \t Val. Loss: 0.459 |  Val. Acc: 78.91% \t | B. Val. Loss: 0.458 |  B. Val. Acc: 78.91%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 005 | Batch Id: 00060 | ET: 7.64s | lr: 8.23e-05 | Val. Loss: 0.458 |  Val. Acc: 79.14% | B. Val. Loss: 0.458 |  B. Val. Acc: 79.14%\n",
      "\t | Epoch: 005 | Batch Id: 00120 | ET: 19.93s | lr: 8.67e-05 | Val. Loss: 0.448 |  Val. Acc: 79.37% | B. Val. Loss: 0.448 |  B. Val. Acc: 79.37%\n",
      "\t | Epoch: 005 | Batch Id: 00180 | ET: 32.26s | lr: 9.06e-05 | Val. Loss: 0.442 |  Val. Acc: 80.34% | B. Val. Loss: 0.442 |  B. Val. Acc: 80.34%\n",
      "\t | Epoch: 005 | Batch Id: 00240 | ET: 44.50s | lr: 9.39e-05 | Val. Loss: 0.429 |  Val. Acc: 80.92% | B. Val. Loss: 0.429 |  B. Val. Acc: 80.92%\n",
      "Epoch: 005 | ET: 68.47s | \t Train Loss: 0.349 | Train Acc: 84.24% \t Val. Loss: 0.446 |  Val. Acc: 79.76% \t | B. Val. Loss: 0.429 |  B. Val. Acc: 80.92%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 006 | Batch Id: 00060 | ET: 7.67s | lr: 9.66e-05 | Val. Loss: 0.451 |  Val. Acc: 79.06% | B. Val. Loss: 0.429 |  B. Val. Acc: 80.92%\n",
      "\t | Epoch: 006 | Batch Id: 00120 | ET: 19.90s | lr: 9.84e-05 | Val. Loss: 0.450 |  Val. Acc: 79.72% | B. Val. Loss: 0.429 |  B. Val. Acc: 80.92%\n",
      "\t | Epoch: 006 | Batch Id: 00180 | ET: 31.84s | lr: 9.96e-05 | Val. Loss: 0.418 |  Val. Acc: 80.92% | B. Val. Loss: 0.418 |  B. Val. Acc: 80.92%\n",
      "\t | Epoch: 006 | Batch Id: 00240 | ET: 44.00s | lr: 1.00e-04 | Val. Loss: 0.413 |  Val. Acc: 82.01% | B. Val. Loss: 0.413 |  B. Val. Acc: 82.01%\n",
      "Epoch: 006 | ET: 68.14s | \t Train Loss: 0.299 | Train Acc: 87.79% \t Val. Loss: 0.419 |  Val. Acc: 81.58% \t | B. Val. Loss: 0.413 |  B. Val. Acc: 82.01%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 007 | Batch Id: 00060 | ET: 7.48s | lr: 9.99e-05 | Val. Loss: 0.434 |  Val. Acc: 81.39% | B. Val. Loss: 0.413 |  B. Val. Acc: 82.01%\n",
      "\t | Epoch: 007 | Batch Id: 00120 | ET: 19.58s | lr: 9.97e-05 | Val. Loss: 0.445 |  Val. Acc: 79.95% | B. Val. Loss: 0.413 |  B. Val. Acc: 82.01%\n",
      "\t | Epoch: 007 | Batch Id: 00180 | ET: 31.67s | lr: 9.93e-05 | Val. Loss: 0.406 |  Val. Acc: 82.12% | B. Val. Loss: 0.406 |  B. Val. Acc: 82.12%\n",
      "\t | Epoch: 007 | Batch Id: 00240 | ET: 43.91s | lr: 9.88e-05 | Val. Loss: 0.414 |  Val. Acc: 82.47% | B. Val. Loss: 0.406 |  B. Val. Acc: 82.47%\n",
      "Epoch: 007 | ET: 68.24s | \t Train Loss: 0.267 | Train Acc: 89.13% \t Val. Loss: 0.435 |  Val. Acc: 82.63% \t | B. Val. Loss: 0.406 |  B. Val. Acc: 82.63%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 008 | Batch Id: 00060 | ET: 7.61s | lr: 9.81e-05 | Val. Loss: 0.427 |  Val. Acc: 82.01% | B. Val. Loss: 0.406 |  B. Val. Acc: 82.63%\n",
      "\t | Epoch: 008 | Batch Id: 00120 | ET: 19.72s | lr: 9.72e-05 | Val. Loss: 0.458 |  Val. Acc: 79.95% | B. Val. Loss: 0.406 |  B. Val. Acc: 82.63%\n",
      "\t | Epoch: 008 | Batch Id: 00180 | ET: 31.84s | lr: 9.63e-05 | Val. Loss: 0.419 |  Val. Acc: 82.44% | B. Val. Loss: 0.406 |  B. Val. Acc: 82.63%\n",
      "\t | Epoch: 008 | Batch Id: 00240 | ET: 44.05s | lr: 9.51e-05 | Val. Loss: 0.407 |  Val. Acc: 83.02% | B. Val. Loss: 0.406 |  B. Val. Acc: 83.02%\n",
      "Epoch: 008 | ET: 68.29s | \t Train Loss: 0.214 | Train Acc: 91.37% \t Val. Loss: 0.403 |  Val. Acc: 82.47% \t | B. Val. Loss: 0.403 |  B. Val. Acc: 83.02%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 009 | Batch Id: 00060 | ET: 7.67s | lr: 9.38e-05 | Val. Loss: 0.447 |  Val. Acc: 81.00% | B. Val. Loss: 0.403 |  B. Val. Acc: 83.02%\n",
      "\t | Epoch: 009 | Batch Id: 00120 | ET: 19.89s | lr: 9.24e-05 | Val. Loss: 0.487 |  Val. Acc: 81.16% | B. Val. Loss: 0.403 |  B. Val. Acc: 83.02%\n",
      "\t | Epoch: 009 | Batch Id: 00180 | ET: 32.10s | lr: 9.09e-05 | Val. Loss: 0.424 |  Val. Acc: 81.62% | B. Val. Loss: 0.403 |  B. Val. Acc: 83.02%\n",
      "\t | Epoch: 009 | Batch Id: 00240 | ET: 44.31s | lr: 8.93e-05 | Val. Loss: 0.448 |  Val. Acc: 82.59% | B. Val. Loss: 0.403 |  B. Val. Acc: 83.02%\n",
      "Epoch: 009 | ET: 68.43s | \t Train Loss: 0.190 | Train Acc: 91.90% \t Val. Loss: 0.450 |  Val. Acc: 82.44% \t | B. Val. Loss: 0.403 |  B. Val. Acc: 83.02%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 010 | Batch Id: 00060 | ET: 7.67s | lr: 8.74e-05 | Val. Loss: 0.453 |  Val. Acc: 82.05% | B. Val. Loss: 0.403 |  B. Val. Acc: 83.02%\n",
      "\t | Epoch: 010 | Batch Id: 00120 | ET: 19.89s | lr: 8.55e-05 | Val. Loss: 0.603 |  Val. Acc: 79.80% | B. Val. Loss: 0.403 |  B. Val. Acc: 83.02%\n",
      "\t | Epoch: 010 | Batch Id: 00180 | ET: 32.11s | lr: 8.35e-05 | Val. Loss: 0.431 |  Val. Acc: 82.71% | B. Val. Loss: 0.403 |  B. Val. Acc: 83.02%\n",
      "\t | Epoch: 010 | Batch Id: 00240 | ET: 44.32s | lr: 8.14e-05 | Val. Loss: 0.422 |  Val. Acc: 82.47% | B. Val. Loss: 0.403 |  B. Val. Acc: 83.02%\n",
      "Epoch: 010 | ET: 68.47s | \t Train Loss: 0.167 | Train Acc: 93.98% \t Val. Loss: 0.408 |  Val. Acc: 83.60% \t | B. Val. Loss: 0.403 |  B. Val. Acc: 83.60%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 011 | Batch Id: 00060 | ET: 7.68s | lr: 7.92e-05 | Val. Loss: 0.454 |  Val. Acc: 82.44% | B. Val. Loss: 0.403 |  B. Val. Acc: 83.60%\n",
      "\t | Epoch: 011 | Batch Id: 00120 | ET: 19.89s | lr: 7.69e-05 | Val. Loss: 0.572 |  Val. Acc: 82.32% | B. Val. Loss: 0.403 |  B. Val. Acc: 83.60%\n",
      "\t | Epoch: 011 | Batch Id: 00180 | ET: 31.99s | lr: 7.45e-05 | Val. Loss: 0.573 |  Val. Acc: 81.16% | B. Val. Loss: 0.403 |  B. Val. Acc: 83.60%\n",
      "\t | Epoch: 011 | Batch Id: 00240 | ET: 44.09s | lr: 7.21e-05 | Val. Loss: 0.448 |  Val. Acc: 82.59% | B. Val. Loss: 0.403 |  B. Val. Acc: 83.60%\n",
      "Epoch: 011 | ET: 68.02s | \t Train Loss: 0.116 | Train Acc: 96.52% \t Val. Loss: 0.428 |  Val. Acc: 82.82% \t | B. Val. Loss: 0.403 |  B. Val. Acc: 83.60%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 012 | Batch Id: 00060 | ET: 7.66s | lr: 6.95e-05 | Val. Loss: 0.510 |  Val. Acc: 82.47% | B. Val. Loss: 0.403 |  B. Val. Acc: 83.60%\n",
      "\t | Epoch: 012 | Batch Id: 00120 | ET: 19.86s | lr: 6.69e-05 | Val. Loss: 0.680 |  Val. Acc: 81.78% | B. Val. Loss: 0.403 |  B. Val. Acc: 83.60%\n",
      "\t | Epoch: 012 | Batch Id: 00180 | ET: 32.11s | lr: 6.43e-05 | Val. Loss: 0.586 |  Val. Acc: 81.43% | B. Val. Loss: 0.403 |  B. Val. Acc: 83.60%\n",
      "\t | Epoch: 012 | Batch Id: 00240 | ET: 44.23s | lr: 6.16e-05 | Val. Loss: 0.627 |  Val. Acc: 81.54% | B. Val. Loss: 0.403 |  B. Val. Acc: 83.60%\n",
      "Epoch: 012 | ET: 68.34s | \t Train Loss: 0.077 | Train Acc: 96.82% \t Val. Loss: 0.680 |  Val. Acc: 81.27% \t | B. Val. Loss: 0.403 |  B. Val. Acc: 83.60%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 013 | Batch Id: 00060 | ET: 7.68s | lr: 5.88e-05 | Val. Loss: 0.549 |  Val. Acc: 81.50% | B. Val. Loss: 0.403 |  B. Val. Acc: 83.60%\n",
      "\t | Epoch: 013 | Batch Id: 00120 | ET: 19.89s | lr: 5.61e-05 | Val. Loss: 0.790 |  Val. Acc: 81.66% | B. Val. Loss: 0.403 |  B. Val. Acc: 83.60%\n",
      "\t | Epoch: 013 | Batch Id: 00180 | ET: 32.09s | lr: 5.33e-05 | Val. Loss: 0.869 |  Val. Acc: 81.93% | B. Val. Loss: 0.403 |  B. Val. Acc: 83.60%\n",
      "\t | Epoch: 013 | Batch Id: 00240 | ET: 44.19s | lr: 5.06e-05 | Val. Loss: 0.822 |  Val. Acc: 82.12% | B. Val. Loss: 0.403 |  B. Val. Acc: 83.60%\n",
      "Epoch: 013 | ET: 68.14s | \t Train Loss: 0.014 | Train Acc: 99.75% \t Val. Loss: 0.811 |  Val. Acc: 81.74% \t | B. Val. Loss: 0.403 |  B. Val. Acc: 83.60%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 014 | Batch Id: 00060 | ET: 7.58s | lr: 4.77e-05 | Val. Loss: 0.577 |  Val. Acc: 82.24% | B. Val. Loss: 0.403 |  B. Val. Acc: 83.60%\n",
      "\t | Epoch: 014 | Batch Id: 00120 | ET: 19.65s | lr: 4.50e-05 | Val. Loss: 0.782 |  Val. Acc: 81.78% | B. Val. Loss: 0.403 |  B. Val. Acc: 83.60%\n",
      "\t | Epoch: 014 | Batch Id: 00180 | ET: 31.72s | lr: 4.23e-05 | Val. Loss: 0.881 |  Val. Acc: 82.01% | B. Val. Loss: 0.403 |  B. Val. Acc: 83.60%\n",
      "\t | Epoch: 014 | Batch Id: 00240 | ET: 43.79s | lr: 3.96e-05 | Val. Loss: 0.937 |  Val. Acc: 81.58% | B. Val. Loss: 0.403 |  B. Val. Acc: 83.60%\n",
      "Epoch: 014 | ET: 67.79s | \t Train Loss: 0.008 | Train Acc: 99.74% \t Val. Loss: 0.941 |  Val. Acc: 81.50% \t | B. Val. Loss: 0.403 |  B. Val. Acc: 83.60%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 015 | Batch Id: 00060 | ET: 7.63s | lr: 3.68e-05 | Val. Loss: 0.631 |  Val. Acc: 82.44% | B. Val. Loss: 0.403 |  B. Val. Acc: 83.60%\n",
      "\t | Epoch: 015 | Batch Id: 00120 | ET: 19.73s | lr: 3.42e-05 | Val. Loss: 0.794 |  Val. Acc: 82.16% | B. Val. Loss: 0.403 |  B. Val. Acc: 83.60%\n",
      "\t | Epoch: 015 | Batch Id: 00180 | ET: 31.81s | lr: 3.16e-05 | Val. Loss: 0.871 |  Val. Acc: 82.51% | B. Val. Loss: 0.403 |  B. Val. Acc: 83.60%\n",
      "\t | Epoch: 015 | Batch Id: 00240 | ET: 43.90s | lr: 2.91e-05 | Val. Loss: 0.945 |  Val. Acc: 81.74% | B. Val. Loss: 0.403 |  B. Val. Acc: 83.60%\n",
      "Epoch: 015 | ET: 67.73s | \t Train Loss: 0.001 | Train Acc: 100.00% \t Val. Loss: 0.948 |  Val. Acc: 81.74% \t | B. Val. Loss: 0.403 |  B. Val. Acc: 83.60%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 016 | Batch Id: 00060 | ET: 7.59s | lr: 2.66e-05 | Val. Loss: 0.648 |  Val. Acc: 81.78% | B. Val. Loss: 0.403 |  B. Val. Acc: 83.60%\n",
      "\t | Epoch: 016 | Batch Id: 00120 | ET: 19.80s | lr: 2.42e-05 | Val. Loss: 0.784 |  Val. Acc: 82.05% | B. Val. Loss: 0.403 |  B. Val. Acc: 83.60%\n",
      "\t | Epoch: 016 | Batch Id: 00180 | ET: 31.95s | lr: 2.19e-05 | Val. Loss: 0.856 |  Val. Acc: 81.81% | B. Val. Loss: 0.403 |  B. Val. Acc: 83.60%\n",
      "\t | Epoch: 016 | Batch Id: 00240 | ET: 44.03s | lr: 1.97e-05 | Val. Loss: 0.915 |  Val. Acc: 81.97% | B. Val. Loss: 0.403 |  B. Val. Acc: 83.60%\n",
      "Epoch: 016 | ET: 68.17s | \t Train Loss: 0.001 | Train Acc: 100.00% \t Val. Loss: 0.916 |  Val. Acc: 82.01% \t | B. Val. Loss: 0.403 |  B. Val. Acc: 83.60%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 017 | Batch Id: 00060 | ET: 7.57s | lr: 1.75e-05 | Val. Loss: 0.666 |  Val. Acc: 82.09% | B. Val. Loss: 0.403 |  B. Val. Acc: 83.60%\n",
      "\t | Epoch: 017 | Batch Id: 00120 | ET: 19.62s | lr: 1.55e-05 | Val. Loss: 0.751 |  Val. Acc: 82.09% | B. Val. Loss: 0.403 |  B. Val. Acc: 83.60%\n",
      "\t | Epoch: 017 | Batch Id: 00180 | ET: 31.67s | lr: 1.36e-05 | Val. Loss: 0.818 |  Val. Acc: 81.97% | B. Val. Loss: 0.403 |  B. Val. Acc: 83.60%\n",
      "\t | Epoch: 017 | Batch Id: 00240 | ET: 43.73s | lr: 1.19e-05 | Val. Loss: 0.856 |  Val. Acc: 82.05% | B. Val. Loss: 0.403 |  B. Val. Acc: 83.60%\n",
      "Epoch: 017 | ET: 67.71s | \t Train Loss: 0.001 | Train Acc: 100.00% \t Val. Loss: 0.857 |  Val. Acc: 82.05% \t | B. Val. Loss: 0.403 |  B. Val. Acc: 83.60%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 018 | Batch Id: 00060 | ET: 7.56s | lr: 1.01e-05 | Val. Loss: 0.685 |  Val. Acc: 81.97% | B. Val. Loss: 0.403 |  B. Val. Acc: 83.60%\n",
      "\t | Epoch: 018 | Batch Id: 00120 | ET: 19.48s | lr: 8.61e-06 | Val. Loss: 0.723 |  Val. Acc: 82.24% | B. Val. Loss: 0.403 |  B. Val. Acc: 83.60%\n",
      "\t | Epoch: 018 | Batch Id: 00180 | ET: 31.40s | lr: 7.21e-06 | Val. Loss: 0.768 |  Val. Acc: 82.32% | B. Val. Loss: 0.403 |  B. Val. Acc: 83.60%\n",
      "\t | Epoch: 018 | Batch Id: 00240 | ET: 43.61s | lr: 5.94e-06 | Val. Loss: 0.797 |  Val. Acc: 82.36% | B. Val. Loss: 0.403 |  B. Val. Acc: 83.60%\n",
      "Epoch: 018 | ET: 67.60s | \t Train Loss: 0.002 | Train Acc: 100.00% \t Val. Loss: 0.798 |  Val. Acc: 82.36% \t | B. Val. Loss: 0.403 |  B. Val. Acc: 83.60%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 019 | Batch Id: 00060 | ET: 7.54s | lr: 4.78e-06 | Val. Loss: 0.680 |  Val. Acc: 82.44% | B. Val. Loss: 0.403 |  B. Val. Acc: 83.60%\n",
      "\t | Epoch: 019 | Batch Id: 00120 | ET: 19.43s | lr: 3.79e-06 | Val. Loss: 0.697 |  Val. Acc: 82.36% | B. Val. Loss: 0.403 |  B. Val. Acc: 83.60%\n",
      "\t | Epoch: 019 | Batch Id: 00180 | ET: 31.48s | lr: 2.95e-06 | Val. Loss: 0.721 |  Val. Acc: 82.16% | B. Val. Loss: 0.403 |  B. Val. Acc: 83.60%\n",
      "\t | Epoch: 019 | Batch Id: 00240 | ET: 43.54s | lr: 2.26e-06 | Val. Loss: 0.739 |  Val. Acc: 82.16% | B. Val. Loss: 0.403 |  B. Val. Acc: 83.60%\n",
      "Epoch: 019 | ET: 67.65s | \t Train Loss: 0.003 | Train Acc: 100.00% \t Val. Loss: 0.739 |  Val. Acc: 82.16% \t | B. Val. Loss: 0.403 |  B. Val. Acc: 83.60%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 020 | Batch Id: 00060 | ET: 7.69s | lr: 1.70e-06 | Val. Loss: 0.676 |  Val. Acc: 82.24% | B. Val. Loss: 0.403 |  B. Val. Acc: 83.60%\n",
      "\t | Epoch: 020 | Batch Id: 00120 | ET: 19.77s | lr: 1.32e-06 | Val. Loss: 0.683 |  Val. Acc: 82.20% | B. Val. Loss: 0.403 |  B. Val. Acc: 83.60%\n",
      "\t | Epoch: 020 | Batch Id: 00180 | ET: 31.90s | lr: 1.08e-06 | Val. Loss: 0.693 |  Val. Acc: 82.32% | B. Val. Loss: 0.403 |  B. Val. Acc: 83.60%\n",
      "\t | Epoch: 020 | Batch Id: 00240 | ET: 44.10s | lr: 1.00e-06 | Val. Loss: 0.700 |  Val. Acc: 82.32% | B. Val. Loss: 0.403 |  B. Val. Acc: 83.60%\n",
      "Epoch: 020 | ET: 68.02s | \t Train Loss: 0.003 | Train Acc: 100.00% \t Val. Loss: 0.700 |  Val. Acc: 82.32% \t | B. Val. Loss: 0.403 |  B. Val. Acc: 83.60%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for sim_num in range(0,5):\n",
    "    model = MobileNetV2()\n",
    "\n",
    "    if len(dev_names)>1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "    model.to(device)\n",
    "    print(f\"Number of the parameters: {count_parameters(model)}\\n\")\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    criterion = torch.nn.BCEWithLogitsLoss(reduction=\"sum\").to(device)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=LEARNING_RATE, div_factor=DIV_FACTOR, final_div_factor=FINAL_DIV_FACTOR, steps_per_epoch=len(train_loader), epochs = EPOCHS, verbose=0)\n",
    "\n",
    "\n",
    "    train_accs = []\n",
    "    train_losses = []\n",
    "    valid_accs = []\n",
    "    valid_losses = []\n",
    "\n",
    "\n",
    "\n",
    "    f = open(f\"{RESULTS_FILENAME}_{sim_num}.csv\", \"w\")\n",
    "    f.write(160*\"-\"+\"\\n\")\n",
    "    f.write(f\"Device: {dev_names[0]} | Number: {len(dev_names)}\\n\")\n",
    "    f.write(f\"Epochs: {EPOCHS}\\n\")\n",
    "    f.write(f\"Optimizer: {type (optimizer).__name__}\\n\")\n",
    "    f.write(f\"Scheduler: {type (scheduler).__name__}\\n\")\n",
    "    f.write(f\"Div factor: {DIV_FACTOR}\\n\")\n",
    "    f.write(f\"Final div factor: {FINAL_DIV_FACTOR}\\n\")\n",
    "    f.write(f\"Weight decay: {WEIGHT_DECAY}\\n\")\n",
    "    f.write(f\"Learning rate: {LEARNING_RATE}\\n\")\n",
    "    f.write(f\"Number of the parameters: {count_parameters(model)}\\n\")\n",
    "    f.write(f\"Model: {model}\\n\")\n",
    "    f.write(160*\"-\"+\"\\n\")\n",
    "    f.close()\n",
    "    print(\"Training\")\n",
    "    print(5 * \"-\" + f\"{sim_num:5}\" + 4*\" \"+ 160 * \"-\")\n",
    "\n",
    "    best_valid_loss = float('inf')\n",
    "    best_valid_acc = -1.0\n",
    "    valid_acc = 0.0\n",
    "\n",
    "    all_time_s = 0.0\n",
    "    lr = 0.0\n",
    "\n",
    "    train_accs = []\n",
    "    train_losses = []\n",
    "    valid_accs = []\n",
    "    valid_losses = []\n",
    "    valid_indices = []\n",
    "\n",
    "    # Training the `sim_num`-th model\n",
    "    for epoch in range(EPOCHS):\n",
    "\n",
    "        start_time = default_timer()\n",
    "\n",
    "        epoch_loss = 0.0\n",
    "        epoch_acc = 0.0\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        batch_id = 0\n",
    "        number_of_training_elements = 0\n",
    "\n",
    "        valid_accs_temp = []\n",
    "        valid_losses_temp = []\n",
    "        valid_indices_temp = []\n",
    "\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.float().to(device).view(-1,1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred = model(x)\n",
    "            loss = criterion(y_pred, y)\n",
    "            batch_size = x.shape[0]\n",
    "            number_of_training_elements += batch_size\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            end_time = default_timer()\n",
    "\n",
    "            # Evaluating the model\n",
    "            if (batch_id+1)%EVAL_FREQ==0:\n",
    "\n",
    "                valid_indices_temp.append(batch_id+1)\n",
    "                valid_loss, valid_acc = evaluate(model, valid_loader, criterion, device)\n",
    "\n",
    "                valid_losses_temp.append(valid_loss)\n",
    "                valid_accs_temp.append(valid_acc)\n",
    "\n",
    "                if valid_acc > best_valid_acc:\n",
    "                    best_valid_acc = valid_acc\n",
    "                    torch.save(model.state_dict(), f\"{BEST_MODEL_FILENAME}_{sim_num}.pt\")\n",
    "\n",
    "                if valid_loss < best_valid_loss:\n",
    "                    best_valid_loss = valid_loss\n",
    "\n",
    "                lr = scheduler.get_last_lr()[0]\n",
    "\n",
    "                line = f'\\t | Epoch: {epoch+1:03} | Batch Id: {batch_id+1:05} | ET: {end_time-start_time:.2f}s | lr: {lr:.2e} | Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}% | B. Val. Loss: {best_valid_loss:.3f} |  B. Val. Acc: {best_valid_acc*100:.2f}%'\n",
    "                print(line)\n",
    "                f = open(f\"{RESULTS_FILENAME}_{sim_num}.csv\", \"a\")\n",
    "                f.write(line+\"\\n\")\n",
    "                f.close()\n",
    "\n",
    "\n",
    "\n",
    "            batch_id+=1\n",
    "            scheduler.step()\n",
    "\n",
    "        valid_indices_temp.append(batch_id)\n",
    "        valid_loss, valid_acc = evaluate(model, valid_loader, criterion, device)\n",
    "\n",
    "        valid_losses_temp.append(valid_loss)\n",
    "        valid_accs_temp.append(valid_acc)\n",
    "\n",
    "        valid_losses.append(valid_losses_temp)\n",
    "        valid_accs.append(valid_accs_temp)\n",
    "\n",
    "        valid_indices.append(valid_indices_temp)\n",
    "\n",
    "        if valid_acc > best_valid_acc:\n",
    "            best_valid_acc = valid_acc\n",
    "            torch.save(model.state_dict(), f\"{BEST_MODEL_FILENAME}_{sim_num}.pt\")\n",
    "\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "\n",
    "        train_loss, train_acc = evaluate(model, train_loader, criterion, device)\n",
    "\n",
    "        end_time = default_timer()\n",
    "\n",
    "        all_time_s += end_time - start_time\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "\n",
    "        line = f'Epoch: {epoch+1:03} | ET: {end_time-start_time:.2f}s | \\t Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% \\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}% \\t | B. Val. Loss: {best_valid_loss:.3f} |  B. Val. Acc: {best_valid_acc*100:.2f}%'\n",
    "        print(line)\n",
    "        print(160*\"-\")\n",
    "\n",
    "        f = open(f\"{RESULTS_FILENAME}_{sim_num}.csv\", \"a\")\n",
    "        f.write(line+\"\\n\")\n",
    "        f.write(160*\"-\"+\"\\n\")\n",
    "        f.close()\n",
    "\n",
    "    line = f\"\\nDuration: {all_time_s:.2f}s\\n\"\n",
    "    f = open(f\"{RESULTS_FILENAME}_{sim_num}.csv\", \"a\")\n",
    "    f.write(line+\"\\n\")\n",
    "    f.write(80*\"-\"+\"\\n\")\n",
    "    f.close()\n",
    "\n",
    "    # Saving the results for analyzing them later in the evaluation part\n",
    "    valid_losses_plot = []\n",
    "    valid_accs_plot = []\n",
    "    epoch_plot = []\n",
    "    for epoch in range(len(valid_accs)):\n",
    "        valid_accs_temp = valid_accs[epoch]\n",
    "        valid_losses_temp = valid_losses[epoch]\n",
    "        valid_indices_temp = valid_indices[epoch]\n",
    "        ind = 0\n",
    "        for mini_batch_id in valid_indices_temp:\n",
    "            epoch_plot.append(epoch + mini_batch_id/len(train_loader))\n",
    "            valid_accs_plot.append(valid_accs_temp[ind]*100)\n",
    "            valid_losses_plot.append(valid_losses_temp[ind])\n",
    "            ind += 1\n",
    "\n",
    "    valid_results = pd.DataFrame({\"epoch\":epoch_plot,\n",
    "                  \"valid_loss\":valid_losses_plot,\n",
    "                  \"valid_acc\":valid_accs_plot\n",
    "                  })\n",
    "\n",
    "    valid_results.to_csv(f\"{VALID_RESULTS_FILENAME}_{sim_num}.csv\",sep=\";\",index=False)\n",
    "    train_accs = [acc*100 for acc in train_accs]\n",
    "    train_results = pd.DataFrame({\"epoch\":list(np.arange(1,EPOCHS+1,1)),\n",
    "                  \"train_loss\":train_losses,\n",
    "                  \"train_acc\":train_accs\n",
    "                  })\n",
    "    train_results.to_csv(f\"{TRAIN_RESULTS_FILENAME}_{sim_num}.csv\",sep=\";\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e24536b-29e4-48e2-a0ed-b58cf1540998",
   "metadata": {
    "id": "0e24536b-29e4-48e2-a0ed-b58cf1540998"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
