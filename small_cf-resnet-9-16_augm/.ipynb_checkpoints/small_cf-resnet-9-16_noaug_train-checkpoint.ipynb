{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "845c8b9b-fb9a-4e89-a5e8-eff6f1e9af4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szekeres/anaconda3/envs/pytorch-gpu/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from timeit import default_timer\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import random\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "from sklearn.metrics import balanced_accuracy_score, accuracy_score\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] ='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d81157d-48c2-4639-86c9-e40c9edb2503",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49821284-5509-4a51-819c-8c219876ce31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed May  1 10:26:20 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.182.03   Driver Version: 470.182.03   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro P5000        Off  | 00000000:04:00.0 Off |                  Off |\n",
      "| 35%   34C    P8     6W / 180W |    385MiB / 16276MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1123      G   /usr/lib/xorg/Xorg                 39MiB |\n",
      "|    0   N/A  N/A      1620      G   /usr/lib/xorg/Xorg                142MiB |\n",
      "|    0   N/A  N/A      1759      G   /usr/bin/gnome-shell               95MiB |\n",
      "|    0   N/A  N/A      2227      G   ...769660681633685620,262144       99MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60f47788-9a7b-46a1-abb2-1254484d142d",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "SEED = 2024\n",
    "BATCH_SIZE = 32\n",
    "#OUT_FEATURES = 2\n",
    "TEST_SPLIT_RATIO = 0.25\n",
    "SR = 8000\n",
    "N_FFT = 256\n",
    "HOP_LEN = 256 // 6\n",
    "AUGM = True\n",
    "# Creating the results directory\n",
    "if not os.path.exists('results'):\n",
    "    os.makedirs('results')\n",
    "RESULTS_FILENAME = \"./results/inrun_results\" # _x.csv\n",
    "VALID_RESULTS_FILENAME = \"./results/valid_results\" # _x.csv\n",
    "TRAIN_RESULTS_FILENAME = \"./results/train_results\" # _x.csv\n",
    "BEST_MODEL_FILENAME = \"./results/best-model\" # _x.pt\n",
    "DIV_FACTOR = 10.\n",
    "FINAL_DIV_FACTOR = 10.\n",
    "WEIGHT_DECAY = 0.005\n",
    "LEARNING_RATE = 0.0001\n",
    "EVAL_FREQ=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1385666-618c-4f6b-a7c3-f6557b4ea2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATASET = \"../data/train_whales.csv\"\n",
    "TEST_DATASET = \"../data/test_whales.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01b41649-9ec3-4cb7-be98-df999eed5c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda is available: True\n",
      "Device: ['Quadro P5000']\n"
     ]
    }
   ],
   "source": [
    "# Fixing the seeds\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "print(f\"Cuda is available: {torch.cuda.is_available()}\")\n",
    "dev_names = [torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())]\n",
    "print(f\"Device: {dev_names}\")\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed78bb6c-63b7-424a-aa78-26cb11def586",
   "metadata": {},
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4159c0f8-3c55-4436-9ea0-6e8fe91884f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39dedb9f-9568-48bd-8435-7643eba80e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = [\"no-whale\",\"whale\"]\n",
    "target_names_dict = {target_names[i]: i for i in range(len(target_names))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f584bdfe-0b63-4359-bcf2-f06605c58423",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ccfa212-ff54-421c-8209-cc0acd7381c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading - Elapsed time: 12.44s\n"
     ]
    }
   ],
   "source": [
    "t_s = default_timer()\n",
    "data_train = pd.read_csv(TRAIN_DATASET,sep=\",\")\n",
    "columns = data_train.columns\n",
    "data_train[columns[-1]]=data_train[columns[-1]].replace(target_names_dict)\n",
    "data_train = data_train.values\n",
    "data_train_labels = data_train[:,-1].reshape(-1)\n",
    "data_train_labels = data_train_labels.astype(int)\n",
    "data_train = data_train[:,:-1]\n",
    "t_e = default_timer()\n",
    "\n",
    "print(f\"Data loading - Elapsed time: {t_e-t_s:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e557db05-6f57-4bc2-8578-f1959148bf09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10316, 4000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bf742c8-bba3-4faf-b8ce-e4be92b5cf84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10316, 4000)\n"
     ]
    }
   ],
   "source": [
    "print(data_train.shape)\n",
    "#data_train = np.squeeze(data_train)\n",
    "#data_train = np.expand_dims(data_train, axis = -1)\n",
    "#data_train = np.transpose(data_train, (0, 3, 1, 2))\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(data_train, data_train_labels, stratify = data_train_labels, test_size = TEST_SPLIT_RATIO, random_state = SEED)   \n",
    "\n",
    "std_ = np.std(X_train)\n",
    "mean_ = np.mean(X_train)\n",
    "X_train = (X_train - mean_) / std_\n",
    "X_valid = (X_valid - mean_) / std_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c54f1520-0536-4b53-a30a-991169077286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_data_shift(data, u=1.0):\n",
    "    if np.random.random() < u:\n",
    "        shift = int(round(np.random.uniform(-len(data)*0.25, len(data)*0.25)))\n",
    "        data = np.roll(data, shift)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d06685e9-8ed9-4b3c-8ce7-a89bdeecc968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20,) [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n",
      "(20,) [18 19  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17]\n"
     ]
    }
   ],
   "source": [
    "test = np.arange(20)\n",
    "print(test.shape, test)\n",
    "test_out = random_data_shift(test)\n",
    "print(test_out.shape,test_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a6bf254-fc7b-49f2-af44-9ec6bce5c0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AugmentedSTFTDataset(Dataset):\n",
    "    def __init__(self, inputs, targets, n_fft, hop_length, augment=False):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        sample = self.inputs[idx]\n",
    "        #print(sample.shape)\n",
    "        if self.augment:\n",
    "            sample = random_data_shift(sample)\n",
    "            #print(\"OK\")\n",
    "        #print(sample.shape)\n",
    "        # data = librosa.stft(sample, n_fft=self.n_fft, hop_length=self.hop_length)\n",
    "        # data = librosa.amplitude_to_db(np.abs(data), ref=np.max)\n",
    "        #data = np.flipud(data)  # Flip vertically\n",
    "        data = sample.copy()  # Itt készítünk egy másolatot a tömbről, hogy megoldjuk a negatív stride problémát\n",
    "        #data = np.expand_dims(data, axis=-1)  # Add channel dimension\n",
    "        #data = np.transpose(data, (2, 0, 1))  # Reorder dimensions to match PyTorch expectations\n",
    "        return torch.FloatTensor(data), torch.LongTensor([self.targets[idx]])\n",
    "\n",
    "# Data loader\n",
    "def create_dataloader(inputs, targets, batch_size, n_fft, hop_length, shuffle=True, augment=False):\n",
    "    dataset = AugmentedSTFTDataset(inputs, targets, n_fft, hop_length, augment=augment)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e471dc3a-514f-4239-b48f-26af07d92d33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2debff9c-4aae-4748-86f9-dd656c3c6f42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b59a08ee-5808-4805-8cda-a9b6094e3f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = create_dataloader(X_train, y_train, BATCH_SIZE, N_FFT, HOP_LEN, shuffle=True, augment=AUGM)\n",
    "valid_loader = create_dataloader(X_valid, y_valid, BATCH_SIZE, N_FFT, HOP_LEN, shuffle=False, augment=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "efeba4cc-fc21-40a4-bfbf-dd6a1d9af502",
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_FREQ = len(train_loader)//EVAL_FREQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5a10c9b-5bc4-4cc1-a92d-b2a95e29566a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242 60\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader), EVAL_FREQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a84162aa-f6cc-4c06-97fe-0240b18a0c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 4000]) torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_loader:\n",
    "    break\n",
    "print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b323426a-55a5-4cf8-87b5-ab8e2984dbb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 4000]) torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "for x, y in valid_loader:\n",
    "    break\n",
    "print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97aa8121-08c4-4e4a-8b81-6fcad8821ff8",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb480af7-7e5d-4c3b-969c-4b7554ed28bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "#  1d spectral layer - FNO\n",
    "################################################################\n",
    "class SpectralConv1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, modes1):\n",
    "        super(SpectralConv1d, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        1D Fourier layer. It does FFT, linear transform, and Inverse FFT.    \n",
    "        ** Source : https://github.com/neural-operator/fourier_neural_operator **\n",
    "        \"\"\"\n",
    "      \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.modes1 = modes1  #Number of Fourier modes to multiply, at most floor(N/2) + 1\n",
    "\n",
    "        self.scale = (1 / (in_channels*out_channels))\n",
    "        self.weights1 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1, 2, dtype=torch.float))\n",
    "        \n",
    "\n",
    "    # Complex multiplication\n",
    "    def compl_mul1d(self, input, weights):\n",
    "        # (batch, in_channel, x ), (in_channel, out_channel, x) -> (batch, out_channel, x)\n",
    "        return torch.einsum(\"bix,iox->box\", input, weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchsize = x.shape[0]\n",
    "        #Compute Fourier coeffcients up to factor of e^(- something constant)\n",
    "        x_ft = torch.fft.rfft(x)\n",
    "        \n",
    "        # Multiply relevant Fourier modes\n",
    "        out_ft = torch.zeros(batchsize, self.out_channels, x.size(-1)//2 + 1,device = x.device, dtype=torch.cfloat)\n",
    "        out_ft[:, :, :self.modes1] = self.compl_mul1d(x_ft[:, :, :self.modes1], torch.view_as_complex(self.weights1))\n",
    "\n",
    "        # Return to physical space\n",
    "        x = torch.fft.irfft(out_ft, n=x.size(-1))\n",
    "        \n",
    "        return x\n",
    "\n",
    "################################################################\n",
    "#  1d Fourier layer\n",
    "################################################################\n",
    "class FourierLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    A Fourier Layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, padding, stride, modes):\n",
    "        super(FourierLayer, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size,\n",
    "                               padding=padding, stride=stride, bias=True)\n",
    "        self.conv_fno1 = SpectralConv1d(in_channels,out_channels, modes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.conv_fno1(x)\n",
    "        out = x1 + x2\n",
    "        \n",
    "        return out\n",
    "        \n",
    "################################################################\n",
    "#  Residual Block\n",
    "################################################################\n",
    "class ResidualBlock_FNO(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual block\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels, kernel_size, padding, stride, modes):\n",
    "        super(ResidualBlock_FNO, self).__init__()\n",
    "                                   \n",
    "        self.fn1 = FourierLayer(in_channels=channels, out_channels=channels, kernel_size=kernel_size, stride=stride, padding=padding, modes = modes)\n",
    "        self.fn2 = FourierLayer(in_channels=channels, out_channels=channels, kernel_size=kernel_size, stride=stride, padding=padding, modes = modes)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm1d(num_features=channels)\n",
    "        self.bn2 = nn.BatchNorm1d(num_features=channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        \n",
    "        out = F.gelu(self.fn1(x))\n",
    "        out = self.bn1(out)\n",
    "        \n",
    "        out = F.gelu(self.fn2(out))\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        out = out + residual\n",
    "        \n",
    "        return out\n",
    "    \n",
    "################################################################\n",
    "#  Residual Network - FNO\n",
    "################################################################\n",
    "class ResNet9_FNO_small(nn.Module):\n",
    "    \"\"\"\n",
    "    A Residual network.\n",
    "    \"\"\"\n",
    "    def __init__(self,pool_size=2,kernel_size=11,modes=16):\n",
    "        super(ResNet9_FNO_small, self).__init__()\n",
    "        \n",
    "        self.pool_size = pool_size\n",
    "        self.kernel_size = kernel_size\n",
    "        \n",
    "        self.conv1 = FourierLayer(in_channels=1, out_channels=32, kernel_size=self.kernel_size, stride=1, padding=self.kernel_size//2, modes = modes)\n",
    "        self.bn1 = nn.BatchNorm1d(num_features=32)\n",
    "        \n",
    "        self.conv2 = FourierLayer(in_channels=32, out_channels=64, kernel_size=self.kernel_size, stride=1, padding=self.kernel_size//2, modes = modes)\n",
    "        self.bn2 = nn.BatchNorm1d(num_features=64)\n",
    "        \n",
    "        self.rb1 = ResidualBlock_FNO(channels=64, kernel_size=self.kernel_size, stride=1, padding=self.kernel_size//2, modes = modes)\n",
    "        \n",
    "        self.conv3 = FourierLayer(in_channels=64, out_channels=96, kernel_size=self.kernel_size, stride=1, padding=self.kernel_size//2, modes = modes)\n",
    "        self.bn3 = nn.BatchNorm1d(num_features=96)\n",
    "        \n",
    "        self.conv4 = FourierLayer(in_channels=96, out_channels=128, kernel_size=self.kernel_size, stride=1, padding=self.kernel_size//2, modes = modes)\n",
    "        self.bn4 = nn.BatchNorm1d(num_features=128)\n",
    "        \n",
    "        self.rb2 = ResidualBlock_FNO(channels=128, kernel_size=self.kernel_size, stride=1, padding=self.kernel_size//2, modes=modes)\n",
    "\n",
    "        self.gap = torch.nn.AdaptiveAvgPool1d(1)\n",
    "        \n",
    "        self.fc = nn.Linear(in_features=128, out_features = 1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x[:,None,:]\n",
    "        batch_size = len(x)\n",
    "        \n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.bn1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.bn2(x)\n",
    "        \n",
    "        ##################\n",
    "        # 1st residual\n",
    "        ##################\n",
    "        \n",
    "        x = F.avg_pool1d(x,kernel_size=self.pool_size,stride=self.pool_size)\n",
    "        x = self.rb1(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.bn3(x)\n",
    "        \n",
    "        x = F.avg_pool1d(x,kernel_size=self.pool_size,stride=self.pool_size)\n",
    "        \n",
    "        x = self.conv4(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.bn4(x)\n",
    "        \n",
    "        ##################\n",
    "        # 2nd residual\n",
    "        ##################\n",
    "        \n",
    "        x = F.avg_pool1d(x,kernel_size=self.pool_size,stride=self.pool_size)\n",
    "        x = self.rb2(x)\n",
    "                \n",
    "        x = self.gap(x)\n",
    "        x = x.view(batch_size,-1)\n",
    "        \n",
    "        \n",
    "        out = self.fc(x)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fad4ce4b-f93b-4945-97d0-010181735fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet9_FNO_small()\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "14e9fbb2-df81-4a42-8ce7-8ddfbc4887e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary(model, (1, 4000)) # it doesn't work for resnet9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95167e3b-192e-4076-b4e0-a576a3c0d810",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2a5fbf62-5852-4d05-8f21-56c458e791f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "83695a75-92cb-4df8-b695-3cbb44b1dd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion, device):\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "    epoch_acc = 0.0\n",
    "\n",
    "    model.eval()\n",
    "    number_of_elements = 0\n",
    "    \n",
    "    correct_pred = torch.zeros(2)\n",
    "    total_pred = torch.zeros(2)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        for x, y in iterator:\n",
    "\n",
    "            x = x.to(device)\n",
    "            y = y.float().to(device).view(-1,1)\n",
    "            \n",
    "            batch_size = x.shape[0]\n",
    "            number_of_elements += batch_size\n",
    "            \n",
    "            pred = model(x).view(-1,1)\n",
    "            loss = criterion(pred, y)\n",
    "            \n",
    "            top_pred = (torch.sigmoid(pred) > 0.5).int()\n",
    "            acc = top_pred.eq(y.int().view_as(top_pred)).sum()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            \n",
    "            y_true.append(y.int().cpu().numpy())\n",
    "            y_pred.append(top_pred.cpu().numpy())\n",
    "            \n",
    "        y_true_a = np.concatenate(y_true, axis=0)\n",
    "        y_pred_a = np.concatenate(y_pred, axis=0)\n",
    "                        \n",
    "        #balanced_acc = balanced_accuracy_score(y_true_a, y_pred_a)\n",
    "        acc = accuracy_score(y_true_a, y_pred_a)\n",
    "\n",
    "    return epoch_loss / number_of_elements, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8ac50b-fdaf-4973-806b-274f46017039",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9233037b-22f8-4668-8ae9-b01dcb7a2c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of the parameters: 2645537\n",
      "\n",
      "Training\n",
      "-----    0    ----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 001 | Batch Id: 00060 | ET: 5.76s | lr: 1.04e-05 | Val. Loss: 0.563 |  Val. Acc: 73.94% | B. Val. Loss: 0.563 |  B. Val. Acc: 73.94%\n",
      "\t | Epoch: 001 | Batch Id: 00120 | ET: 12.14s | lr: 1.15e-05 | Val. Loss: 0.550 |  Val. Acc: 72.74% | B. Val. Loss: 0.550 |  B. Val. Acc: 73.94%\n",
      "\t | Epoch: 001 | Batch Id: 00180 | ET: 18.50s | lr: 1.33e-05 | Val. Loss: 0.516 |  Val. Acc: 73.94% | B. Val. Loss: 0.516 |  B. Val. Acc: 73.94%\n",
      "\t | Epoch: 001 | Batch Id: 00240 | ET: 24.86s | lr: 1.59e-05 | Val. Loss: 0.501 |  Val. Acc: 75.77% | B. Val. Loss: 0.501 |  B. Val. Acc: 75.77%\n",
      "Epoch: 001 | ET: 35.61s | \t Train Loss: 0.487 | Train Acc: 76.06% \t Val. Loss: 0.501 |  Val. Acc: 75.73% \t | B. Val. Loss: 0.501 |  B. Val. Acc: 75.77%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 002 | Batch Id: 00060 | ET: 4.42s | lr: 1.92e-05 | Val. Loss: 0.506 |  Val. Acc: 75.03% | B. Val. Loss: 0.501 |  B. Val. Acc: 75.77%\n",
      "\t | Epoch: 002 | Batch Id: 00120 | ET: 10.78s | lr: 2.31e-05 | Val. Loss: 0.476 |  Val. Acc: 78.67% | B. Val. Loss: 0.476 |  B. Val. Acc: 78.67%\n",
      "\t | Epoch: 002 | Batch Id: 00180 | ET: 17.22s | lr: 2.74e-05 | Val. Loss: 0.461 |  Val. Acc: 78.87% | B. Val. Loss: 0.461 |  B. Val. Acc: 78.87%\n",
      "\t | Epoch: 002 | Batch Id: 00240 | ET: 23.68s | lr: 3.23e-05 | Val. Loss: 0.465 |  Val. Acc: 78.36% | B. Val. Loss: 0.461 |  B. Val. Acc: 78.87%\n",
      "Epoch: 002 | ET: 34.39s | \t Train Loss: 0.453 | Train Acc: 78.47% \t Val. Loss: 0.460 |  Val. Acc: 78.83% \t | B. Val. Loss: 0.460 |  B. Val. Acc: 78.87%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 003 | Batch Id: 00060 | ET: 4.43s | lr: 3.77e-05 | Val. Loss: 0.476 |  Val. Acc: 77.78% | B. Val. Loss: 0.460 |  B. Val. Acc: 78.87%\n",
      "\t | Epoch: 003 | Batch Id: 00120 | ET: 10.78s | lr: 4.32e-05 | Val. Loss: 0.456 |  Val. Acc: 79.29% | B. Val. Loss: 0.456 |  B. Val. Acc: 79.29%\n",
      "\t | Epoch: 003 | Batch Id: 00180 | ET: 17.26s | lr: 4.89e-05 | Val. Loss: 0.465 |  Val. Acc: 79.02% | B. Val. Loss: 0.456 |  B. Val. Acc: 79.29%\n",
      "\t | Epoch: 003 | Batch Id: 00240 | ET: 23.62s | lr: 5.48e-05 | Val. Loss: 0.473 |  Val. Acc: 78.75% | B. Val. Loss: 0.456 |  B. Val. Acc: 79.29%\n",
      "Epoch: 003 | ET: 34.33s | \t Train Loss: 0.451 | Train Acc: 79.42% \t Val. Loss: 0.457 |  Val. Acc: 79.18% \t | B. Val. Loss: 0.456 |  B. Val. Acc: 79.29%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 004 | Batch Id: 00060 | ET: 4.42s | lr: 6.08e-05 | Val. Loss: 0.474 |  Val. Acc: 78.52% | B. Val. Loss: 0.456 |  B. Val. Acc: 79.29%\n",
      "\t | Epoch: 004 | Batch Id: 00120 | ET: 10.80s | lr: 6.65e-05 | Val. Loss: 0.456 |  Val. Acc: 79.57% | B. Val. Loss: 0.456 |  B. Val. Acc: 79.57%\n",
      "\t | Epoch: 004 | Batch Id: 00180 | ET: 17.28s | lr: 7.21e-05 | Val. Loss: 0.487 |  Val. Acc: 79.88% | B. Val. Loss: 0.456 |  B. Val. Acc: 79.88%\n",
      "\t | Epoch: 004 | Batch Id: 00240 | ET: 23.77s | lr: 7.73e-05 | Val. Loss: 0.446 |  Val. Acc: 79.60% | B. Val. Loss: 0.446 |  B. Val. Acc: 79.88%\n",
      "Epoch: 004 | ET: 34.48s | \t Train Loss: 0.432 | Train Acc: 79.97% \t Val. Loss: 0.449 |  Val. Acc: 79.57% \t | B. Val. Loss: 0.446 |  B. Val. Acc: 79.88%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 005 | Batch Id: 00060 | ET: 4.42s | lr: 8.23e-05 | Val. Loss: 0.456 |  Val. Acc: 79.64% | B. Val. Loss: 0.446 |  B. Val. Acc: 79.88%\n",
      "\t | Epoch: 005 | Batch Id: 00120 | ET: 10.79s | lr: 8.67e-05 | Val. Loss: 0.427 |  Val. Acc: 80.81% | B. Val. Loss: 0.427 |  B. Val. Acc: 80.81%\n",
      "\t | Epoch: 005 | Batch Id: 00180 | ET: 17.26s | lr: 9.06e-05 | Val. Loss: 0.405 |  Val. Acc: 82.63% | B. Val. Loss: 0.405 |  B. Val. Acc: 82.63%\n",
      "\t | Epoch: 005 | Batch Id: 00240 | ET: 23.73s | lr: 9.39e-05 | Val. Loss: 0.335 |  Val. Acc: 86.47% | B. Val. Loss: 0.335 |  B. Val. Acc: 86.47%\n",
      "Epoch: 005 | ET: 34.48s | \t Train Loss: 0.317 | Train Acc: 87.41% \t Val. Loss: 0.337 |  Val. Acc: 86.39% \t | B. Val. Loss: 0.335 |  B. Val. Acc: 86.47%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 006 | Batch Id: 00060 | ET: 4.42s | lr: 9.66e-05 | Val. Loss: 0.382 |  Val. Acc: 84.37% | B. Val. Loss: 0.335 |  B. Val. Acc: 86.47%\n",
      "\t | Epoch: 006 | Batch Id: 00120 | ET: 10.80s | lr: 9.84e-05 | Val. Loss: 0.330 |  Val. Acc: 85.96% | B. Val. Loss: 0.330 |  B. Val. Acc: 86.47%\n",
      "\t | Epoch: 006 | Batch Id: 00180 | ET: 17.17s | lr: 9.96e-05 | Val. Loss: 0.320 |  Val. Acc: 85.92% | B. Val. Loss: 0.320 |  B. Val. Acc: 86.47%\n",
      "\t | Epoch: 006 | Batch Id: 00240 | ET: 23.52s | lr: 1.00e-04 | Val. Loss: 0.284 |  Val. Acc: 89.22% | B. Val. Loss: 0.284 |  B. Val. Acc: 89.22%\n",
      "Epoch: 006 | ET: 34.32s | \t Train Loss: 0.253 | Train Acc: 89.48% \t Val. Loss: 0.279 |  Val. Acc: 89.03% \t | B. Val. Loss: 0.279 |  B. Val. Acc: 89.22%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 007 | Batch Id: 00060 | ET: 4.43s | lr: 9.99e-05 | Val. Loss: 0.349 |  Val. Acc: 86.47% | B. Val. Loss: 0.279 |  B. Val. Acc: 89.22%\n",
      "\t | Epoch: 007 | Batch Id: 00120 | ET: 10.80s | lr: 9.97e-05 | Val. Loss: 0.304 |  Val. Acc: 88.25% | B. Val. Loss: 0.279 |  B. Val. Acc: 89.22%\n",
      "\t | Epoch: 007 | Batch Id: 00180 | ET: 17.16s | lr: 9.93e-05 | Val. Loss: 0.305 |  Val. Acc: 87.75% | B. Val. Loss: 0.279 |  B. Val. Acc: 89.22%\n",
      "\t | Epoch: 007 | Batch Id: 00240 | ET: 23.52s | lr: 9.88e-05 | Val. Loss: 0.275 |  Val. Acc: 89.26% | B. Val. Loss: 0.275 |  B. Val. Acc: 89.26%\n",
      "Epoch: 007 | ET: 34.29s | \t Train Loss: 0.245 | Train Acc: 90.18% \t Val. Loss: 0.275 |  Val. Acc: 88.21% \t | B. Val. Loss: 0.275 |  B. Val. Acc: 89.26%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 008 | Batch Id: 00060 | ET: 4.43s | lr: 9.81e-05 | Val. Loss: 0.325 |  Val. Acc: 85.58% | B. Val. Loss: 0.275 |  B. Val. Acc: 89.26%\n",
      "\t | Epoch: 008 | Batch Id: 00120 | ET: 10.79s | lr: 9.72e-05 | Val. Loss: 0.277 |  Val. Acc: 88.87% | B. Val. Loss: 0.275 |  B. Val. Acc: 89.26%\n",
      "\t | Epoch: 008 | Batch Id: 00180 | ET: 17.16s | lr: 9.63e-05 | Val. Loss: 0.269 |  Val. Acc: 90.15% | B. Val. Loss: 0.269 |  B. Val. Acc: 90.15%\n",
      "\t | Epoch: 008 | Batch Id: 00240 | ET: 23.66s | lr: 9.51e-05 | Val. Loss: 0.270 |  Val. Acc: 90.00% | B. Val. Loss: 0.269 |  B. Val. Acc: 90.15%\n",
      "Epoch: 008 | ET: 34.39s | \t Train Loss: 0.223 | Train Acc: 91.38% \t Val. Loss: 0.266 |  Val. Acc: 89.69% \t | B. Val. Loss: 0.266 |  B. Val. Acc: 90.15%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 009 | Batch Id: 00060 | ET: 4.43s | lr: 9.38e-05 | Val. Loss: 0.294 |  Val. Acc: 88.52% | B. Val. Loss: 0.266 |  B. Val. Acc: 90.15%\n",
      "\t | Epoch: 009 | Batch Id: 00120 | ET: 10.80s | lr: 9.24e-05 | Val. Loss: 0.269 |  Val. Acc: 88.87% | B. Val. Loss: 0.266 |  B. Val. Acc: 90.15%\n",
      "\t | Epoch: 009 | Batch Id: 00180 | ET: 17.17s | lr: 9.09e-05 | Val. Loss: 0.262 |  Val. Acc: 89.41% | B. Val. Loss: 0.262 |  B. Val. Acc: 90.15%\n",
      "\t | Epoch: 009 | Batch Id: 00240 | ET: 23.54s | lr: 8.93e-05 | Val. Loss: 0.304 |  Val. Acc: 88.52% | B. Val. Loss: 0.262 |  B. Val. Acc: 90.15%\n",
      "Epoch: 009 | ET: 34.19s | \t Train Loss: 0.280 | Train Acc: 89.96% \t Val. Loss: 0.342 |  Val. Acc: 87.40% \t | B. Val. Loss: 0.262 |  B. Val. Acc: 90.15%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 010 | Batch Id: 00060 | ET: 4.43s | lr: 8.74e-05 | Val. Loss: 0.274 |  Val. Acc: 89.22% | B. Val. Loss: 0.262 |  B. Val. Acc: 90.15%\n",
      "\t | Epoch: 010 | Batch Id: 00120 | ET: 10.79s | lr: 8.55e-05 | Val. Loss: 0.266 |  Val. Acc: 89.18% | B. Val. Loss: 0.262 |  B. Val. Acc: 90.15%\n",
      "\t | Epoch: 010 | Batch Id: 00180 | ET: 17.15s | lr: 8.35e-05 | Val. Loss: 0.262 |  Val. Acc: 89.30% | B. Val. Loss: 0.262 |  B. Val. Acc: 90.15%\n",
      "\t | Epoch: 010 | Batch Id: 00240 | ET: 23.51s | lr: 8.14e-05 | Val. Loss: 0.257 |  Val. Acc: 89.49% | B. Val. Loss: 0.257 |  B. Val. Acc: 90.15%\n",
      "Epoch: 010 | ET: 34.21s | \t Train Loss: 0.209 | Train Acc: 91.59% \t Val. Loss: 0.259 |  Val. Acc: 89.07% \t | B. Val. Loss: 0.257 |  B. Val. Acc: 90.15%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 011 | Batch Id: 00060 | ET: 4.43s | lr: 7.92e-05 | Val. Loss: 0.273 |  Val. Acc: 89.57% | B. Val. Loss: 0.257 |  B. Val. Acc: 90.15%\n",
      "\t | Epoch: 011 | Batch Id: 00120 | ET: 10.79s | lr: 7.69e-05 | Val. Loss: 0.270 |  Val. Acc: 89.92% | B. Val. Loss: 0.257 |  B. Val. Acc: 90.15%\n",
      "\t | Epoch: 011 | Batch Id: 00180 | ET: 17.16s | lr: 7.45e-05 | Val. Loss: 0.257 |  Val. Acc: 89.61% | B. Val. Loss: 0.257 |  B. Val. Acc: 90.15%\n",
      "\t | Epoch: 011 | Batch Id: 00240 | ET: 23.53s | lr: 7.21e-05 | Val. Loss: 0.258 |  Val. Acc: 89.61% | B. Val. Loss: 0.257 |  B. Val. Acc: 90.15%\n",
      "Epoch: 011 | ET: 34.27s | \t Train Loss: 0.202 | Train Acc: 92.27% \t Val. Loss: 0.263 |  Val. Acc: 89.38% \t | B. Val. Loss: 0.257 |  B. Val. Acc: 90.15%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 012 | Batch Id: 00060 | ET: 4.43s | lr: 6.95e-05 | Val. Loss: 0.259 |  Val. Acc: 89.34% | B. Val. Loss: 0.257 |  B. Val. Acc: 90.15%\n",
      "\t | Epoch: 012 | Batch Id: 00120 | ET: 10.80s | lr: 6.69e-05 | Val. Loss: 0.258 |  Val. Acc: 89.49% | B. Val. Loss: 0.257 |  B. Val. Acc: 90.15%\n",
      "\t | Epoch: 012 | Batch Id: 00180 | ET: 17.16s | lr: 6.43e-05 | Val. Loss: 0.258 |  Val. Acc: 89.61% | B. Val. Loss: 0.257 |  B. Val. Acc: 90.15%\n",
      "\t | Epoch: 012 | Batch Id: 00240 | ET: 23.52s | lr: 6.16e-05 | Val. Loss: 0.247 |  Val. Acc: 90.58% | B. Val. Loss: 0.247 |  B. Val. Acc: 90.58%\n",
      "Epoch: 012 | ET: 34.36s | \t Train Loss: 0.193 | Train Acc: 92.58% \t Val. Loss: 0.248 |  Val. Acc: 90.27% \t | B. Val. Loss: 0.247 |  B. Val. Acc: 90.58%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 013 | Batch Id: 00060 | ET: 4.43s | lr: 5.88e-05 | Val. Loss: 0.267 |  Val. Acc: 88.76% | B. Val. Loss: 0.247 |  B. Val. Acc: 90.58%\n",
      "\t | Epoch: 013 | Batch Id: 00120 | ET: 10.81s | lr: 5.61e-05 | Val. Loss: 0.269 |  Val. Acc: 88.99% | B. Val. Loss: 0.247 |  B. Val. Acc: 90.58%\n",
      "\t | Epoch: 013 | Batch Id: 00180 | ET: 17.17s | lr: 5.33e-05 | Val. Loss: 0.250 |  Val. Acc: 90.35% | B. Val. Loss: 0.247 |  B. Val. Acc: 90.58%\n",
      "\t | Epoch: 013 | Batch Id: 00240 | ET: 23.52s | lr: 5.06e-05 | Val. Loss: 0.247 |  Val. Acc: 90.00% | B. Val. Loss: 0.247 |  B. Val. Acc: 90.58%\n",
      "Epoch: 013 | ET: 34.23s | \t Train Loss: 0.187 | Train Acc: 92.76% \t Val. Loss: 0.247 |  Val. Acc: 90.11% \t | B. Val. Loss: 0.247 |  B. Val. Acc: 90.58%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 014 | Batch Id: 00060 | ET: 4.43s | lr: 4.77e-05 | Val. Loss: 0.269 |  Val. Acc: 89.84% | B. Val. Loss: 0.247 |  B. Val. Acc: 90.58%\n",
      "\t | Epoch: 014 | Batch Id: 00120 | ET: 10.80s | lr: 4.50e-05 | Val. Loss: 0.259 |  Val. Acc: 89.18% | B. Val. Loss: 0.247 |  B. Val. Acc: 90.58%\n",
      "\t | Epoch: 014 | Batch Id: 00180 | ET: 17.16s | lr: 4.23e-05 | Val. Loss: 0.253 |  Val. Acc: 90.15% | B. Val. Loss: 0.247 |  B. Val. Acc: 90.58%\n",
      "\t | Epoch: 014 | Batch Id: 00240 | ET: 23.51s | lr: 3.96e-05 | Val. Loss: 0.250 |  Val. Acc: 90.19% | B. Val. Loss: 0.247 |  B. Val. Acc: 90.58%\n",
      "Epoch: 014 | ET: 34.21s | \t Train Loss: 0.186 | Train Acc: 93.12% \t Val. Loss: 0.250 |  Val. Acc: 90.03% \t | B. Val. Loss: 0.247 |  B. Val. Acc: 90.58%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 015 | Batch Id: 00060 | ET: 4.42s | lr: 3.68e-05 | Val. Loss: 0.250 |  Val. Acc: 89.84% | B. Val. Loss: 0.247 |  B. Val. Acc: 90.58%\n",
      "\t | Epoch: 015 | Batch Id: 00120 | ET: 10.78s | lr: 3.42e-05 | Val. Loss: 0.251 |  Val. Acc: 90.15% | B. Val. Loss: 0.247 |  B. Val. Acc: 90.58%\n",
      "\t | Epoch: 015 | Batch Id: 00180 | ET: 17.14s | lr: 3.16e-05 | Val. Loss: 0.251 |  Val. Acc: 90.00% | B. Val. Loss: 0.247 |  B. Val. Acc: 90.58%\n",
      "\t | Epoch: 015 | Batch Id: 00240 | ET: 23.49s | lr: 2.91e-05 | Val. Loss: 0.249 |  Val. Acc: 90.31% | B. Val. Loss: 0.247 |  B. Val. Acc: 90.58%\n",
      "Epoch: 015 | ET: 34.19s | \t Train Loss: 0.180 | Train Acc: 92.80% \t Val. Loss: 0.248 |  Val. Acc: 90.23% \t | B. Val. Loss: 0.247 |  B. Val. Acc: 90.58%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 016 | Batch Id: 00060 | ET: 4.42s | lr: 2.66e-05 | Val. Loss: 0.252 |  Val. Acc: 90.19% | B. Val. Loss: 0.247 |  B. Val. Acc: 90.58%\n",
      "\t | Epoch: 016 | Batch Id: 00120 | ET: 10.77s | lr: 2.42e-05 | Val. Loss: 0.248 |  Val. Acc: 90.11% | B. Val. Loss: 0.247 |  B. Val. Acc: 90.58%\n",
      "\t | Epoch: 016 | Batch Id: 00180 | ET: 17.13s | lr: 2.19e-05 | Val. Loss: 0.254 |  Val. Acc: 90.07% | B. Val. Loss: 0.247 |  B. Val. Acc: 90.58%\n",
      "\t | Epoch: 016 | Batch Id: 00240 | ET: 23.49s | lr: 1.97e-05 | Val. Loss: 0.252 |  Val. Acc: 90.19% | B. Val. Loss: 0.247 |  B. Val. Acc: 90.58%\n",
      "Epoch: 016 | ET: 34.19s | \t Train Loss: 0.182 | Train Acc: 93.01% \t Val. Loss: 0.253 |  Val. Acc: 90.19% \t | B. Val. Loss: 0.247 |  B. Val. Acc: 90.58%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 017 | Batch Id: 00060 | ET: 4.42s | lr: 1.75e-05 | Val. Loss: 0.257 |  Val. Acc: 90.03% | B. Val. Loss: 0.247 |  B. Val. Acc: 90.58%\n",
      "\t | Epoch: 017 | Batch Id: 00120 | ET: 10.78s | lr: 1.55e-05 | Val. Loss: 0.250 |  Val. Acc: 90.03% | B. Val. Loss: 0.247 |  B. Val. Acc: 90.58%\n",
      "\t | Epoch: 017 | Batch Id: 00180 | ET: 17.13s | lr: 1.36e-05 | Val. Loss: 0.252 |  Val. Acc: 90.27% | B. Val. Loss: 0.247 |  B. Val. Acc: 90.58%\n",
      "\t | Epoch: 017 | Batch Id: 00240 | ET: 23.48s | lr: 1.19e-05 | Val. Loss: 0.248 |  Val. Acc: 89.72% | B. Val. Loss: 0.247 |  B. Val. Acc: 90.58%\n",
      "Epoch: 017 | ET: 34.17s | \t Train Loss: 0.181 | Train Acc: 93.05% \t Val. Loss: 0.248 |  Val. Acc: 89.88% \t | B. Val. Loss: 0.247 |  B. Val. Acc: 90.58%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 018 | Batch Id: 00060 | ET: 4.42s | lr: 1.01e-05 | Val. Loss: 0.274 |  Val. Acc: 90.11% | B. Val. Loss: 0.247 |  B. Val. Acc: 90.58%\n",
      "\t | Epoch: 018 | Batch Id: 00120 | ET: 10.77s | lr: 8.61e-06 | Val. Loss: 0.251 |  Val. Acc: 89.92% | B. Val. Loss: 0.247 |  B. Val. Acc: 90.58%\n",
      "\t | Epoch: 018 | Batch Id: 00180 | ET: 17.12s | lr: 7.21e-06 | Val. Loss: 0.255 |  Val. Acc: 89.34% | B. Val. Loss: 0.247 |  B. Val. Acc: 90.58%\n",
      "\t | Epoch: 018 | Batch Id: 00240 | ET: 23.47s | lr: 5.94e-06 | Val. Loss: 0.251 |  Val. Acc: 90.27% | B. Val. Loss: 0.247 |  B. Val. Acc: 90.58%\n",
      "Epoch: 018 | ET: 34.20s | \t Train Loss: 0.185 | Train Acc: 93.07% \t Val. Loss: 0.251 |  Val. Acc: 90.27% \t | B. Val. Loss: 0.247 |  B. Val. Acc: 90.58%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 019 | Batch Id: 00060 | ET: 4.41s | lr: 4.78e-06 | Val. Loss: 0.258 |  Val. Acc: 89.96% | B. Val. Loss: 0.247 |  B. Val. Acc: 90.58%\n",
      "\t | Epoch: 019 | Batch Id: 00120 | ET: 10.78s | lr: 3.79e-06 | Val. Loss: 0.254 |  Val. Acc: 90.03% | B. Val. Loss: 0.247 |  B. Val. Acc: 90.58%\n",
      "\t | Epoch: 019 | Batch Id: 00180 | ET: 17.13s | lr: 2.95e-06 | Val. Loss: 0.250 |  Val. Acc: 89.96% | B. Val. Loss: 0.247 |  B. Val. Acc: 90.58%\n",
      "\t | Epoch: 019 | Batch Id: 00240 | ET: 23.48s | lr: 2.26e-06 | Val. Loss: 0.247 |  Val. Acc: 90.15% | B. Val. Loss: 0.247 |  B. Val. Acc: 90.58%\n",
      "Epoch: 019 | ET: 34.17s | \t Train Loss: 0.183 | Train Acc: 92.96% \t Val. Loss: 0.247 |  Val. Acc: 90.15% \t | B. Val. Loss: 0.247 |  B. Val. Acc: 90.58%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 020 | Batch Id: 00060 | ET: 4.43s | lr: 1.70e-06 | Val. Loss: 0.247 |  Val. Acc: 90.07% | B. Val. Loss: 0.247 |  B. Val. Acc: 90.58%\n",
      "\t | Epoch: 020 | Batch Id: 00120 | ET: 10.78s | lr: 1.32e-06 | Val. Loss: 0.247 |  Val. Acc: 90.19% | B. Val. Loss: 0.247 |  B. Val. Acc: 90.58%\n",
      "\t | Epoch: 020 | Batch Id: 00180 | ET: 17.15s | lr: 1.08e-06 | Val. Loss: 0.248 |  Val. Acc: 90.19% | B. Val. Loss: 0.247 |  B. Val. Acc: 90.58%\n",
      "\t | Epoch: 020 | Batch Id: 00240 | ET: 23.50s | lr: 1.00e-06 | Val. Loss: 0.248 |  Val. Acc: 90.23% | B. Val. Loss: 0.247 |  B. Val. Acc: 90.58%\n",
      "Epoch: 020 | ET: 34.22s | \t Train Loss: 0.180 | Train Acc: 92.93% \t Val. Loss: 0.248 |  Val. Acc: 90.23% \t | B. Val. Loss: 0.247 |  B. Val. Acc: 90.58%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Number of the parameters: 2645537\n",
      "\n",
      "Training\n",
      "-----    1    ----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 001 | Batch Id: 00060 | ET: 4.42s | lr: 1.04e-05 | Val. Loss: 0.538 |  Val. Acc: 72.31% | B. Val. Loss: 0.538 |  B. Val. Acc: 72.31%\n",
      "\t | Epoch: 001 | Batch Id: 00120 | ET: 10.80s | lr: 1.15e-05 | Val. Loss: 0.503 |  Val. Acc: 75.38% | B. Val. Loss: 0.503 |  B. Val. Acc: 75.38%\n",
      "\t | Epoch: 001 | Batch Id: 00180 | ET: 17.23s | lr: 1.33e-05 | Val. Loss: 0.494 |  Val. Acc: 75.65% | B. Val. Loss: 0.494 |  B. Val. Acc: 75.65%\n",
      "\t | Epoch: 001 | Batch Id: 00240 | ET: 23.69s | lr: 1.59e-05 | Val. Loss: 0.482 |  Val. Acc: 78.09% | B. Val. Loss: 0.482 |  B. Val. Acc: 78.09%\n",
      "Epoch: 001 | ET: 34.54s | \t Train Loss: 0.467 | Train Acc: 77.39% \t Val. Loss: 0.481 |  Val. Acc: 77.55% \t | B. Val. Loss: 0.481 |  B. Val. Acc: 78.09%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 002 | Batch Id: 00060 | ET: 4.42s | lr: 1.92e-05 | Val. Loss: 0.521 |  Val. Acc: 74.56% | B. Val. Loss: 0.481 |  B. Val. Acc: 78.09%\n",
      "\t | Epoch: 002 | Batch Id: 00120 | ET: 10.78s | lr: 2.31e-05 | Val. Loss: 0.469 |  Val. Acc: 78.01% | B. Val. Loss: 0.469 |  B. Val. Acc: 78.09%\n",
      "\t | Epoch: 002 | Batch Id: 00180 | ET: 17.15s | lr: 2.74e-05 | Val. Loss: 0.466 |  Val. Acc: 77.98% | B. Val. Loss: 0.466 |  B. Val. Acc: 78.09%\n",
      "\t | Epoch: 002 | Batch Id: 00240 | ET: 23.50s | lr: 3.23e-05 | Val. Loss: 0.457 |  Val. Acc: 79.60% | B. Val. Loss: 0.457 |  B. Val. Acc: 79.60%\n",
      "Epoch: 002 | ET: 34.32s | \t Train Loss: 0.451 | Train Acc: 78.80% \t Val. Loss: 0.458 |  Val. Acc: 79.29% \t | B. Val. Loss: 0.457 |  B. Val. Acc: 79.60%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 003 | Batch Id: 00060 | ET: 4.43s | lr: 3.77e-05 | Val. Loss: 0.480 |  Val. Acc: 77.39% | B. Val. Loss: 0.457 |  B. Val. Acc: 79.60%\n",
      "\t | Epoch: 003 | Batch Id: 00120 | ET: 10.79s | lr: 4.32e-05 | Val. Loss: 0.478 |  Val. Acc: 79.45% | B. Val. Loss: 0.457 |  B. Val. Acc: 79.60%\n",
      "\t | Epoch: 003 | Batch Id: 00180 | ET: 17.15s | lr: 4.89e-05 | Val. Loss: 0.451 |  Val. Acc: 79.49% | B. Val. Loss: 0.451 |  B. Val. Acc: 79.60%\n",
      "\t | Epoch: 003 | Batch Id: 00240 | ET: 23.52s | lr: 5.48e-05 | Val. Loss: 0.455 |  Val. Acc: 79.53% | B. Val. Loss: 0.451 |  B. Val. Acc: 79.60%\n",
      "Epoch: 003 | ET: 34.37s | \t Train Loss: 0.430 | Train Acc: 80.10% \t Val. Loss: 0.450 |  Val. Acc: 79.84% \t | B. Val. Loss: 0.450 |  B. Val. Acc: 79.84%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 004 | Batch Id: 00060 | ET: 4.42s | lr: 6.08e-05 | Val. Loss: 0.469 |  Val. Acc: 77.98% | B. Val. Loss: 0.450 |  B. Val. Acc: 79.84%\n",
      "\t | Epoch: 004 | Batch Id: 00120 | ET: 10.78s | lr: 6.65e-05 | Val. Loss: 0.445 |  Val. Acc: 79.95% | B. Val. Loss: 0.445 |  B. Val. Acc: 79.95%\n",
      "\t | Epoch: 004 | Batch Id: 00180 | ET: 17.23s | lr: 7.21e-05 | Val. Loss: 0.413 |  Val. Acc: 81.35% | B. Val. Loss: 0.413 |  B. Val. Acc: 81.35%\n",
      "\t | Epoch: 004 | Batch Id: 00240 | ET: 23.71s | lr: 7.73e-05 | Val. Loss: 0.396 |  Val. Acc: 83.60% | B. Val. Loss: 0.396 |  B. Val. Acc: 83.60%\n",
      "Epoch: 004 | ET: 34.61s | \t Train Loss: 0.362 | Train Acc: 85.27% \t Val. Loss: 0.387 |  Val. Acc: 84.37% \t | B. Val. Loss: 0.387 |  B. Val. Acc: 84.37%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 005 | Batch Id: 00060 | ET: 4.42s | lr: 8.23e-05 | Val. Loss: 0.418 |  Val. Acc: 80.92% | B. Val. Loss: 0.387 |  B. Val. Acc: 84.37%\n",
      "\t | Epoch: 005 | Batch Id: 00120 | ET: 10.77s | lr: 8.67e-05 | Val. Loss: 0.373 |  Val. Acc: 83.64% | B. Val. Loss: 0.373 |  B. Val. Acc: 84.37%\n",
      "\t | Epoch: 005 | Batch Id: 00180 | ET: 17.12s | lr: 9.06e-05 | Val. Loss: 0.358 |  Val. Acc: 83.87% | B. Val. Loss: 0.358 |  B. Val. Acc: 84.37%\n",
      "\t | Epoch: 005 | Batch Id: 00240 | ET: 23.47s | lr: 9.39e-05 | Val. Loss: 0.364 |  Val. Acc: 87.44% | B. Val. Loss: 0.358 |  B. Val. Acc: 87.44%\n",
      "Epoch: 005 | ET: 34.29s | \t Train Loss: 0.329 | Train Acc: 86.97% \t Val. Loss: 0.364 |  Val. Acc: 86.12% \t | B. Val. Loss: 0.358 |  B. Val. Acc: 87.44%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 006 | Batch Id: 00060 | ET: 4.41s | lr: 9.66e-05 | Val. Loss: 0.366 |  Val. Acc: 83.91% | B. Val. Loss: 0.358 |  B. Val. Acc: 87.44%\n",
      "\t | Epoch: 006 | Batch Id: 00120 | ET: 10.77s | lr: 9.84e-05 | Val. Loss: 0.309 |  Val. Acc: 87.94% | B. Val. Loss: 0.309 |  B. Val. Acc: 87.94%\n",
      "\t | Epoch: 006 | Batch Id: 00180 | ET: 17.24s | lr: 9.96e-05 | Val. Loss: 0.303 |  Val. Acc: 87.79% | B. Val. Loss: 0.303 |  B. Val. Acc: 87.94%\n",
      "\t | Epoch: 006 | Batch Id: 00240 | ET: 23.58s | lr: 1.00e-04 | Val. Loss: 0.284 |  Val. Acc: 89.26% | B. Val. Loss: 0.284 |  B. Val. Acc: 89.26%\n",
      "Epoch: 006 | ET: 34.34s | \t Train Loss: 0.246 | Train Acc: 90.67% \t Val. Loss: 0.285 |  Val. Acc: 88.99% \t | B. Val. Loss: 0.284 |  B. Val. Acc: 89.26%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 007 | Batch Id: 00060 | ET: 4.41s | lr: 9.99e-05 | Val. Loss: 0.326 |  Val. Acc: 88.48% | B. Val. Loss: 0.284 |  B. Val. Acc: 89.26%\n",
      "\t | Epoch: 007 | Batch Id: 00120 | ET: 10.77s | lr: 9.97e-05 | Val. Loss: 0.287 |  Val. Acc: 89.26% | B. Val. Loss: 0.284 |  B. Val. Acc: 89.26%\n",
      "\t | Epoch: 007 | Batch Id: 00180 | ET: 17.11s | lr: 9.93e-05 | Val. Loss: 0.285 |  Val. Acc: 88.41% | B. Val. Loss: 0.284 |  B. Val. Acc: 89.26%\n",
      "\t | Epoch: 007 | Batch Id: 00240 | ET: 23.46s | lr: 9.88e-05 | Val. Loss: 0.274 |  Val. Acc: 89.41% | B. Val. Loss: 0.274 |  B. Val. Acc: 89.41%\n",
      "Epoch: 007 | ET: 34.42s | \t Train Loss: 0.228 | Train Acc: 91.21% \t Val. Loss: 0.271 |  Val. Acc: 90.07% \t | B. Val. Loss: 0.271 |  B. Val. Acc: 90.07%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 008 | Batch Id: 00060 | ET: 4.42s | lr: 9.81e-05 | Val. Loss: 0.306 |  Val. Acc: 87.48% | B. Val. Loss: 0.271 |  B. Val. Acc: 90.07%\n",
      "\t | Epoch: 008 | Batch Id: 00120 | ET: 10.78s | lr: 9.72e-05 | Val. Loss: 0.272 |  Val. Acc: 89.45% | B. Val. Loss: 0.271 |  B. Val. Acc: 90.07%\n",
      "\t | Epoch: 008 | Batch Id: 00180 | ET: 17.14s | lr: 9.63e-05 | Val. Loss: 0.269 |  Val. Acc: 89.88% | B. Val. Loss: 0.269 |  B. Val. Acc: 90.07%\n",
      "\t | Epoch: 008 | Batch Id: 00240 | ET: 23.49s | lr: 9.51e-05 | Val. Loss: 0.263 |  Val. Acc: 89.72% | B. Val. Loss: 0.263 |  B. Val. Acc: 90.07%\n",
      "Epoch: 008 | ET: 34.20s | \t Train Loss: 0.222 | Train Acc: 91.48% \t Val. Loss: 0.264 |  Val. Acc: 89.65% \t | B. Val. Loss: 0.263 |  B. Val. Acc: 90.07%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 009 | Batch Id: 00060 | ET: 4.42s | lr: 9.38e-05 | Val. Loss: 0.278 |  Val. Acc: 89.45% | B. Val. Loss: 0.263 |  B. Val. Acc: 90.07%\n",
      "\t | Epoch: 009 | Batch Id: 00120 | ET: 10.77s | lr: 9.24e-05 | Val. Loss: 0.262 |  Val. Acc: 89.53% | B. Val. Loss: 0.262 |  B. Val. Acc: 90.07%\n",
      "\t | Epoch: 009 | Batch Id: 00180 | ET: 17.12s | lr: 9.09e-05 | Val. Loss: 0.261 |  Val. Acc: 89.61% | B. Val. Loss: 0.261 |  B. Val. Acc: 90.07%\n",
      "\t | Epoch: 009 | Batch Id: 00240 | ET: 23.46s | lr: 8.93e-05 | Val. Loss: 0.259 |  Val. Acc: 89.92% | B. Val. Loss: 0.259 |  B. Val. Acc: 90.07%\n",
      "Epoch: 009 | ET: 34.11s | \t Train Loss: 0.203 | Train Acc: 91.88% \t Val. Loss: 0.258 |  Val. Acc: 90.00% \t | B. Val. Loss: 0.258 |  B. Val. Acc: 90.07%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 010 | Batch Id: 00060 | ET: 4.42s | lr: 8.74e-05 | Val. Loss: 0.274 |  Val. Acc: 89.10% | B. Val. Loss: 0.258 |  B. Val. Acc: 90.07%\n",
      "\t | Epoch: 010 | Batch Id: 00120 | ET: 10.76s | lr: 8.55e-05 | Val. Loss: 0.282 |  Val. Acc: 88.56% | B. Val. Loss: 0.258 |  B. Val. Acc: 90.07%\n",
      "\t | Epoch: 010 | Batch Id: 00180 | ET: 17.10s | lr: 8.35e-05 | Val. Loss: 0.259 |  Val. Acc: 89.72% | B. Val. Loss: 0.258 |  B. Val. Acc: 90.07%\n",
      "\t | Epoch: 010 | Batch Id: 00240 | ET: 23.46s | lr: 8.14e-05 | Val. Loss: 0.270 |  Val. Acc: 89.26% | B. Val. Loss: 0.258 |  B. Val. Acc: 90.07%\n",
      "Epoch: 010 | ET: 34.11s | \t Train Loss: 0.209 | Train Acc: 91.46% \t Val. Loss: 0.273 |  Val. Acc: 89.14% \t | B. Val. Loss: 0.258 |  B. Val. Acc: 90.07%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 011 | Batch Id: 00060 | ET: 4.42s | lr: 7.92e-05 | Val. Loss: 0.273 |  Val. Acc: 90.07% | B. Val. Loss: 0.258 |  B. Val. Acc: 90.07%\n",
      "\t | Epoch: 011 | Batch Id: 00120 | ET: 10.78s | lr: 7.69e-05 | Val. Loss: 0.257 |  Val. Acc: 89.69% | B. Val. Loss: 0.257 |  B. Val. Acc: 90.07%\n",
      "\t | Epoch: 011 | Batch Id: 00180 | ET: 17.13s | lr: 7.45e-05 | Val. Loss: 0.256 |  Val. Acc: 90.19% | B. Val. Loss: 0.256 |  B. Val. Acc: 90.19%\n",
      "\t | Epoch: 011 | Batch Id: 00240 | ET: 23.61s | lr: 7.21e-05 | Val. Loss: 0.253 |  Val. Acc: 89.92% | B. Val. Loss: 0.253 |  B. Val. Acc: 90.19%\n",
      "Epoch: 011 | ET: 34.31s | \t Train Loss: 0.201 | Train Acc: 92.44% \t Val. Loss: 0.256 |  Val. Acc: 90.07% \t | B. Val. Loss: 0.253 |  B. Val. Acc: 90.19%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 012 | Batch Id: 00060 | ET: 4.42s | lr: 6.95e-05 | Val. Loss: 0.260 |  Val. Acc: 90.38% | B. Val. Loss: 0.253 |  B. Val. Acc: 90.38%\n",
      "\t | Epoch: 012 | Batch Id: 00120 | ET: 10.87s | lr: 6.69e-05 | Val. Loss: 0.268 |  Val. Acc: 89.72% | B. Val. Loss: 0.253 |  B. Val. Acc: 90.38%\n",
      "\t | Epoch: 012 | Batch Id: 00180 | ET: 17.22s | lr: 6.43e-05 | Val. Loss: 0.257 |  Val. Acc: 89.57% | B. Val. Loss: 0.253 |  B. Val. Acc: 90.38%\n",
      "\t | Epoch: 012 | Batch Id: 00240 | ET: 23.57s | lr: 6.16e-05 | Val. Loss: 0.250 |  Val. Acc: 89.84% | B. Val. Loss: 0.250 |  B. Val. Acc: 90.38%\n",
      "Epoch: 012 | ET: 34.28s | \t Train Loss: 0.187 | Train Acc: 92.74% \t Val. Loss: 0.249 |  Val. Acc: 89.88% \t | B. Val. Loss: 0.249 |  B. Val. Acc: 90.38%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 013 | Batch Id: 00060 | ET: 4.42s | lr: 5.88e-05 | Val. Loss: 0.264 |  Val. Acc: 90.11% | B. Val. Loss: 0.249 |  B. Val. Acc: 90.38%\n",
      "\t | Epoch: 013 | Batch Id: 00120 | ET: 10.78s | lr: 5.61e-05 | Val. Loss: 0.251 |  Val. Acc: 90.19% | B. Val. Loss: 0.249 |  B. Val. Acc: 90.38%\n",
      "\t | Epoch: 013 | Batch Id: 00180 | ET: 17.13s | lr: 5.33e-05 | Val. Loss: 0.249 |  Val. Acc: 89.69% | B. Val. Loss: 0.249 |  B. Val. Acc: 90.38%\n",
      "\t | Epoch: 013 | Batch Id: 00240 | ET: 23.46s | lr: 5.06e-05 | Val. Loss: 0.255 |  Val. Acc: 90.23% | B. Val. Loss: 0.249 |  B. Val. Acc: 90.38%\n",
      "Epoch: 013 | ET: 34.12s | \t Train Loss: 0.186 | Train Acc: 92.70% \t Val. Loss: 0.258 |  Val. Acc: 90.11% \t | B. Val. Loss: 0.249 |  B. Val. Acc: 90.38%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 014 | Batch Id: 00060 | ET: 4.42s | lr: 4.77e-05 | Val. Loss: 0.252 |  Val. Acc: 89.96% | B. Val. Loss: 0.249 |  B. Val. Acc: 90.38%\n",
      "\t | Epoch: 014 | Batch Id: 00120 | ET: 10.77s | lr: 4.50e-05 | Val. Loss: 0.254 |  Val. Acc: 90.42% | B. Val. Loss: 0.249 |  B. Val. Acc: 90.42%\n",
      "\t | Epoch: 014 | Batch Id: 00180 | ET: 17.20s | lr: 4.23e-05 | Val. Loss: 0.249 |  Val. Acc: 90.00% | B. Val. Loss: 0.249 |  B. Val. Acc: 90.42%\n",
      "\t | Epoch: 014 | Batch Id: 00240 | ET: 23.55s | lr: 3.96e-05 | Val. Loss: 0.252 |  Val. Acc: 89.96% | B. Val. Loss: 0.249 |  B. Val. Acc: 90.42%\n",
      "Epoch: 014 | ET: 34.21s | \t Train Loss: 0.180 | Train Acc: 92.92% \t Val. Loss: 0.253 |  Val. Acc: 89.92% \t | B. Val. Loss: 0.249 |  B. Val. Acc: 90.42%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 015 | Batch Id: 00060 | ET: 4.42s | lr: 3.68e-05 | Val. Loss: 0.267 |  Val. Acc: 89.96% | B. Val. Loss: 0.249 |  B. Val. Acc: 90.42%\n",
      "\t | Epoch: 015 | Batch Id: 00120 | ET: 10.76s | lr: 3.42e-05 | Val. Loss: 0.252 |  Val. Acc: 90.31% | B. Val. Loss: 0.249 |  B. Val. Acc: 90.42%\n",
      "\t | Epoch: 015 | Batch Id: 00180 | ET: 17.13s | lr: 3.16e-05 | Val. Loss: 0.247 |  Val. Acc: 90.46% | B. Val. Loss: 0.247 |  B. Val. Acc: 90.46%\n",
      "\t | Epoch: 015 | Batch Id: 00240 | ET: 23.59s | lr: 2.91e-05 | Val. Loss: 0.249 |  Val. Acc: 90.27% | B. Val. Loss: 0.247 |  B. Val. Acc: 90.46%\n",
      "Epoch: 015 | ET: 34.30s | \t Train Loss: 0.180 | Train Acc: 93.12% \t Val. Loss: 0.249 |  Val. Acc: 90.23% \t | B. Val. Loss: 0.247 |  B. Val. Acc: 90.46%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 016 | Batch Id: 00060 | ET: 4.42s | lr: 2.66e-05 | Val. Loss: 0.252 |  Val. Acc: 90.50% | B. Val. Loss: 0.247 |  B. Val. Acc: 90.50%\n",
      "\t | Epoch: 016 | Batch Id: 00120 | ET: 10.87s | lr: 2.42e-05 | Val. Loss: 0.252 |  Val. Acc: 90.58% | B. Val. Loss: 0.247 |  B. Val. Acc: 90.58%\n",
      "\t | Epoch: 016 | Batch Id: 00180 | ET: 17.35s | lr: 2.19e-05 | Val. Loss: 0.250 |  Val. Acc: 90.15% | B. Val. Loss: 0.247 |  B. Val. Acc: 90.58%\n",
      "\t | Epoch: 016 | Batch Id: 00240 | ET: 23.71s | lr: 1.97e-05 | Val. Loss: 0.248 |  Val. Acc: 90.00% | B. Val. Loss: 0.247 |  B. Val. Acc: 90.58%\n",
      "Epoch: 016 | ET: 34.42s | \t Train Loss: 0.176 | Train Acc: 93.20% \t Val. Loss: 0.248 |  Val. Acc: 89.88% \t | B. Val. Loss: 0.247 |  B. Val. Acc: 90.58%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 017 | Batch Id: 00060 | ET: 4.42s | lr: 1.75e-05 | Val. Loss: 0.261 |  Val. Acc: 90.35% | B. Val. Loss: 0.247 |  B. Val. Acc: 90.58%\n",
      "\t | Epoch: 017 | Batch Id: 00120 | ET: 10.77s | lr: 1.55e-05 | Val. Loss: 0.251 |  Val. Acc: 90.54% | B. Val. Loss: 0.247 |  B. Val. Acc: 90.58%\n",
      "\t | Epoch: 017 | Batch Id: 00180 | ET: 17.11s | lr: 1.36e-05 | Val. Loss: 0.251 |  Val. Acc: 90.15% | B. Val. Loss: 0.247 |  B. Val. Acc: 90.58%\n",
      "\t | Epoch: 017 | Batch Id: 00240 | ET: 23.48s | lr: 1.19e-05 | Val. Loss: 0.246 |  Val. Acc: 90.35% | B. Val. Loss: 0.246 |  B. Val. Acc: 90.58%\n",
      "Epoch: 017 | ET: 34.14s | \t Train Loss: 0.177 | Train Acc: 93.28% \t Val. Loss: 0.246 |  Val. Acc: 90.42% \t | B. Val. Loss: 0.246 |  B. Val. Acc: 90.58%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 018 | Batch Id: 00060 | ET: 4.42s | lr: 1.01e-05 | Val. Loss: 0.260 |  Val. Acc: 90.27% | B. Val. Loss: 0.246 |  B. Val. Acc: 90.58%\n",
      "\t | Epoch: 018 | Batch Id: 00120 | ET: 10.76s | lr: 8.61e-06 | Val. Loss: 0.245 |  Val. Acc: 90.42% | B. Val. Loss: 0.245 |  B. Val. Acc: 90.58%\n",
      "\t | Epoch: 018 | Batch Id: 00180 | ET: 17.10s | lr: 7.21e-06 | Val. Loss: 0.250 |  Val. Acc: 90.66% | B. Val. Loss: 0.245 |  B. Val. Acc: 90.66%\n",
      "\t | Epoch: 018 | Batch Id: 00240 | ET: 23.56s | lr: 5.94e-06 | Val. Loss: 0.248 |  Val. Acc: 90.11% | B. Val. Loss: 0.245 |  B. Val. Acc: 90.66%\n",
      "Epoch: 018 | ET: 34.25s | \t Train Loss: 0.177 | Train Acc: 93.09% \t Val. Loss: 0.248 |  Val. Acc: 90.03% \t | B. Val. Loss: 0.245 |  B. Val. Acc: 90.66%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 019 | Batch Id: 00060 | ET: 4.42s | lr: 4.78e-06 | Val. Loss: 0.249 |  Val. Acc: 90.11% | B. Val. Loss: 0.245 |  B. Val. Acc: 90.66%\n",
      "\t | Epoch: 019 | Batch Id: 00120 | ET: 10.77s | lr: 3.79e-06 | Val. Loss: 0.244 |  Val. Acc: 90.42% | B. Val. Loss: 0.244 |  B. Val. Acc: 90.66%\n",
      "\t | Epoch: 019 | Batch Id: 00180 | ET: 17.12s | lr: 2.95e-06 | Val. Loss: 0.245 |  Val. Acc: 90.31% | B. Val. Loss: 0.244 |  B. Val. Acc: 90.66%\n",
      "\t | Epoch: 019 | Batch Id: 00240 | ET: 23.46s | lr: 2.26e-06 | Val. Loss: 0.246 |  Val. Acc: 90.11% | B. Val. Loss: 0.244 |  B. Val. Acc: 90.66%\n",
      "Epoch: 019 | ET: 34.13s | \t Train Loss: 0.175 | Train Acc: 93.23% \t Val. Loss: 0.246 |  Val. Acc: 90.11% \t | B. Val. Loss: 0.244 |  B. Val. Acc: 90.66%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 020 | Batch Id: 00060 | ET: 4.41s | lr: 1.70e-06 | Val. Loss: 0.256 |  Val. Acc: 89.92% | B. Val. Loss: 0.244 |  B. Val. Acc: 90.66%\n",
      "\t | Epoch: 020 | Batch Id: 00120 | ET: 10.78s | lr: 1.32e-06 | Val. Loss: 0.253 |  Val. Acc: 90.23% | B. Val. Loss: 0.244 |  B. Val. Acc: 90.66%\n",
      "\t | Epoch: 020 | Batch Id: 00180 | ET: 17.13s | lr: 1.08e-06 | Val. Loss: 0.251 |  Val. Acc: 90.50% | B. Val. Loss: 0.244 |  B. Val. Acc: 90.66%\n",
      "\t | Epoch: 020 | Batch Id: 00240 | ET: 23.48s | lr: 1.00e-06 | Val. Loss: 0.252 |  Val. Acc: 90.46% | B. Val. Loss: 0.244 |  B. Val. Acc: 90.66%\n",
      "Epoch: 020 | ET: 34.17s | \t Train Loss: 0.185 | Train Acc: 92.80% \t Val. Loss: 0.252 |  Val. Acc: 90.50% \t | B. Val. Loss: 0.244 |  B. Val. Acc: 90.66%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Number of the parameters: 2645537\n",
      "\n",
      "Training\n",
      "-----    2    ----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 001 | Batch Id: 00060 | ET: 4.42s | lr: 1.04e-05 | Val. Loss: 0.560 |  Val. Acc: 72.28% | B. Val. Loss: 0.560 |  B. Val. Acc: 72.28%\n",
      "\t | Epoch: 001 | Batch Id: 00120 | ET: 10.80s | lr: 1.15e-05 | Val. Loss: 0.529 |  Val. Acc: 74.10% | B. Val. Loss: 0.529 |  B. Val. Acc: 74.10%\n",
      "\t | Epoch: 001 | Batch Id: 00180 | ET: 17.28s | lr: 1.33e-05 | Val. Loss: 0.507 |  Val. Acc: 75.07% | B. Val. Loss: 0.507 |  B. Val. Acc: 75.07%\n",
      "\t | Epoch: 001 | Batch Id: 00240 | ET: 23.72s | lr: 1.59e-05 | Val. Loss: 0.519 |  Val. Acc: 74.45% | B. Val. Loss: 0.507 |  B. Val. Acc: 75.07%\n",
      "Epoch: 001 | ET: 34.43s | \t Train Loss: 0.498 | Train Acc: 74.91% \t Val. Loss: 0.515 |  Val. Acc: 74.76% \t | B. Val. Loss: 0.507 |  B. Val. Acc: 75.07%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 002 | Batch Id: 00060 | ET: 4.42s | lr: 1.92e-05 | Val. Loss: 0.491 |  Val. Acc: 75.96% | B. Val. Loss: 0.491 |  B. Val. Acc: 75.96%\n",
      "\t | Epoch: 002 | Batch Id: 00120 | ET: 10.90s | lr: 2.31e-05 | Val. Loss: 0.470 |  Val. Acc: 77.08% | B. Val. Loss: 0.470 |  B. Val. Acc: 77.08%\n",
      "\t | Epoch: 002 | Batch Id: 00180 | ET: 17.38s | lr: 2.74e-05 | Val. Loss: 0.479 |  Val. Acc: 77.98% | B. Val. Loss: 0.470 |  B. Val. Acc: 77.98%\n",
      "\t | Epoch: 002 | Batch Id: 00240 | ET: 23.83s | lr: 3.23e-05 | Val. Loss: 0.455 |  Val. Acc: 79.18% | B. Val. Loss: 0.455 |  B. Val. Acc: 79.18%\n",
      "Epoch: 002 | ET: 34.77s | \t Train Loss: 0.440 | Train Acc: 79.60% \t Val. Loss: 0.454 |  Val. Acc: 79.26% \t | B. Val. Loss: 0.454 |  B. Val. Acc: 79.26%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 003 | Batch Id: 00060 | ET: 4.43s | lr: 3.77e-05 | Val. Loss: 0.470 |  Val. Acc: 77.51% | B. Val. Loss: 0.454 |  B. Val. Acc: 79.26%\n",
      "\t | Epoch: 003 | Batch Id: 00120 | ET: 10.79s | lr: 4.32e-05 | Val. Loss: 0.454 |  Val. Acc: 79.80% | B. Val. Loss: 0.454 |  B. Val. Acc: 79.80%\n",
      "\t | Epoch: 003 | Batch Id: 00180 | ET: 17.25s | lr: 4.89e-05 | Val. Loss: 0.479 |  Val. Acc: 79.33% | B. Val. Loss: 0.454 |  B. Val. Acc: 79.80%\n",
      "\t | Epoch: 003 | Batch Id: 00240 | ET: 23.61s | lr: 5.48e-05 | Val. Loss: 0.437 |  Val. Acc: 80.54% | B. Val. Loss: 0.437 |  B. Val. Acc: 80.54%\n",
      "Epoch: 003 | ET: 34.55s | \t Train Loss: 0.422 | Train Acc: 82.50% \t Val. Loss: 0.445 |  Val. Acc: 80.77% \t | B. Val. Loss: 0.437 |  B. Val. Acc: 80.77%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 004 | Batch Id: 00060 | ET: 4.42s | lr: 6.08e-05 | Val. Loss: 0.447 |  Val. Acc: 79.99% | B. Val. Loss: 0.437 |  B. Val. Acc: 80.77%\n",
      "\t | Epoch: 004 | Batch Id: 00120 | ET: 10.77s | lr: 6.65e-05 | Val. Loss: 0.424 |  Val. Acc: 80.77% | B. Val. Loss: 0.424 |  B. Val. Acc: 80.77%\n",
      "\t | Epoch: 004 | Batch Id: 00180 | ET: 17.13s | lr: 7.21e-05 | Val. Loss: 0.427 |  Val. Acc: 82.90% | B. Val. Loss: 0.424 |  B. Val. Acc: 82.90%\n",
      "\t | Epoch: 004 | Batch Id: 00240 | ET: 23.60s | lr: 7.73e-05 | Val. Loss: 0.399 |  Val. Acc: 83.09% | B. Val. Loss: 0.399 |  B. Val. Acc: 83.09%\n",
      "Epoch: 004 | ET: 34.58s | \t Train Loss: 0.363 | Train Acc: 84.59% \t Val. Loss: 0.399 |  Val. Acc: 83.13% \t | B. Val. Loss: 0.399 |  B. Val. Acc: 83.13%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 005 | Batch Id: 00060 | ET: 4.42s | lr: 8.23e-05 | Val. Loss: 0.411 |  Val. Acc: 82.51% | B. Val. Loss: 0.399 |  B. Val. Acc: 83.13%\n",
      "\t | Epoch: 005 | Batch Id: 00120 | ET: 10.78s | lr: 8.67e-05 | Val. Loss: 0.350 |  Val. Acc: 85.11% | B. Val. Loss: 0.350 |  B. Val. Acc: 85.11%\n",
      "\t | Epoch: 005 | Batch Id: 00180 | ET: 17.24s | lr: 9.06e-05 | Val. Loss: 0.349 |  Val. Acc: 85.38% | B. Val. Loss: 0.349 |  B. Val. Acc: 85.38%\n",
      "\t | Epoch: 005 | Batch Id: 00240 | ET: 23.72s | lr: 9.39e-05 | Val. Loss: 0.309 |  Val. Acc: 88.17% | B. Val. Loss: 0.309 |  B. Val. Acc: 88.17%\n",
      "Epoch: 005 | ET: 34.55s | \t Train Loss: 0.284 | Train Acc: 88.76% \t Val. Loss: 0.311 |  Val. Acc: 87.86% \t | B. Val. Loss: 0.309 |  B. Val. Acc: 88.17%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 006 | Batch Id: 00060 | ET: 4.42s | lr: 9.66e-05 | Val. Loss: 0.365 |  Val. Acc: 84.30% | B. Val. Loss: 0.309 |  B. Val. Acc: 88.17%\n",
      "\t | Epoch: 006 | Batch Id: 00120 | ET: 10.78s | lr: 9.84e-05 | Val. Loss: 0.318 |  Val. Acc: 87.40% | B. Val. Loss: 0.309 |  B. Val. Acc: 88.17%\n",
      "\t | Epoch: 006 | Batch Id: 00180 | ET: 17.13s | lr: 9.96e-05 | Val. Loss: 0.291 |  Val. Acc: 88.76% | B. Val. Loss: 0.291 |  B. Val. Acc: 88.76%\n",
      "\t | Epoch: 006 | Batch Id: 00240 | ET: 23.60s | lr: 1.00e-04 | Val. Loss: 0.287 |  Val. Acc: 88.41% | B. Val. Loss: 0.287 |  B. Val. Acc: 88.76%\n",
      "Epoch: 006 | ET: 34.43s | \t Train Loss: 0.244 | Train Acc: 90.59% \t Val. Loss: 0.288 |  Val. Acc: 88.79% \t | B. Val. Loss: 0.287 |  B. Val. Acc: 88.79%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 007 | Batch Id: 00060 | ET: 4.42s | lr: 9.99e-05 | Val. Loss: 0.326 |  Val. Acc: 88.06% | B. Val. Loss: 0.287 |  B. Val. Acc: 88.79%\n",
      "\t | Epoch: 007 | Batch Id: 00120 | ET: 10.77s | lr: 9.97e-05 | Val. Loss: 0.313 |  Val. Acc: 87.01% | B. Val. Loss: 0.287 |  B. Val. Acc: 88.79%\n",
      "\t | Epoch: 007 | Batch Id: 00180 | ET: 17.13s | lr: 9.93e-05 | Val. Loss: 0.276 |  Val. Acc: 88.52% | B. Val. Loss: 0.276 |  B. Val. Acc: 88.79%\n",
      "\t | Epoch: 007 | Batch Id: 00240 | ET: 23.49s | lr: 9.88e-05 | Val. Loss: 0.288 |  Val. Acc: 88.21% | B. Val. Loss: 0.276 |  B. Val. Acc: 88.79%\n",
      "Epoch: 007 | ET: 34.33s | \t Train Loss: 0.230 | Train Acc: 91.25% \t Val. Loss: 0.275 |  Val. Acc: 89.34% \t | B. Val. Loss: 0.275 |  B. Val. Acc: 89.34%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 008 | Batch Id: 00060 | ET: 4.42s | lr: 9.81e-05 | Val. Loss: 0.301 |  Val. Acc: 88.13% | B. Val. Loss: 0.275 |  B. Val. Acc: 89.34%\n",
      "\t | Epoch: 008 | Batch Id: 00120 | ET: 10.78s | lr: 9.72e-05 | Val. Loss: 0.277 |  Val. Acc: 89.53% | B. Val. Loss: 0.275 |  B. Val. Acc: 89.53%\n",
      "\t | Epoch: 008 | Batch Id: 00180 | ET: 17.25s | lr: 9.63e-05 | Val. Loss: 0.271 |  Val. Acc: 89.26% | B. Val. Loss: 0.271 |  B. Val. Acc: 89.53%\n",
      "\t | Epoch: 008 | Batch Id: 00240 | ET: 23.60s | lr: 9.51e-05 | Val. Loss: 0.271 |  Val. Acc: 89.84% | B. Val. Loss: 0.271 |  B. Val. Acc: 89.84%\n",
      "Epoch: 008 | ET: 34.45s | \t Train Loss: 0.221 | Train Acc: 91.50% \t Val. Loss: 0.277 |  Val. Acc: 89.53% \t | B. Val. Loss: 0.271 |  B. Val. Acc: 89.84%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 009 | Batch Id: 00060 | ET: 4.41s | lr: 9.38e-05 | Val. Loss: 0.295 |  Val. Acc: 88.72% | B. Val. Loss: 0.271 |  B. Val. Acc: 89.84%\n",
      "\t | Epoch: 009 | Batch Id: 00120 | ET: 10.78s | lr: 9.24e-05 | Val. Loss: 0.270 |  Val. Acc: 89.22% | B. Val. Loss: 0.270 |  B. Val. Acc: 89.84%\n",
      "\t | Epoch: 009 | Batch Id: 00180 | ET: 17.14s | lr: 9.09e-05 | Val. Loss: 0.265 |  Val. Acc: 89.30% | B. Val. Loss: 0.265 |  B. Val. Acc: 89.84%\n",
      "\t | Epoch: 009 | Batch Id: 00240 | ET: 23.50s | lr: 8.93e-05 | Val. Loss: 0.269 |  Val. Acc: 88.99% | B. Val. Loss: 0.265 |  B. Val. Acc: 89.84%\n",
      "Epoch: 009 | ET: 34.21s | \t Train Loss: 0.214 | Train Acc: 91.20% \t Val. Loss: 0.273 |  Val. Acc: 89.03% \t | B. Val. Loss: 0.265 |  B. Val. Acc: 89.84%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 010 | Batch Id: 00060 | ET: 4.42s | lr: 8.74e-05 | Val. Loss: 0.294 |  Val. Acc: 87.82% | B. Val. Loss: 0.265 |  B. Val. Acc: 89.84%\n",
      "\t | Epoch: 010 | Batch Id: 00120 | ET: 10.79s | lr: 8.55e-05 | Val. Loss: 0.259 |  Val. Acc: 89.41% | B. Val. Loss: 0.259 |  B. Val. Acc: 89.84%\n",
      "\t | Epoch: 010 | Batch Id: 00180 | ET: 17.15s | lr: 8.35e-05 | Val. Loss: 0.270 |  Val. Acc: 89.45% | B. Val. Loss: 0.259 |  B. Val. Acc: 89.84%\n",
      "\t | Epoch: 010 | Batch Id: 00240 | ET: 23.50s | lr: 8.14e-05 | Val. Loss: 0.258 |  Val. Acc: 89.26% | B. Val. Loss: 0.258 |  B. Val. Acc: 89.84%\n",
      "Epoch: 010 | ET: 34.22s | \t Train Loss: 0.207 | Train Acc: 91.87% \t Val. Loss: 0.255 |  Val. Acc: 89.41% \t | B. Val. Loss: 0.255 |  B. Val. Acc: 89.84%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 011 | Batch Id: 00060 | ET: 4.42s | lr: 7.92e-05 | Val. Loss: 0.271 |  Val. Acc: 89.57% | B. Val. Loss: 0.255 |  B. Val. Acc: 89.84%\n",
      "\t | Epoch: 011 | Batch Id: 00120 | ET: 10.77s | lr: 7.69e-05 | Val. Loss: 0.269 |  Val. Acc: 88.64% | B. Val. Loss: 0.255 |  B. Val. Acc: 89.84%\n",
      "\t | Epoch: 011 | Batch Id: 00180 | ET: 17.14s | lr: 7.45e-05 | Val. Loss: 0.288 |  Val. Acc: 89.26% | B. Val. Loss: 0.255 |  B. Val. Acc: 89.84%\n",
      "\t | Epoch: 011 | Batch Id: 00240 | ET: 23.49s | lr: 7.21e-05 | Val. Loss: 0.258 |  Val. Acc: 89.41% | B. Val. Loss: 0.255 |  B. Val. Acc: 89.84%\n",
      "Epoch: 011 | ET: 34.21s | \t Train Loss: 0.203 | Train Acc: 91.72% \t Val. Loss: 0.259 |  Val. Acc: 89.30% \t | B. Val. Loss: 0.255 |  B. Val. Acc: 89.84%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 012 | Batch Id: 00060 | ET: 4.42s | lr: 6.95e-05 | Val. Loss: 0.272 |  Val. Acc: 89.80% | B. Val. Loss: 0.255 |  B. Val. Acc: 89.84%\n",
      "\t | Epoch: 012 | Batch Id: 00120 | ET: 10.78s | lr: 6.69e-05 | Val. Loss: 0.266 |  Val. Acc: 88.56% | B. Val. Loss: 0.255 |  B. Val. Acc: 89.84%\n",
      "\t | Epoch: 012 | Batch Id: 00180 | ET: 17.14s | lr: 6.43e-05 | Val. Loss: 0.260 |  Val. Acc: 89.38% | B. Val. Loss: 0.255 |  B. Val. Acc: 89.84%\n",
      "\t | Epoch: 012 | Batch Id: 00240 | ET: 23.50s | lr: 6.16e-05 | Val. Loss: 0.256 |  Val. Acc: 89.53% | B. Val. Loss: 0.255 |  B. Val. Acc: 89.84%\n",
      "Epoch: 012 | ET: 34.21s | \t Train Loss: 0.188 | Train Acc: 92.50% \t Val. Loss: 0.254 |  Val. Acc: 89.57% \t | B. Val. Loss: 0.254 |  B. Val. Acc: 89.84%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 013 | Batch Id: 00060 | ET: 4.42s | lr: 5.88e-05 | Val. Loss: 0.286 |  Val. Acc: 89.07% | B. Val. Loss: 0.254 |  B. Val. Acc: 89.84%\n",
      "\t | Epoch: 013 | Batch Id: 00120 | ET: 10.78s | lr: 5.61e-05 | Val. Loss: 0.256 |  Val. Acc: 89.57% | B. Val. Loss: 0.254 |  B. Val. Acc: 89.84%\n",
      "\t | Epoch: 013 | Batch Id: 00180 | ET: 17.13s | lr: 5.33e-05 | Val. Loss: 0.260 |  Val. Acc: 89.26% | B. Val. Loss: 0.254 |  B. Val. Acc: 89.84%\n",
      "\t | Epoch: 013 | Batch Id: 00240 | ET: 23.49s | lr: 5.06e-05 | Val. Loss: 0.263 |  Val. Acc: 88.95% | B. Val. Loss: 0.254 |  B. Val. Acc: 89.84%\n",
      "Epoch: 013 | ET: 34.22s | \t Train Loss: 0.192 | Train Acc: 92.03% \t Val. Loss: 0.259 |  Val. Acc: 89.14% \t | B. Val. Loss: 0.254 |  B. Val. Acc: 89.84%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 014 | Batch Id: 00060 | ET: 4.42s | lr: 4.77e-05 | Val. Loss: 0.291 |  Val. Acc: 89.45% | B. Val. Loss: 0.254 |  B. Val. Acc: 89.84%\n",
      "\t | Epoch: 014 | Batch Id: 00120 | ET: 10.77s | lr: 4.50e-05 | Val. Loss: 0.261 |  Val. Acc: 90.11% | B. Val. Loss: 0.254 |  B. Val. Acc: 90.11%\n",
      "\t | Epoch: 014 | Batch Id: 00180 | ET: 17.24s | lr: 4.23e-05 | Val. Loss: 0.258 |  Val. Acc: 89.92% | B. Val. Loss: 0.254 |  B. Val. Acc: 90.11%\n",
      "\t | Epoch: 014 | Batch Id: 00240 | ET: 23.59s | lr: 3.96e-05 | Val. Loss: 0.253 |  Val. Acc: 90.15% | B. Val. Loss: 0.253 |  B. Val. Acc: 90.15%\n",
      "Epoch: 014 | ET: 34.41s | \t Train Loss: 0.183 | Train Acc: 92.79% \t Val. Loss: 0.253 |  Val. Acc: 90.11% \t | B. Val. Loss: 0.253 |  B. Val. Acc: 90.15%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 015 | Batch Id: 00060 | ET: 4.42s | lr: 3.68e-05 | Val. Loss: 0.262 |  Val. Acc: 89.72% | B. Val. Loss: 0.253 |  B. Val. Acc: 90.15%\n",
      "\t | Epoch: 015 | Batch Id: 00120 | ET: 10.77s | lr: 3.42e-05 | Val. Loss: 0.258 |  Val. Acc: 90.11% | B. Val. Loss: 0.253 |  B. Val. Acc: 90.15%\n",
      "\t | Epoch: 015 | Batch Id: 00180 | ET: 17.13s | lr: 3.16e-05 | Val. Loss: 0.248 |  Val. Acc: 89.84% | B. Val. Loss: 0.248 |  B. Val. Acc: 90.15%\n",
      "\t | Epoch: 015 | Batch Id: 00240 | ET: 23.48s | lr: 2.91e-05 | Val. Loss: 0.253 |  Val. Acc: 89.72% | B. Val. Loss: 0.248 |  B. Val. Acc: 90.15%\n",
      "Epoch: 015 | ET: 34.21s | \t Train Loss: 0.181 | Train Acc: 92.88% \t Val. Loss: 0.253 |  Val. Acc: 89.80% \t | B. Val. Loss: 0.248 |  B. Val. Acc: 90.15%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 016 | Batch Id: 00060 | ET: 4.41s | lr: 2.66e-05 | Val. Loss: 0.286 |  Val. Acc: 88.13% | B. Val. Loss: 0.248 |  B. Val. Acc: 90.15%\n",
      "\t | Epoch: 016 | Batch Id: 00120 | ET: 10.78s | lr: 2.42e-05 | Val. Loss: 0.255 |  Val. Acc: 89.65% | B. Val. Loss: 0.248 |  B. Val. Acc: 90.15%\n",
      "\t | Epoch: 016 | Batch Id: 00180 | ET: 17.13s | lr: 2.19e-05 | Val. Loss: 0.252 |  Val. Acc: 90.19% | B. Val. Loss: 0.248 |  B. Val. Acc: 90.19%\n",
      "\t | Epoch: 016 | Batch Id: 00240 | ET: 23.60s | lr: 1.97e-05 | Val. Loss: 0.250 |  Val. Acc: 90.03% | B. Val. Loss: 0.248 |  B. Val. Acc: 90.19%\n",
      "Epoch: 016 | ET: 34.33s | \t Train Loss: 0.179 | Train Acc: 92.94% \t Val. Loss: 0.251 |  Val. Acc: 89.96% \t | B. Val. Loss: 0.248 |  B. Val. Acc: 90.19%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 017 | Batch Id: 00060 | ET: 4.42s | lr: 1.75e-05 | Val. Loss: 0.263 |  Val. Acc: 89.41% | B. Val. Loss: 0.248 |  B. Val. Acc: 90.19%\n",
      "\t | Epoch: 017 | Batch Id: 00120 | ET: 10.78s | lr: 1.55e-05 | Val. Loss: 0.259 |  Val. Acc: 90.03% | B. Val. Loss: 0.248 |  B. Val. Acc: 90.19%\n",
      "\t | Epoch: 017 | Batch Id: 00180 | ET: 17.14s | lr: 1.36e-05 | Val. Loss: 0.250 |  Val. Acc: 89.84% | B. Val. Loss: 0.248 |  B. Val. Acc: 90.19%\n",
      "\t | Epoch: 017 | Batch Id: 00240 | ET: 23.49s | lr: 1.19e-05 | Val. Loss: 0.252 |  Val. Acc: 90.23% | B. Val. Loss: 0.248 |  B. Val. Acc: 90.23%\n",
      "Epoch: 017 | ET: 34.40s | \t Train Loss: 0.178 | Train Acc: 93.03% \t Val. Loss: 0.252 |  Val. Acc: 90.27% \t | B. Val. Loss: 0.248 |  B. Val. Acc: 90.27%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 018 | Batch Id: 00060 | ET: 4.42s | lr: 1.01e-05 | Val. Loss: 0.262 |  Val. Acc: 89.61% | B. Val. Loss: 0.248 |  B. Val. Acc: 90.27%\n",
      "\t | Epoch: 018 | Batch Id: 00120 | ET: 10.77s | lr: 8.61e-06 | Val. Loss: 0.250 |  Val. Acc: 90.31% | B. Val. Loss: 0.248 |  B. Val. Acc: 90.31%\n",
      "\t | Epoch: 018 | Batch Id: 00180 | ET: 17.28s | lr: 7.21e-06 | Val. Loss: 0.249 |  Val. Acc: 90.07% | B. Val. Loss: 0.248 |  B. Val. Acc: 90.31%\n",
      "\t | Epoch: 018 | Batch Id: 00240 | ET: 23.63s | lr: 5.94e-06 | Val. Loss: 0.252 |  Val. Acc: 89.88% | B. Val. Loss: 0.248 |  B. Val. Acc: 90.31%\n",
      "Epoch: 018 | ET: 34.34s | \t Train Loss: 0.180 | Train Acc: 92.90% \t Val. Loss: 0.252 |  Val. Acc: 89.88% \t | B. Val. Loss: 0.248 |  B. Val. Acc: 90.31%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 019 | Batch Id: 00060 | ET: 4.42s | lr: 4.78e-06 | Val. Loss: 0.273 |  Val. Acc: 89.53% | B. Val. Loss: 0.248 |  B. Val. Acc: 90.31%\n",
      "\t | Epoch: 019 | Batch Id: 00120 | ET: 10.77s | lr: 3.79e-06 | Val. Loss: 0.254 |  Val. Acc: 89.80% | B. Val. Loss: 0.248 |  B. Val. Acc: 90.31%\n",
      "\t | Epoch: 019 | Batch Id: 00180 | ET: 17.12s | lr: 2.95e-06 | Val. Loss: 0.249 |  Val. Acc: 90.11% | B. Val. Loss: 0.248 |  B. Val. Acc: 90.31%\n",
      "\t | Epoch: 019 | Batch Id: 00240 | ET: 23.47s | lr: 2.26e-06 | Val. Loss: 0.249 |  Val. Acc: 90.11% | B. Val. Loss: 0.248 |  B. Val. Acc: 90.31%\n",
      "Epoch: 019 | ET: 34.16s | \t Train Loss: 0.180 | Train Acc: 92.96% \t Val. Loss: 0.249 |  Val. Acc: 90.11% \t | B. Val. Loss: 0.248 |  B. Val. Acc: 90.31%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 020 | Batch Id: 00060 | ET: 4.42s | lr: 1.70e-06 | Val. Loss: 0.259 |  Val. Acc: 89.72% | B. Val. Loss: 0.248 |  B. Val. Acc: 90.31%\n",
      "\t | Epoch: 020 | Batch Id: 00120 | ET: 10.77s | lr: 1.32e-06 | Val. Loss: 0.256 |  Val. Acc: 89.88% | B. Val. Loss: 0.248 |  B. Val. Acc: 90.31%\n",
      "\t | Epoch: 020 | Batch Id: 00180 | ET: 17.11s | lr: 1.08e-06 | Val. Loss: 0.254 |  Val. Acc: 89.84% | B. Val. Loss: 0.248 |  B. Val. Acc: 90.31%\n",
      "\t | Epoch: 020 | Batch Id: 00240 | ET: 23.47s | lr: 1.00e-06 | Val. Loss: 0.253 |  Val. Acc: 89.96% | B. Val. Loss: 0.248 |  B. Val. Acc: 90.31%\n",
      "Epoch: 020 | ET: 34.13s | \t Train Loss: 0.182 | Train Acc: 92.99% \t Val. Loss: 0.253 |  Val. Acc: 90.00% \t | B. Val. Loss: 0.248 |  B. Val. Acc: 90.31%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Number of the parameters: 2645537\n",
      "\n",
      "Training\n",
      "-----    3    ----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 001 | Batch Id: 00060 | ET: 4.42s | lr: 1.04e-05 | Val. Loss: 0.573 |  Val. Acc: 68.59% | B. Val. Loss: 0.573 |  B. Val. Acc: 68.59%\n",
      "\t | Epoch: 001 | Batch Id: 00120 | ET: 10.80s | lr: 1.15e-05 | Val. Loss: 0.526 |  Val. Acc: 73.17% | B. Val. Loss: 0.526 |  B. Val. Acc: 73.17%\n",
      "\t | Epoch: 001 | Batch Id: 00180 | ET: 17.27s | lr: 1.33e-05 | Val. Loss: 0.527 |  Val. Acc: 77.36% | B. Val. Loss: 0.526 |  B. Val. Acc: 77.36%\n",
      "\t | Epoch: 001 | Batch Id: 00240 | ET: 23.74s | lr: 1.59e-05 | Val. Loss: 0.531 |  Val. Acc: 74.60% | B. Val. Loss: 0.526 |  B. Val. Acc: 77.36%\n",
      "Epoch: 001 | ET: 34.47s | \t Train Loss: 0.506 | Train Acc: 75.03% \t Val. Loss: 0.520 |  Val. Acc: 74.80% \t | B. Val. Loss: 0.520 |  B. Val. Acc: 77.36%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 002 | Batch Id: 00060 | ET: 4.42s | lr: 1.92e-05 | Val. Loss: 0.539 |  Val. Acc: 73.63% | B. Val. Loss: 0.520 |  B. Val. Acc: 77.36%\n",
      "\t | Epoch: 002 | Batch Id: 00120 | ET: 10.78s | lr: 2.31e-05 | Val. Loss: 0.487 |  Val. Acc: 78.95% | B. Val. Loss: 0.487 |  B. Val. Acc: 78.95%\n",
      "\t | Epoch: 002 | Batch Id: 00180 | ET: 17.25s | lr: 2.74e-05 | Val. Loss: 0.466 |  Val. Acc: 78.60% | B. Val. Loss: 0.466 |  B. Val. Acc: 78.95%\n",
      "\t | Epoch: 002 | Batch Id: 00240 | ET: 23.59s | lr: 3.23e-05 | Val. Loss: 0.479 |  Val. Acc: 78.44% | B. Val. Loss: 0.466 |  B. Val. Acc: 78.95%\n",
      "Epoch: 002 | ET: 34.32s | \t Train Loss: 0.482 | Train Acc: 78.71% \t Val. Loss: 0.483 |  Val. Acc: 78.44% \t | B. Val. Loss: 0.466 |  B. Val. Acc: 78.95%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 003 | Batch Id: 00060 | ET: 4.41s | lr: 3.77e-05 | Val. Loss: 0.504 |  Val. Acc: 76.23% | B. Val. Loss: 0.466 |  B. Val. Acc: 78.95%\n",
      "\t | Epoch: 003 | Batch Id: 00120 | ET: 10.78s | lr: 4.32e-05 | Val. Loss: 0.502 |  Val. Acc: 78.44% | B. Val. Loss: 0.466 |  B. Val. Acc: 78.95%\n",
      "\t | Epoch: 003 | Batch Id: 00180 | ET: 17.14s | lr: 4.89e-05 | Val. Loss: 0.450 |  Val. Acc: 79.72% | B. Val. Loss: 0.450 |  B. Val. Acc: 79.72%\n",
      "\t | Epoch: 003 | Batch Id: 00240 | ET: 23.61s | lr: 5.48e-05 | Val. Loss: 0.465 |  Val. Acc: 79.45% | B. Val. Loss: 0.450 |  B. Val. Acc: 79.72%\n",
      "Epoch: 003 | ET: 34.44s | \t Train Loss: 0.450 | Train Acc: 79.33% \t Val. Loss: 0.458 |  Val. Acc: 79.76% \t | B. Val. Loss: 0.450 |  B. Val. Acc: 79.76%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 004 | Batch Id: 00060 | ET: 4.42s | lr: 6.08e-05 | Val. Loss: 0.468 |  Val. Acc: 78.75% | B. Val. Loss: 0.450 |  B. Val. Acc: 79.76%\n",
      "\t | Epoch: 004 | Batch Id: 00120 | ET: 10.79s | lr: 6.65e-05 | Val. Loss: 0.444 |  Val. Acc: 79.76% | B. Val. Loss: 0.444 |  B. Val. Acc: 79.76%\n",
      "\t | Epoch: 004 | Batch Id: 00180 | ET: 17.15s | lr: 7.21e-05 | Val. Loss: 0.436 |  Val. Acc: 79.88% | B. Val. Loss: 0.436 |  B. Val. Acc: 79.88%\n",
      "\t | Epoch: 004 | Batch Id: 00240 | ET: 23.62s | lr: 7.73e-05 | Val. Loss: 0.454 |  Val. Acc: 80.92% | B. Val. Loss: 0.436 |  B. Val. Acc: 80.92%\n",
      "Epoch: 004 | ET: 34.44s | \t Train Loss: 0.495 | Train Acc: 72.57% \t Val. Loss: 0.508 |  Val. Acc: 72.94% \t | B. Val. Loss: 0.436 |  B. Val. Acc: 80.92%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 005 | Batch Id: 00060 | ET: 4.42s | lr: 8.23e-05 | Val. Loss: 0.442 |  Val. Acc: 79.60% | B. Val. Loss: 0.436 |  B. Val. Acc: 80.92%\n",
      "\t | Epoch: 005 | Batch Id: 00120 | ET: 10.79s | lr: 8.67e-05 | Val. Loss: 0.412 |  Val. Acc: 81.78% | B. Val. Loss: 0.412 |  B. Val. Acc: 81.78%\n",
      "\t | Epoch: 005 | Batch Id: 00180 | ET: 17.27s | lr: 9.06e-05 | Val. Loss: 0.383 |  Val. Acc: 84.30% | B. Val. Loss: 0.383 |  B. Val. Acc: 84.30%\n",
      "\t | Epoch: 005 | Batch Id: 00240 | ET: 23.73s | lr: 9.39e-05 | Val. Loss: 0.355 |  Val. Acc: 84.80% | B. Val. Loss: 0.355 |  B. Val. Acc: 84.80%\n",
      "Epoch: 005 | ET: 34.53s | \t Train Loss: 0.386 | Train Acc: 83.55% \t Val. Loss: 0.430 |  Val. Acc: 82.01% \t | B. Val. Loss: 0.355 |  B. Val. Acc: 84.80%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 006 | Batch Id: 00060 | ET: 4.42s | lr: 9.66e-05 | Val. Loss: 0.398 |  Val. Acc: 82.01% | B. Val. Loss: 0.355 |  B. Val. Acc: 84.80%\n",
      "\t | Epoch: 006 | Batch Id: 00120 | ET: 10.79s | lr: 9.84e-05 | Val. Loss: 0.341 |  Val. Acc: 85.58% | B. Val. Loss: 0.341 |  B. Val. Acc: 85.58%\n",
      "\t | Epoch: 006 | Batch Id: 00180 | ET: 17.27s | lr: 9.96e-05 | Val. Loss: 0.331 |  Val. Acc: 87.05% | B. Val. Loss: 0.331 |  B. Val. Acc: 87.05%\n",
      "\t | Epoch: 006 | Batch Id: 00240 | ET: 23.74s | lr: 1.00e-04 | Val. Loss: 0.302 |  Val. Acc: 88.06% | B. Val. Loss: 0.302 |  B. Val. Acc: 88.06%\n",
      "Epoch: 006 | ET: 34.68s | \t Train Loss: 0.268 | Train Acc: 90.11% \t Val. Loss: 0.298 |  Val. Acc: 88.68% \t | B. Val. Loss: 0.298 |  B. Val. Acc: 88.68%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 007 | Batch Id: 00060 | ET: 4.42s | lr: 9.99e-05 | Val. Loss: 0.345 |  Val. Acc: 86.16% | B. Val. Loss: 0.298 |  B. Val. Acc: 88.68%\n",
      "\t | Epoch: 007 | Batch Id: 00120 | ET: 10.77s | lr: 9.97e-05 | Val. Loss: 0.305 |  Val. Acc: 87.71% | B. Val. Loss: 0.298 |  B. Val. Acc: 88.68%\n",
      "\t | Epoch: 007 | Batch Id: 00180 | ET: 17.14s | lr: 9.93e-05 | Val. Loss: 0.292 |  Val. Acc: 88.37% | B. Val. Loss: 0.292 |  B. Val. Acc: 88.68%\n",
      "\t | Epoch: 007 | Batch Id: 00240 | ET: 23.49s | lr: 9.88e-05 | Val. Loss: 0.282 |  Val. Acc: 88.95% | B. Val. Loss: 0.282 |  B. Val. Acc: 88.95%\n",
      "Epoch: 007 | ET: 34.35s | \t Train Loss: 0.237 | Train Acc: 90.63% \t Val. Loss: 0.279 |  Val. Acc: 88.91% \t | B. Val. Loss: 0.279 |  B. Val. Acc: 88.95%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 008 | Batch Id: 00060 | ET: 4.43s | lr: 9.81e-05 | Val. Loss: 0.321 |  Val. Acc: 87.44% | B. Val. Loss: 0.279 |  B. Val. Acc: 88.95%\n",
      "\t | Epoch: 008 | Batch Id: 00120 | ET: 10.77s | lr: 9.72e-05 | Val. Loss: 0.292 |  Val. Acc: 87.90% | B. Val. Loss: 0.279 |  B. Val. Acc: 88.95%\n",
      "\t | Epoch: 008 | Batch Id: 00180 | ET: 17.13s | lr: 9.63e-05 | Val. Loss: 0.283 |  Val. Acc: 88.83% | B. Val. Loss: 0.279 |  B. Val. Acc: 88.95%\n",
      "\t | Epoch: 008 | Batch Id: 00240 | ET: 23.50s | lr: 9.51e-05 | Val. Loss: 0.306 |  Val. Acc: 88.79% | B. Val. Loss: 0.279 |  B. Val. Acc: 88.95%\n",
      "Epoch: 008 | ET: 34.18s | \t Train Loss: 0.242 | Train Acc: 90.84% \t Val. Loss: 0.296 |  Val. Acc: 88.91% \t | B. Val. Loss: 0.279 |  B. Val. Acc: 88.95%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 009 | Batch Id: 00060 | ET: 4.43s | lr: 9.38e-05 | Val. Loss: 0.296 |  Val. Acc: 88.45% | B. Val. Loss: 0.279 |  B. Val. Acc: 88.95%\n",
      "\t | Epoch: 009 | Batch Id: 00120 | ET: 10.78s | lr: 9.24e-05 | Val. Loss: 0.275 |  Val. Acc: 88.91% | B. Val. Loss: 0.275 |  B. Val. Acc: 88.95%\n",
      "\t | Epoch: 009 | Batch Id: 00180 | ET: 17.13s | lr: 9.09e-05 | Val. Loss: 0.263 |  Val. Acc: 89.72% | B. Val. Loss: 0.263 |  B. Val. Acc: 89.72%\n",
      "\t | Epoch: 009 | Batch Id: 00240 | ET: 23.60s | lr: 8.93e-05 | Val. Loss: 0.266 |  Val. Acc: 89.69% | B. Val. Loss: 0.263 |  B. Val. Acc: 89.72%\n",
      "Epoch: 009 | ET: 34.43s | \t Train Loss: 0.223 | Train Acc: 91.62% \t Val. Loss: 0.270 |  Val. Acc: 89.80% \t | B. Val. Loss: 0.263 |  B. Val. Acc: 89.80%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 010 | Batch Id: 00060 | ET: 4.42s | lr: 8.74e-05 | Val. Loss: 0.319 |  Val. Acc: 86.23% | B. Val. Loss: 0.263 |  B. Val. Acc: 89.80%\n",
      "\t | Epoch: 010 | Batch Id: 00120 | ET: 10.78s | lr: 8.55e-05 | Val. Loss: 0.265 |  Val. Acc: 89.84% | B. Val. Loss: 0.263 |  B. Val. Acc: 89.84%\n",
      "\t | Epoch: 010 | Batch Id: 00180 | ET: 17.30s | lr: 8.35e-05 | Val. Loss: 0.260 |  Val. Acc: 89.26% | B. Val. Loss: 0.260 |  B. Val. Acc: 89.84%\n",
      "\t | Epoch: 010 | Batch Id: 00240 | ET: 23.66s | lr: 8.14e-05 | Val. Loss: 0.262 |  Val. Acc: 89.41% | B. Val. Loss: 0.260 |  B. Val. Acc: 89.84%\n",
      "Epoch: 010 | ET: 34.37s | \t Train Loss: 0.205 | Train Acc: 91.70% \t Val. Loss: 0.264 |  Val. Acc: 89.22% \t | B. Val. Loss: 0.260 |  B. Val. Acc: 89.84%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 011 | Batch Id: 00060 | ET: 4.41s | lr: 7.92e-05 | Val. Loss: 0.279 |  Val. Acc: 89.65% | B. Val. Loss: 0.260 |  B. Val. Acc: 89.84%\n",
      "\t | Epoch: 011 | Batch Id: 00120 | ET: 10.77s | lr: 7.69e-05 | Val. Loss: 0.264 |  Val. Acc: 88.68% | B. Val. Loss: 0.260 |  B. Val. Acc: 89.84%\n",
      "\t | Epoch: 011 | Batch Id: 00180 | ET: 17.13s | lr: 7.45e-05 | Val. Loss: 0.263 |  Val. Acc: 89.92% | B. Val. Loss: 0.260 |  B. Val. Acc: 89.92%\n",
      "\t | Epoch: 011 | Batch Id: 00240 | ET: 23.58s | lr: 7.21e-05 | Val. Loss: 0.252 |  Val. Acc: 89.61% | B. Val. Loss: 0.252 |  B. Val. Acc: 89.92%\n",
      "Epoch: 011 | ET: 34.30s | \t Train Loss: 0.198 | Train Acc: 92.14% \t Val. Loss: 0.253 |  Val. Acc: 89.69% \t | B. Val. Loss: 0.252 |  B. Val. Acc: 89.92%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 012 | Batch Id: 00060 | ET: 4.42s | lr: 6.95e-05 | Val. Loss: 0.270 |  Val. Acc: 89.96% | B. Val. Loss: 0.252 |  B. Val. Acc: 89.96%\n",
      "\t | Epoch: 012 | Batch Id: 00120 | ET: 10.90s | lr: 6.69e-05 | Val. Loss: 0.274 |  Val. Acc: 89.61% | B. Val. Loss: 0.252 |  B. Val. Acc: 89.96%\n",
      "\t | Epoch: 012 | Batch Id: 00180 | ET: 17.26s | lr: 6.43e-05 | Val. Loss: 0.253 |  Val. Acc: 89.65% | B. Val. Loss: 0.252 |  B. Val. Acc: 89.96%\n",
      "\t | Epoch: 012 | Batch Id: 00240 | ET: 23.62s | lr: 6.16e-05 | Val. Loss: 0.255 |  Val. Acc: 90.15% | B. Val. Loss: 0.252 |  B. Val. Acc: 90.15%\n",
      "Epoch: 012 | ET: 34.46s | \t Train Loss: 0.191 | Train Acc: 92.58% \t Val. Loss: 0.254 |  Val. Acc: 90.07% \t | B. Val. Loss: 0.252 |  B. Val. Acc: 90.15%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 013 | Batch Id: 00060 | ET: 4.42s | lr: 5.88e-05 | Val. Loss: 0.264 |  Val. Acc: 89.96% | B. Val. Loss: 0.252 |  B. Val. Acc: 90.15%\n",
      "\t | Epoch: 013 | Batch Id: 00120 | ET: 10.80s | lr: 5.61e-05 | Val. Loss: 0.260 |  Val. Acc: 89.49% | B. Val. Loss: 0.252 |  B. Val. Acc: 90.15%\n",
      "\t | Epoch: 013 | Batch Id: 00180 | ET: 17.16s | lr: 5.33e-05 | Val. Loss: 0.255 |  Val. Acc: 89.65% | B. Val. Loss: 0.252 |  B. Val. Acc: 90.15%\n",
      "\t | Epoch: 013 | Batch Id: 00240 | ET: 23.51s | lr: 5.06e-05 | Val. Loss: 0.258 |  Val. Acc: 89.57% | B. Val. Loss: 0.252 |  B. Val. Acc: 90.15%\n",
      "Epoch: 013 | ET: 34.24s | \t Train Loss: 0.192 | Train Acc: 92.62% \t Val. Loss: 0.258 |  Val. Acc: 89.76% \t | B. Val. Loss: 0.252 |  B. Val. Acc: 90.15%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 014 | Batch Id: 00060 | ET: 4.42s | lr: 4.77e-05 | Val. Loss: 0.267 |  Val. Acc: 89.72% | B. Val. Loss: 0.252 |  B. Val. Acc: 90.15%\n",
      "\t | Epoch: 014 | Batch Id: 00120 | ET: 10.77s | lr: 4.50e-05 | Val. Loss: 0.258 |  Val. Acc: 89.84% | B. Val. Loss: 0.252 |  B. Val. Acc: 90.15%\n",
      "\t | Epoch: 014 | Batch Id: 00180 | ET: 17.15s | lr: 4.23e-05 | Val. Loss: 0.259 |  Val. Acc: 89.72% | B. Val. Loss: 0.252 |  B. Val. Acc: 90.15%\n",
      "\t | Epoch: 014 | Batch Id: 00240 | ET: 23.50s | lr: 3.96e-05 | Val. Loss: 0.254 |  Val. Acc: 89.41% | B. Val. Loss: 0.252 |  B. Val. Acc: 90.15%\n",
      "Epoch: 014 | ET: 34.23s | \t Train Loss: 0.186 | Train Acc: 92.80% \t Val. Loss: 0.253 |  Val. Acc: 89.41% \t | B. Val. Loss: 0.252 |  B. Val. Acc: 90.15%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 015 | Batch Id: 00060 | ET: 4.42s | lr: 3.68e-05 | Val. Loss: 0.257 |  Val. Acc: 90.07% | B. Val. Loss: 0.252 |  B. Val. Acc: 90.15%\n",
      "\t | Epoch: 015 | Batch Id: 00120 | ET: 10.78s | lr: 3.42e-05 | Val. Loss: 0.251 |  Val. Acc: 89.96% | B. Val. Loss: 0.251 |  B. Val. Acc: 90.15%\n",
      "\t | Epoch: 015 | Batch Id: 00180 | ET: 17.14s | lr: 3.16e-05 | Val. Loss: 0.251 |  Val. Acc: 89.41% | B. Val. Loss: 0.251 |  B. Val. Acc: 90.15%\n",
      "\t | Epoch: 015 | Batch Id: 00240 | ET: 23.50s | lr: 2.91e-05 | Val. Loss: 0.251 |  Val. Acc: 89.61% | B. Val. Loss: 0.251 |  B. Val. Acc: 90.15%\n",
      "Epoch: 015 | ET: 34.20s | \t Train Loss: 0.187 | Train Acc: 92.96% \t Val. Loss: 0.250 |  Val. Acc: 89.96% \t | B. Val. Loss: 0.250 |  B. Val. Acc: 90.15%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 016 | Batch Id: 00060 | ET: 4.42s | lr: 2.66e-05 | Val. Loss: 0.267 |  Val. Acc: 90.11% | B. Val. Loss: 0.250 |  B. Val. Acc: 90.15%\n",
      "\t | Epoch: 016 | Batch Id: 00120 | ET: 10.77s | lr: 2.42e-05 | Val. Loss: 0.251 |  Val. Acc: 90.03% | B. Val. Loss: 0.250 |  B. Val. Acc: 90.15%\n",
      "\t | Epoch: 016 | Batch Id: 00180 | ET: 17.13s | lr: 2.19e-05 | Val. Loss: 0.251 |  Val. Acc: 89.88% | B. Val. Loss: 0.250 |  B. Val. Acc: 90.15%\n",
      "\t | Epoch: 016 | Batch Id: 00240 | ET: 23.49s | lr: 1.97e-05 | Val. Loss: 0.251 |  Val. Acc: 89.96% | B. Val. Loss: 0.250 |  B. Val. Acc: 90.15%\n",
      "Epoch: 016 | ET: 34.22s | \t Train Loss: 0.182 | Train Acc: 93.11% \t Val. Loss: 0.251 |  Val. Acc: 90.15% \t | B. Val. Loss: 0.250 |  B. Val. Acc: 90.15%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 017 | Batch Id: 00060 | ET: 4.42s | lr: 1.75e-05 | Val. Loss: 0.261 |  Val. Acc: 89.26% | B. Val. Loss: 0.250 |  B. Val. Acc: 90.15%\n",
      "\t | Epoch: 017 | Batch Id: 00120 | ET: 10.78s | lr: 1.55e-05 | Val. Loss: 0.251 |  Val. Acc: 90.31% | B. Val. Loss: 0.250 |  B. Val. Acc: 90.31%\n",
      "\t | Epoch: 017 | Batch Id: 00180 | ET: 17.26s | lr: 1.36e-05 | Val. Loss: 0.250 |  Val. Acc: 90.00% | B. Val. Loss: 0.250 |  B. Val. Acc: 90.31%\n",
      "\t | Epoch: 017 | Batch Id: 00240 | ET: 23.62s | lr: 1.19e-05 | Val. Loss: 0.250 |  Val. Acc: 90.19% | B. Val. Loss: 0.250 |  B. Val. Acc: 90.31%\n",
      "Epoch: 017 | ET: 34.35s | \t Train Loss: 0.184 | Train Acc: 93.27% \t Val. Loss: 0.250 |  Val. Acc: 90.23% \t | B. Val. Loss: 0.250 |  B. Val. Acc: 90.31%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 018 | Batch Id: 00060 | ET: 4.42s | lr: 1.01e-05 | Val. Loss: 0.260 |  Val. Acc: 90.27% | B. Val. Loss: 0.250 |  B. Val. Acc: 90.31%\n",
      "\t | Epoch: 018 | Batch Id: 00120 | ET: 10.78s | lr: 8.61e-06 | Val. Loss: 0.249 |  Val. Acc: 89.88% | B. Val. Loss: 0.249 |  B. Val. Acc: 90.31%\n",
      "\t | Epoch: 018 | Batch Id: 00180 | ET: 17.13s | lr: 7.21e-06 | Val. Loss: 0.249 |  Val. Acc: 90.03% | B. Val. Loss: 0.249 |  B. Val. Acc: 90.31%\n",
      "\t | Epoch: 018 | Batch Id: 00240 | ET: 23.48s | lr: 5.94e-06 | Val. Loss: 0.250 |  Val. Acc: 90.15% | B. Val. Loss: 0.249 |  B. Val. Acc: 90.31%\n",
      "Epoch: 018 | ET: 34.21s | \t Train Loss: 0.183 | Train Acc: 93.25% \t Val. Loss: 0.250 |  Val. Acc: 90.11% \t | B. Val. Loss: 0.249 |  B. Val. Acc: 90.31%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 019 | Batch Id: 00060 | ET: 4.42s | lr: 4.78e-06 | Val. Loss: 0.257 |  Val. Acc: 90.31% | B. Val. Loss: 0.249 |  B. Val. Acc: 90.31%\n",
      "\t | Epoch: 019 | Batch Id: 00120 | ET: 10.80s | lr: 3.79e-06 | Val. Loss: 0.252 |  Val. Acc: 90.50% | B. Val. Loss: 0.249 |  B. Val. Acc: 90.50%\n",
      "\t | Epoch: 019 | Batch Id: 00180 | ET: 17.27s | lr: 2.95e-06 | Val. Loss: 0.249 |  Val. Acc: 90.15% | B. Val. Loss: 0.249 |  B. Val. Acc: 90.50%\n",
      "\t | Epoch: 019 | Batch Id: 00240 | ET: 23.62s | lr: 2.26e-06 | Val. Loss: 0.250 |  Val. Acc: 90.31% | B. Val. Loss: 0.249 |  B. Val. Acc: 90.50%\n",
      "Epoch: 019 | ET: 34.35s | \t Train Loss: 0.180 | Train Acc: 93.23% \t Val. Loss: 0.250 |  Val. Acc: 90.31% \t | B. Val. Loss: 0.249 |  B. Val. Acc: 90.50%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 020 | Batch Id: 00060 | ET: 4.43s | lr: 1.70e-06 | Val. Loss: 0.254 |  Val. Acc: 90.27% | B. Val. Loss: 0.249 |  B. Val. Acc: 90.50%\n",
      "\t | Epoch: 020 | Batch Id: 00120 | ET: 10.79s | lr: 1.32e-06 | Val. Loss: 0.252 |  Val. Acc: 90.23% | B. Val. Loss: 0.249 |  B. Val. Acc: 90.50%\n",
      "\t | Epoch: 020 | Batch Id: 00180 | ET: 17.14s | lr: 1.08e-06 | Val. Loss: 0.251 |  Val. Acc: 90.31% | B. Val. Loss: 0.249 |  B. Val. Acc: 90.50%\n",
      "\t | Epoch: 020 | Batch Id: 00240 | ET: 23.49s | lr: 1.00e-06 | Val. Loss: 0.250 |  Val. Acc: 90.15% | B. Val. Loss: 0.249 |  B. Val. Acc: 90.50%\n",
      "Epoch: 020 | ET: 34.19s | \t Train Loss: 0.182 | Train Acc: 93.25% \t Val. Loss: 0.250 |  Val. Acc: 90.15% \t | B. Val. Loss: 0.249 |  B. Val. Acc: 90.50%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Number of the parameters: 2645537\n",
      "\n",
      "Training\n",
      "-----    4    ----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 001 | Batch Id: 00060 | ET: 4.42s | lr: 1.04e-05 | Val. Loss: 0.559 |  Val. Acc: 70.18% | B. Val. Loss: 0.559 |  B. Val. Acc: 70.18%\n",
      "\t | Epoch: 001 | Batch Id: 00120 | ET: 10.81s | lr: 1.15e-05 | Val. Loss: 0.562 |  Val. Acc: 69.99% | B. Val. Loss: 0.559 |  B. Val. Acc: 70.18%\n",
      "\t | Epoch: 001 | Batch Id: 00180 | ET: 17.19s | lr: 1.33e-05 | Val. Loss: 0.538 |  Val. Acc: 74.41% | B. Val. Loss: 0.538 |  B. Val. Acc: 74.41%\n",
      "\t | Epoch: 001 | Batch Id: 00240 | ET: 23.66s | lr: 1.59e-05 | Val. Loss: 0.513 |  Val. Acc: 74.99% | B. Val. Loss: 0.513 |  B. Val. Acc: 74.99%\n",
      "Epoch: 001 | ET: 34.49s | \t Train Loss: 0.505 | Train Acc: 74.32% \t Val. Loss: 0.512 |  Val. Acc: 74.72% \t | B. Val. Loss: 0.512 |  B. Val. Acc: 74.99%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 002 | Batch Id: 00060 | ET: 4.42s | lr: 1.92e-05 | Val. Loss: 0.534 |  Val. Acc: 72.74% | B. Val. Loss: 0.512 |  B. Val. Acc: 74.99%\n",
      "\t | Epoch: 002 | Batch Id: 00120 | ET: 10.78s | lr: 2.31e-05 | Val. Loss: 0.499 |  Val. Acc: 78.09% | B. Val. Loss: 0.499 |  B. Val. Acc: 78.09%\n",
      "\t | Epoch: 002 | Batch Id: 00180 | ET: 17.26s | lr: 2.74e-05 | Val. Loss: 0.475 |  Val. Acc: 77.94% | B. Val. Loss: 0.475 |  B. Val. Acc: 78.09%\n",
      "\t | Epoch: 002 | Batch Id: 00240 | ET: 23.61s | lr: 3.23e-05 | Val. Loss: 0.484 |  Val. Acc: 79.26% | B. Val. Loss: 0.475 |  B. Val. Acc: 79.26%\n",
      "Epoch: 002 | ET: 34.45s | \t Train Loss: 0.461 | Train Acc: 79.02% \t Val. Loss: 0.469 |  Val. Acc: 79.18% \t | B. Val. Loss: 0.469 |  B. Val. Acc: 79.26%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 003 | Batch Id: 00060 | ET: 4.42s | lr: 3.77e-05 | Val. Loss: 0.487 |  Val. Acc: 76.00% | B. Val. Loss: 0.469 |  B. Val. Acc: 79.26%\n",
      "\t | Epoch: 003 | Batch Id: 00120 | ET: 10.77s | lr: 4.32e-05 | Val. Loss: 0.470 |  Val. Acc: 78.79% | B. Val. Loss: 0.469 |  B. Val. Acc: 79.26%\n",
      "\t | Epoch: 003 | Batch Id: 00180 | ET: 17.11s | lr: 4.89e-05 | Val. Loss: 0.449 |  Val. Acc: 79.53% | B. Val. Loss: 0.449 |  B. Val. Acc: 79.53%\n",
      "\t | Epoch: 003 | Batch Id: 00240 | ET: 23.59s | lr: 5.48e-05 | Val. Loss: 0.449 |  Val. Acc: 79.41% | B. Val. Loss: 0.449 |  B. Val. Acc: 79.53%\n",
      "Epoch: 003 | ET: 34.33s | \t Train Loss: 0.445 | Train Acc: 80.43% \t Val. Loss: 0.456 |  Val. Acc: 79.53% \t | B. Val. Loss: 0.449 |  B. Val. Acc: 79.53%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 004 | Batch Id: 00060 | ET: 4.42s | lr: 6.08e-05 | Val. Loss: 0.465 |  Val. Acc: 78.67% | B. Val. Loss: 0.449 |  B. Val. Acc: 79.53%\n",
      "\t | Epoch: 004 | Batch Id: 00120 | ET: 10.77s | lr: 6.65e-05 | Val. Loss: 0.452 |  Val. Acc: 79.72% | B. Val. Loss: 0.449 |  B. Val. Acc: 79.72%\n",
      "\t | Epoch: 004 | Batch Id: 00180 | ET: 17.22s | lr: 7.21e-05 | Val. Loss: 0.439 |  Val. Acc: 79.95% | B. Val. Loss: 0.439 |  B. Val. Acc: 79.95%\n",
      "\t | Epoch: 004 | Batch Id: 00240 | ET: 23.70s | lr: 7.73e-05 | Val. Loss: 0.423 |  Val. Acc: 80.85% | B. Val. Loss: 0.423 |  B. Val. Acc: 80.85%\n",
      "Epoch: 004 | ET: 34.53s | \t Train Loss: 0.405 | Train Acc: 81.52% \t Val. Loss: 0.420 |  Val. Acc: 80.42% \t | B. Val. Loss: 0.420 |  B. Val. Acc: 80.85%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 005 | Batch Id: 00060 | ET: 4.42s | lr: 8.23e-05 | Val. Loss: 0.436 |  Val. Acc: 80.15% | B. Val. Loss: 0.420 |  B. Val. Acc: 80.85%\n",
      "\t | Epoch: 005 | Batch Id: 00120 | ET: 10.78s | lr: 8.67e-05 | Val. Loss: 0.399 |  Val. Acc: 81.54% | B. Val. Loss: 0.399 |  B. Val. Acc: 81.54%\n",
      "\t | Epoch: 005 | Batch Id: 00180 | ET: 17.25s | lr: 9.06e-05 | Val. Loss: 0.375 |  Val. Acc: 84.22% | B. Val. Loss: 0.375 |  B. Val. Acc: 84.22%\n",
      "\t | Epoch: 005 | Batch Id: 00240 | ET: 23.69s | lr: 9.39e-05 | Val. Loss: 0.341 |  Val. Acc: 85.19% | B. Val. Loss: 0.341 |  B. Val. Acc: 85.19%\n",
      "Epoch: 005 | ET: 34.62s | \t Train Loss: 0.324 | Train Acc: 86.61% \t Val. Loss: 0.354 |  Val. Acc: 85.50% \t | B. Val. Loss: 0.341 |  B. Val. Acc: 85.50%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 006 | Batch Id: 00060 | ET: 4.42s | lr: 9.66e-05 | Val. Loss: 0.376 |  Val. Acc: 82.98% | B. Val. Loss: 0.341 |  B. Val. Acc: 85.50%\n",
      "\t | Epoch: 006 | Batch Id: 00120 | ET: 10.78s | lr: 9.84e-05 | Val. Loss: 0.329 |  Val. Acc: 86.70% | B. Val. Loss: 0.329 |  B. Val. Acc: 86.70%\n",
      "\t | Epoch: 006 | Batch Id: 00180 | ET: 17.25s | lr: 9.96e-05 | Val. Loss: 0.321 |  Val. Acc: 87.86% | B. Val. Loss: 0.321 |  B. Val. Acc: 87.86%\n",
      "\t | Epoch: 006 | Batch Id: 00240 | ET: 23.72s | lr: 1.00e-04 | Val. Loss: 0.303 |  Val. Acc: 88.37% | B. Val. Loss: 0.303 |  B. Val. Acc: 88.37%\n",
      "Epoch: 006 | ET: 34.54s | \t Train Loss: 0.274 | Train Acc: 89.70% \t Val. Loss: 0.314 |  Val. Acc: 88.13% \t | B. Val. Loss: 0.303 |  B. Val. Acc: 88.37%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 007 | Batch Id: 00060 | ET: 4.41s | lr: 9.99e-05 | Val. Loss: 0.335 |  Val. Acc: 87.63% | B. Val. Loss: 0.303 |  B. Val. Acc: 88.37%\n",
      "\t | Epoch: 007 | Batch Id: 00120 | ET: 10.77s | lr: 9.97e-05 | Val. Loss: 0.304 |  Val. Acc: 87.94% | B. Val. Loss: 0.303 |  B. Val. Acc: 88.37%\n",
      "\t | Epoch: 007 | Batch Id: 00180 | ET: 17.12s | lr: 9.93e-05 | Val. Loss: 0.291 |  Val. Acc: 88.76% | B. Val. Loss: 0.291 |  B. Val. Acc: 88.76%\n",
      "\t | Epoch: 007 | Batch Id: 00240 | ET: 23.59s | lr: 9.88e-05 | Val. Loss: 0.288 |  Val. Acc: 88.10% | B. Val. Loss: 0.288 |  B. Val. Acc: 88.76%\n",
      "Epoch: 007 | ET: 34.31s | \t Train Loss: 0.240 | Train Acc: 90.55% \t Val. Loss: 0.287 |  Val. Acc: 88.52% \t | B. Val. Loss: 0.287 |  B. Val. Acc: 88.76%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 008 | Batch Id: 00060 | ET: 4.42s | lr: 9.81e-05 | Val. Loss: 0.331 |  Val. Acc: 87.44% | B. Val. Loss: 0.287 |  B. Val. Acc: 88.76%\n",
      "\t | Epoch: 008 | Batch Id: 00120 | ET: 10.79s | lr: 9.72e-05 | Val. Loss: 0.287 |  Val. Acc: 88.10% | B. Val. Loss: 0.287 |  B. Val. Acc: 88.76%\n",
      "\t | Epoch: 008 | Batch Id: 00180 | ET: 17.15s | lr: 9.63e-05 | Val. Loss: 0.274 |  Val. Acc: 89.65% | B. Val. Loss: 0.274 |  B. Val. Acc: 89.65%\n",
      "\t | Epoch: 008 | Batch Id: 00240 | ET: 23.60s | lr: 9.51e-05 | Val. Loss: 0.271 |  Val. Acc: 89.26% | B. Val. Loss: 0.271 |  B. Val. Acc: 89.65%\n",
      "Epoch: 008 | ET: 34.33s | \t Train Loss: 0.224 | Train Acc: 91.19% \t Val. Loss: 0.271 |  Val. Acc: 89.65% \t | B. Val. Loss: 0.271 |  B. Val. Acc: 89.65%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 009 | Batch Id: 00060 | ET: 4.42s | lr: 9.38e-05 | Val. Loss: 0.301 |  Val. Acc: 88.06% | B. Val. Loss: 0.271 |  B. Val. Acc: 89.65%\n",
      "\t | Epoch: 009 | Batch Id: 00120 | ET: 10.78s | lr: 9.24e-05 | Val. Loss: 0.276 |  Val. Acc: 89.45% | B. Val. Loss: 0.271 |  B. Val. Acc: 89.65%\n",
      "\t | Epoch: 009 | Batch Id: 00180 | ET: 17.15s | lr: 9.09e-05 | Val. Loss: 0.273 |  Val. Acc: 89.57% | B. Val. Loss: 0.271 |  B. Val. Acc: 89.65%\n",
      "\t | Epoch: 009 | Batch Id: 00240 | ET: 23.50s | lr: 8.93e-05 | Val. Loss: 0.268 |  Val. Acc: 89.96% | B. Val. Loss: 0.268 |  B. Val. Acc: 89.96%\n",
      "Epoch: 009 | ET: 34.46s | \t Train Loss: 0.217 | Train Acc: 91.72% \t Val. Loss: 0.267 |  Val. Acc: 90.07% \t | B. Val. Loss: 0.267 |  B. Val. Acc: 90.07%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 010 | Batch Id: 00060 | ET: 4.42s | lr: 8.74e-05 | Val. Loss: 0.284 |  Val. Acc: 88.48% | B. Val. Loss: 0.267 |  B. Val. Acc: 90.07%\n",
      "\t | Epoch: 010 | Batch Id: 00120 | ET: 10.78s | lr: 8.55e-05 | Val. Loss: 0.283 |  Val. Acc: 89.92% | B. Val. Loss: 0.267 |  B. Val. Acc: 90.07%\n",
      "\t | Epoch: 010 | Batch Id: 00180 | ET: 17.15s | lr: 8.35e-05 | Val. Loss: 0.275 |  Val. Acc: 88.17% | B. Val. Loss: 0.267 |  B. Val. Acc: 90.07%\n",
      "\t | Epoch: 010 | Batch Id: 00240 | ET: 23.51s | lr: 8.14e-05 | Val. Loss: 0.268 |  Val. Acc: 89.30% | B. Val. Loss: 0.267 |  B. Val. Acc: 90.07%\n",
      "Epoch: 010 | ET: 34.24s | \t Train Loss: 0.209 | Train Acc: 92.09% \t Val. Loss: 0.264 |  Val. Acc: 89.45% \t | B. Val. Loss: 0.264 |  B. Val. Acc: 90.07%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 011 | Batch Id: 00060 | ET: 4.42s | lr: 7.92e-05 | Val. Loss: 0.276 |  Val. Acc: 89.30% | B. Val. Loss: 0.264 |  B. Val. Acc: 90.07%\n",
      "\t | Epoch: 011 | Batch Id: 00120 | ET: 10.78s | lr: 7.69e-05 | Val. Loss: 0.259 |  Val. Acc: 90.11% | B. Val. Loss: 0.259 |  B. Val. Acc: 90.11%\n",
      "\t | Epoch: 011 | Batch Id: 00180 | ET: 17.26s | lr: 7.45e-05 | Val. Loss: 0.258 |  Val. Acc: 90.27% | B. Val. Loss: 0.258 |  B. Val. Acc: 90.27%\n",
      "\t | Epoch: 011 | Batch Id: 00240 | ET: 23.74s | lr: 7.21e-05 | Val. Loss: 0.263 |  Val. Acc: 89.88% | B. Val. Loss: 0.258 |  B. Val. Acc: 90.27%\n",
      "Epoch: 011 | ET: 34.46s | \t Train Loss: 0.201 | Train Acc: 92.25% \t Val. Loss: 0.262 |  Val. Acc: 90.23% \t | B. Val. Loss: 0.258 |  B. Val. Acc: 90.27%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 012 | Batch Id: 00060 | ET: 4.42s | lr: 6.95e-05 | Val. Loss: 0.283 |  Val. Acc: 88.76% | B. Val. Loss: 0.258 |  B. Val. Acc: 90.27%\n",
      "\t | Epoch: 012 | Batch Id: 00120 | ET: 10.78s | lr: 6.69e-05 | Val. Loss: 0.263 |  Val. Acc: 89.10% | B. Val. Loss: 0.258 |  B. Val. Acc: 90.27%\n",
      "\t | Epoch: 012 | Batch Id: 00180 | ET: 17.13s | lr: 6.43e-05 | Val. Loss: 0.260 |  Val. Acc: 89.53% | B. Val. Loss: 0.258 |  B. Val. Acc: 90.27%\n",
      "\t | Epoch: 012 | Batch Id: 00240 | ET: 23.51s | lr: 6.16e-05 | Val. Loss: 0.258 |  Val. Acc: 89.57% | B. Val. Loss: 0.258 |  B. Val. Acc: 90.27%\n",
      "Epoch: 012 | ET: 34.24s | \t Train Loss: 0.208 | Train Acc: 92.01% \t Val. Loss: 0.260 |  Val. Acc: 89.26% \t | B. Val. Loss: 0.258 |  B. Val. Acc: 90.27%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 013 | Batch Id: 00060 | ET: 4.42s | lr: 5.88e-05 | Val. Loss: 0.275 |  Val. Acc: 89.26% | B. Val. Loss: 0.258 |  B. Val. Acc: 90.27%\n",
      "\t | Epoch: 013 | Batch Id: 00120 | ET: 10.78s | lr: 5.61e-05 | Val. Loss: 0.266 |  Val. Acc: 89.76% | B. Val. Loss: 0.258 |  B. Val. Acc: 90.27%\n",
      "\t | Epoch: 013 | Batch Id: 00180 | ET: 17.14s | lr: 5.33e-05 | Val. Loss: 0.254 |  Val. Acc: 90.27% | B. Val. Loss: 0.254 |  B. Val. Acc: 90.27%\n",
      "\t | Epoch: 013 | Batch Id: 00240 | ET: 23.50s | lr: 5.06e-05 | Val. Loss: 0.261 |  Val. Acc: 89.14% | B. Val. Loss: 0.254 |  B. Val. Acc: 90.27%\n",
      "Epoch: 013 | ET: 34.18s | \t Train Loss: 0.211 | Train Acc: 91.79% \t Val. Loss: 0.266 |  Val. Acc: 88.83% \t | B. Val. Loss: 0.254 |  B. Val. Acc: 90.27%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 014 | Batch Id: 00060 | ET: 4.42s | lr: 4.77e-05 | Val. Loss: 0.276 |  Val. Acc: 89.18% | B. Val. Loss: 0.254 |  B. Val. Acc: 90.27%\n",
      "\t | Epoch: 014 | Batch Id: 00120 | ET: 10.78s | lr: 4.50e-05 | Val. Loss: 0.254 |  Val. Acc: 90.00% | B. Val. Loss: 0.254 |  B. Val. Acc: 90.27%\n",
      "\t | Epoch: 014 | Batch Id: 00180 | ET: 17.13s | lr: 4.23e-05 | Val. Loss: 0.249 |  Val. Acc: 90.31% | B. Val. Loss: 0.249 |  B. Val. Acc: 90.31%\n",
      "\t | Epoch: 014 | Batch Id: 00240 | ET: 23.60s | lr: 3.96e-05 | Val. Loss: 0.261 |  Val. Acc: 90.11% | B. Val. Loss: 0.249 |  B. Val. Acc: 90.31%\n",
      "Epoch: 014 | ET: 34.29s | \t Train Loss: 0.193 | Train Acc: 92.63% \t Val. Loss: 0.261 |  Val. Acc: 90.23% \t | B. Val. Loss: 0.249 |  B. Val. Acc: 90.31%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 015 | Batch Id: 00060 | ET: 4.41s | lr: 3.68e-05 | Val. Loss: 0.266 |  Val. Acc: 90.15% | B. Val. Loss: 0.249 |  B. Val. Acc: 90.31%\n",
      "\t | Epoch: 015 | Batch Id: 00120 | ET: 10.78s | lr: 3.42e-05 | Val. Loss: 0.252 |  Val. Acc: 90.31% | B. Val. Loss: 0.249 |  B. Val. Acc: 90.31%\n",
      "\t | Epoch: 015 | Batch Id: 00180 | ET: 17.13s | lr: 3.16e-05 | Val. Loss: 0.252 |  Val. Acc: 90.50% | B. Val. Loss: 0.249 |  B. Val. Acc: 90.50%\n",
      "\t | Epoch: 015 | Batch Id: 00240 | ET: 23.58s | lr: 2.91e-05 | Val. Loss: 0.252 |  Val. Acc: 90.46% | B. Val. Loss: 0.249 |  B. Val. Acc: 90.50%\n",
      "Epoch: 015 | ET: 34.41s | \t Train Loss: 0.181 | Train Acc: 93.03% \t Val. Loss: 0.251 |  Val. Acc: 90.54% \t | B. Val. Loss: 0.249 |  B. Val. Acc: 90.54%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 016 | Batch Id: 00060 | ET: 4.42s | lr: 2.66e-05 | Val. Loss: 0.269 |  Val. Acc: 89.53% | B. Val. Loss: 0.249 |  B. Val. Acc: 90.54%\n",
      "\t | Epoch: 016 | Batch Id: 00120 | ET: 10.79s | lr: 2.42e-05 | Val. Loss: 0.252 |  Val. Acc: 90.35% | B. Val. Loss: 0.249 |  B. Val. Acc: 90.54%\n",
      "\t | Epoch: 016 | Batch Id: 00180 | ET: 17.14s | lr: 2.19e-05 | Val. Loss: 0.249 |  Val. Acc: 90.38% | B. Val. Loss: 0.249 |  B. Val. Acc: 90.54%\n",
      "\t | Epoch: 016 | Batch Id: 00240 | ET: 23.50s | lr: 1.97e-05 | Val. Loss: 0.252 |  Val. Acc: 89.96% | B. Val. Loss: 0.249 |  B. Val. Acc: 90.54%\n",
      "Epoch: 016 | ET: 34.19s | \t Train Loss: 0.191 | Train Acc: 92.79% \t Val. Loss: 0.253 |  Val. Acc: 89.84% \t | B. Val. Loss: 0.249 |  B. Val. Acc: 90.54%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 017 | Batch Id: 00060 | ET: 4.42s | lr: 1.75e-05 | Val. Loss: 0.266 |  Val. Acc: 89.88% | B. Val. Loss: 0.249 |  B. Val. Acc: 90.54%\n",
      "\t | Epoch: 017 | Batch Id: 00120 | ET: 10.77s | lr: 1.55e-05 | Val. Loss: 0.258 |  Val. Acc: 90.31% | B. Val. Loss: 0.249 |  B. Val. Acc: 90.54%\n",
      "\t | Epoch: 017 | Batch Id: 00180 | ET: 17.15s | lr: 1.36e-05 | Val. Loss: 0.249 |  Val. Acc: 90.46% | B. Val. Loss: 0.249 |  B. Val. Acc: 90.54%\n",
      "\t | Epoch: 017 | Batch Id: 00240 | ET: 23.50s | lr: 1.19e-05 | Val. Loss: 0.249 |  Val. Acc: 90.23% | B. Val. Loss: 0.249 |  B. Val. Acc: 90.54%\n",
      "Epoch: 017 | ET: 34.20s | \t Train Loss: 0.180 | Train Acc: 93.23% \t Val. Loss: 0.249 |  Val. Acc: 90.27% \t | B. Val. Loss: 0.249 |  B. Val. Acc: 90.54%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 018 | Batch Id: 00060 | ET: 4.42s | lr: 1.01e-05 | Val. Loss: 0.260 |  Val. Acc: 89.88% | B. Val. Loss: 0.249 |  B. Val. Acc: 90.54%\n",
      "\t | Epoch: 018 | Batch Id: 00120 | ET: 10.78s | lr: 8.61e-06 | Val. Loss: 0.250 |  Val. Acc: 90.23% | B. Val. Loss: 0.249 |  B. Val. Acc: 90.54%\n",
      "\t | Epoch: 018 | Batch Id: 00180 | ET: 17.14s | lr: 7.21e-06 | Val. Loss: 0.249 |  Val. Acc: 90.38% | B. Val. Loss: 0.249 |  B. Val. Acc: 90.54%\n",
      "\t | Epoch: 018 | Batch Id: 00240 | ET: 23.50s | lr: 5.94e-06 | Val. Loss: 0.247 |  Val. Acc: 90.15% | B. Val. Loss: 0.247 |  B. Val. Acc: 90.54%\n",
      "Epoch: 018 | ET: 34.19s | \t Train Loss: 0.182 | Train Acc: 93.14% \t Val. Loss: 0.248 |  Val. Acc: 90.11% \t | B. Val. Loss: 0.247 |  B. Val. Acc: 90.54%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 019 | Batch Id: 00060 | ET: 4.43s | lr: 4.78e-06 | Val. Loss: 0.259 |  Val. Acc: 89.88% | B. Val. Loss: 0.247 |  B. Val. Acc: 90.54%\n",
      "\t | Epoch: 019 | Batch Id: 00120 | ET: 10.77s | lr: 3.79e-06 | Val. Loss: 0.252 |  Val. Acc: 90.07% | B. Val. Loss: 0.247 |  B. Val. Acc: 90.54%\n",
      "\t | Epoch: 019 | Batch Id: 00180 | ET: 17.13s | lr: 2.95e-06 | Val. Loss: 0.250 |  Val. Acc: 90.31% | B. Val. Loss: 0.247 |  B. Val. Acc: 90.54%\n",
      "\t | Epoch: 019 | Batch Id: 00240 | ET: 23.49s | lr: 2.26e-06 | Val. Loss: 0.250 |  Val. Acc: 90.38% | B. Val. Loss: 0.247 |  B. Val. Acc: 90.54%\n",
      "Epoch: 019 | ET: 34.19s | \t Train Loss: 0.183 | Train Acc: 92.94% \t Val. Loss: 0.250 |  Val. Acc: 90.35% \t | B. Val. Loss: 0.247 |  B. Val. Acc: 90.54%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\t | Epoch: 020 | Batch Id: 00060 | ET: 4.42s | lr: 1.70e-06 | Val. Loss: 0.265 |  Val. Acc: 89.84% | B. Val. Loss: 0.247 |  B. Val. Acc: 90.54%\n",
      "\t | Epoch: 020 | Batch Id: 00120 | ET: 10.78s | lr: 1.32e-06 | Val. Loss: 0.261 |  Val. Acc: 90.11% | B. Val. Loss: 0.247 |  B. Val. Acc: 90.54%\n",
      "\t | Epoch: 020 | Batch Id: 00180 | ET: 17.12s | lr: 1.08e-06 | Val. Loss: 0.256 |  Val. Acc: 90.15% | B. Val. Loss: 0.247 |  B. Val. Acc: 90.54%\n",
      "\t | Epoch: 020 | Batch Id: 00240 | ET: 23.48s | lr: 1.00e-06 | Val. Loss: 0.255 |  Val. Acc: 90.15% | B. Val. Loss: 0.247 |  B. Val. Acc: 90.54%\n",
      "Epoch: 020 | ET: 34.21s | \t Train Loss: 0.188 | Train Acc: 92.92% \t Val. Loss: 0.255 |  Val. Acc: 90.15% \t | B. Val. Loss: 0.247 |  B. Val. Acc: 90.54%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for sim_num in range(0,5):\n",
    "    model = ResNet9_FNO_small()\n",
    "    \n",
    "    if len(dev_names)>1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "    model.to(device)\n",
    "    print(f\"Number of the parameters: {count_parameters(model)}\\n\")\n",
    "    #optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9, weight_decay=WEIGHT_DECAY, nesterov=True)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    #criterion = torch.nn.CrossEntropyLoss(reduction=\"sum\").to(device)\n",
    "    criterion = torch.nn.BCEWithLogitsLoss(reduction=\"sum\").to(device)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=LEARNING_RATE, div_factor=DIV_FACTOR, final_div_factor=FINAL_DIV_FACTOR, steps_per_epoch=len(train_loader), epochs = EPOCHS, verbose=0)\n",
    "    #scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=LEARNING_RATE/DIV_FACTOR, T_max=len(train_loader)*EPOCHS, verbose=0)\n",
    "\n",
    "    train_accs = []\n",
    "    train_losses = []\n",
    "    valid_accs = []\n",
    "    valid_losses = []\n",
    "\n",
    "\n",
    "\n",
    "    f = open(f\"{RESULTS_FILENAME}_{sim_num}.csv\", \"w\")\n",
    "    f.write(160*\"-\"+\"\\n\")\n",
    "    f.write(f\"Device: {dev_names[0]} | Number: {len(dev_names)}\\n\")\n",
    "    f.write(f\"Epochs: {EPOCHS}\\n\")\n",
    "    f.write(f\"Optimizer: {type (optimizer).__name__}\\n\") \n",
    "    f.write(f\"Scheduler: {type (scheduler).__name__}\\n\") \n",
    "    f.write(f\"Div factor: {DIV_FACTOR}\\n\") \n",
    "    f.write(f\"Final div factor: {FINAL_DIV_FACTOR}\\n\") \n",
    "    f.write(f\"Weight decay: {WEIGHT_DECAY}\\n\") \n",
    "    f.write(f\"Learning rate: {LEARNING_RATE}\\n\") \n",
    "    f.write(f\"Number of the parameters: {count_parameters(model)}\\n\")\n",
    "    f.write(f\"Model: {model}\\n\")\n",
    "    f.write(160*\"-\"+\"\\n\")\n",
    "    f.close()\n",
    "    print(\"Training\")\n",
    "    print(5 * \"-\" + f\"{sim_num:5}\" + 4*\" \"+ 160 * \"-\")\n",
    "\n",
    "    best_valid_loss = float('inf')\n",
    "    best_valid_acc = -1.0\n",
    "    valid_acc = 0.0\n",
    "\n",
    "    all_time_s = 0.0\n",
    "    lr = 0.0\n",
    "\n",
    "    train_accs = []\n",
    "    train_losses = []\n",
    "    valid_accs = []\n",
    "    valid_losses = []\n",
    "    valid_indices = []\n",
    "\n",
    "    # Training the `sim_num`-th model\n",
    "    for epoch in range(EPOCHS):\n",
    "\n",
    "        start_time = default_timer()\n",
    "\n",
    "        epoch_loss = 0.0\n",
    "        epoch_acc = 0.0\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        batch_id = 0\n",
    "        number_of_training_elements = 0\n",
    "\n",
    "        valid_accs_temp = []\n",
    "        valid_losses_temp = []\n",
    "        valid_indices_temp = []\n",
    "\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.float().to(device).view(-1,1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred = model(x)\n",
    "            #print(\"LOOOOK\")\n",
    "            #print(y_pred.shape,y.shape)\n",
    "            loss = criterion(y_pred, y)\n",
    "            #print(\"LOOOOK\")\n",
    "            batch_size = x.shape[0]\n",
    "            number_of_training_elements += batch_size\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #print(\"LOOOOK\")\n",
    "\n",
    "            end_time = default_timer()\n",
    "\n",
    "            # Evaluating the model\n",
    "            if (batch_id+1)%EVAL_FREQ==0:\n",
    "\n",
    "                valid_indices_temp.append(batch_id+1)\n",
    "                valid_loss, valid_acc = evaluate(model, valid_loader, criterion, device)\n",
    "\n",
    "                valid_losses_temp.append(valid_loss)\n",
    "                valid_accs_temp.append(valid_acc)\n",
    "\n",
    "                if valid_acc > best_valid_acc:\n",
    "                    best_valid_acc = valid_acc\n",
    "                    torch.save(model.state_dict(), f\"{BEST_MODEL_FILENAME}_{sim_num}.pt\")\n",
    "\n",
    "                if valid_loss < best_valid_loss:\n",
    "                    best_valid_loss = valid_loss\n",
    "\n",
    "                lr = scheduler.get_last_lr()[0]\n",
    "\n",
    "                line = f'\\t | Epoch: {epoch+1:03} | Batch Id: {batch_id+1:05} | ET: {end_time-start_time:.2f}s | lr: {lr:.2e} | Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}% | B. Val. Loss: {best_valid_loss:.3f} |  B. Val. Acc: {best_valid_acc*100:.2f}%'\n",
    "                print(line)\n",
    "                f = open(f\"{RESULTS_FILENAME}_{sim_num}.csv\", \"a\")\n",
    "                f.write(line+\"\\n\")\n",
    "                f.close()\n",
    "\n",
    "\n",
    "\n",
    "            batch_id+=1\n",
    "            scheduler.step()\n",
    "\n",
    "        valid_indices_temp.append(batch_id)\n",
    "        valid_loss, valid_acc = evaluate(model, valid_loader, criterion, device)\n",
    "\n",
    "        valid_losses_temp.append(valid_loss)\n",
    "        valid_accs_temp.append(valid_acc)\n",
    "\n",
    "        valid_losses.append(valid_losses_temp)\n",
    "        valid_accs.append(valid_accs_temp)\n",
    "\n",
    "        valid_indices.append(valid_indices_temp)\n",
    "\n",
    "        if valid_acc > best_valid_acc:\n",
    "            best_valid_acc = valid_acc\n",
    "            torch.save(model.state_dict(), f\"{BEST_MODEL_FILENAME}_{sim_num}.pt\")\n",
    "\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "\n",
    "        train_loss, train_acc = evaluate(model, train_loader, criterion, device)\n",
    "\n",
    "        end_time = default_timer()\n",
    "\n",
    "        all_time_s += end_time - start_time\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "\n",
    "        line = f'Epoch: {epoch+1:03} | ET: {end_time-start_time:.2f}s | \\t Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% \\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}% \\t | B. Val. Loss: {best_valid_loss:.3f} |  B. Val. Acc: {best_valid_acc*100:.2f}%'\n",
    "        print(line)\n",
    "        print(160*\"-\")\n",
    "\n",
    "        f = open(f\"{RESULTS_FILENAME}_{sim_num}.csv\", \"a\")\n",
    "        f.write(line+\"\\n\")\n",
    "        f.write(160*\"-\"+\"\\n\")\n",
    "        f.close()\n",
    "\n",
    "    line = f\"\\nDuration: {all_time_s:.2f}s\\n\"\n",
    "    f = open(f\"{RESULTS_FILENAME}_{sim_num}.csv\", \"a\")\n",
    "    f.write(line+\"\\n\")\n",
    "    f.write(80*\"-\"+\"\\n\")\n",
    "    f.close()\n",
    "\n",
    "    # Saving the results for analyzing them later in the evaluation part\n",
    "    valid_losses_plot = []\n",
    "    valid_accs_plot = []\n",
    "    epoch_plot = []\n",
    "    for epoch in range(len(valid_accs)):\n",
    "        valid_accs_temp = valid_accs[epoch]\n",
    "        valid_losses_temp = valid_losses[epoch]\n",
    "        valid_indices_temp = valid_indices[epoch]\n",
    "        ind = 0\n",
    "        for mini_batch_id in valid_indices_temp:\n",
    "            epoch_plot.append(epoch + mini_batch_id/len(train_loader))\n",
    "            valid_accs_plot.append(valid_accs_temp[ind]*100)\n",
    "            valid_losses_plot.append(valid_losses_temp[ind])\n",
    "            ind += 1\n",
    "\n",
    "    valid_results = pd.DataFrame({\"epoch\":epoch_plot,\n",
    "                  \"valid_loss\":valid_losses_plot,\n",
    "                  \"valid_acc\":valid_accs_plot\n",
    "                  })\n",
    "\n",
    "    valid_results.to_csv(f\"{VALID_RESULTS_FILENAME}_{sim_num}.csv\",sep=\";\",index=False)\n",
    "    train_accs = [acc*100 for acc in train_accs]\n",
    "    train_results = pd.DataFrame({\"epoch\":list(np.arange(1,EPOCHS+1,1)),\n",
    "                  \"train_loss\":train_losses,\n",
    "                  \"train_acc\":train_accs\n",
    "                  })\n",
    "    train_results.to_csv(f\"{TRAIN_RESULTS_FILENAME}_{sim_num}.csv\",sep=\";\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4e4458c1-bb7c-4431-9b28-d50a3cb74b69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    3869\n",
       "0    3868\n",
       "dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(y_train).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0e24536b-29e4-48e2-a0ed-b58cf1540998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1290\n",
       "1    1289\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(y_valid).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac146c6-75f8-4ca0-bc5f-5bd3ac74d79c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
